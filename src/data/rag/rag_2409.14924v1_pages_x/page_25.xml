<page><text>arXiv Template
A PREPRINT
[139] Chaoxu Pang, Yixuan Cao, Qiang Ding, and Ping Luo. Guideline learning for in-context information extraction.
arXiv preprint arXiv:2310.05066, 2023.
[140] Tianjun Zhang, Aman Madaan, Luyu Gao, Steven Zheng, Swaroop Mishra, Yiming Yang, Niket Tandon, and Uri
Alon. In-context principle learning from mistakes. arXiv preprint arXiv:2402.05403, 2024.
[141] Hao Sun, Yong Jiang, Bo Wang, Yingyan Hou, Yan Zhang, Pengjun Xie, and Fei Huang. Retrieved in-context
principles from previous mistakes. arXiv preprint arXiv:2407.05682, 2024.
[142] Ling Yang, Zhaochen Yu, Tianjun Zhang, Shiyi Cao, Minkai Xu, Wentao Zhang, Joseph E. Gonzalez, and Bin
Cui. Buffer of thoughts: Thought-augmented reasoning with large language models, 2024.
[143] Harsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicolo Fusi, Nicholas King, Jonathan
Larson, Yuanzhi Li, Weishung Liu, et al. Can generalist foundation models outcompete special-purpose tuning?
case study in medicine. arXiv preprint arXiv:2311.16452, 2023.
[144] Junkai Li, Siyu Wang, Meng Zhang, Weitao Li, Yunghwei Lai, Xinhui Kang, Weizhi Ma, and Yang Liu. Agent
hospital: A simulacrum of hospital with evolvable medical agents. arXiv preprint arXiv:2405.02957, 2024.
[145] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What makes good
in-context examples for gpt-3? arXiv preprint arXiv:2101.06804, 2021.
[146] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
Advances in neural information processing systems, 33:1877–1901, 2020.
[147] Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael Schärli, and Denny
Zhou. Large language models can be easily distracted by irrelevant context. In International Conference on
Machine Learning, pages 31210–31227. PMLR, 2023.
[148] Jiuhai Chen, Lichang Chen, Chen Zhu, and Tianyi Zhou. How many demonstrations do you need for in-context
learning? arXiv preprint arXiv:2303.08119, 2023.
[149] Zhenyu Wu, YaoXiang Wang, Jiacheng Ye, Jiangtao Feng, Jingjing Xu, Yu Qiao, and Zhiyong Wu. Openicl: An
open-source framework for in-context learning. arXiv preprint arXiv:2303.02913, 2023.
[150] Xiaonan Li, Kai Lv, Hang Yan, Tianyang Lin, Wei Zhu, Yuan Ni, Guotong Xie, Xiaoling Wang, and Xipeng Qiu.
Unified demonstration retriever for in-context learning. arXiv preprint arXiv:2305.04320, 2023.
[151] Jiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu, and Lingpeng Kong. Compositional exemplars for in-context
learning. In International Conference on Machine Learning, pages 39818–39833. PMLR, 2023.
[152] Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf,
Luke Zettlemoyer, Noah A Smith, et al. Selective annotation makes language models better few-shot learners.
arXiv preprint arXiv:2209.01975, 2022.
[153] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large
language models. arXiv preprint arXiv:2210.03493, 2022.
[154] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and
Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint
arXiv:2203.11171, 2022.
[155] Rishabh Agarwal, Avi Singh, Lei M Zhang, Bernd Bohnet, Stephanie Chan, Ankesh Anand, Zaheer Abbas,
Azade Nova, John D Co-Reyes, Eric Chu, et al. Many-shot in-context learning. arXiv preprint arXiv:2404.11018,
2024.
[156] Mohammadreza Pourreza and Davood Rafiei. Din-sql: Decomposed in-context learning of text-to-sql with
self-correction. Advances in Neural Information Processing Systems, 36, 2024.
[157] Qihuang Zhong, Kang Wang, Ziyang Xu, Juhua Liu, Liang Ding, Bo Du, and Dacheng Tao. Achieving&gt; 97% on
gsm8k: Deeply understanding the problems makes llms perfect reasoners. arXiv preprint arXiv:2404.14963,
2024.
[158] Dan Schumacher and Anthony Rios. Team utsa-nlp at semeval 2024 task 5: Prompt ensembling for argument
reasoning in civil procedures with gpt4. arXiv preprint arXiv:2404.01961, 2024.
[159] Chengfeng Dou, Zhi Jin, Wenping Jiao, Haiyan Zhao, Zhenwei Tao, and Yongqiang Zhao. Plugmed: Im-
proving specificity in patient-centered medical dialogue generation using in-context learning. arXiv preprint
arXiv:2305.11508, 2023.
[160] Yinheng Li, Shaofei Wang, Han Ding, and Hang Chen. Large language models in finance: A survey. In
Proceedings of the fourth ACM international conference on AI in finance, pages 374–382, 2023.
25</text></page>