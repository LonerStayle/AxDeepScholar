<page><text>Published as a conference paper at ICLR 2025
A
NUANCES OF ANSWERABILITY
Determining answerability can be challenging. To determine answerability, we use a system that
evaluates the entailment of gold claims against provided documents, referred to as the Natural Lan-
guage Inference (NLI) system. An NLI system can range from a simple exact match (EM) identifier
to an LLM or even a human evaluator, with answerability determined based on q, D and biases of
the NLI9. These biases can be useful in specific RAG applications, such as solving mathematical
problems where the documents provide a formula and the question assigns values to variables. The
choice of NLI depends on whether the RAG system requires the LLM to have mathematical under-
standing. Ideally, to prevent improper evaluations, the NLI model used to construct the gold
claims should also be used to evaluate the LLM responses.
In this paper, our focus is on evaluating the generic comprehension capabilities of LLMs without
specialized knowledge. Thus, we use two NLI mechanisms: 1) identifying whether an exact match
of claims is present in the gold claims, and 2) using a Machine Learning (ML) model to deter-
mine if the documents can entail the gold claims. The ML-based NLI model is used for multiple
purposes, such as alignment dataset construction (data/training) and evaluating generated responses
(metric/testing). For this, we adopt the NLI model from Rashkin et al. (2022). Ï•(cij, si) = 1 if cij
(premise) entails si (hypothesis); otherwise, 0. To determine answerability, we employ the TRUE-
based method (Honovich et al., 2022) to assess whether a gold claim can be entailed by a given
document.
The knowledge grounding problem.
Typically, LLMs are designed to perform question-
answering tasks, where response generation heavily relies on the parametric (internal) knowledge
acquired during their pre-training, tuning, and alignment phases (OpenAI, 2023; Anthropic, 2024).
Thus, most of their knowledge is grounded in parametric memory. This makes them inherently
less suitable for RAG applications, where the knowledge generated by the LLM is expected to be
grounded in input documents. RAG is analogous to a reading comprehension task, where the an-
swers must come from the provided passage (documents in RAG) rather than the prior knowledge of
the person taking the test. Thus, any reliance on parametric knowledge can result in statements that
are not fully grounded in the documents, including providing answers to unanswerable questions.
Our investigation shows that state-of-the-art models, such as GPT-4 and Claude-3.5-Sonnet, overtly
rely on parametric knowledge even when used in a RAG setting.10
B
ANSWERABILITY: A CASE STUDY
Prior works (Liu et al., 2023; Gao et al., 2023b; Ye et al., 2024; Huang et al., 2024a; Li et al.,
2024a) have employed substring matching to indicate entailment. While this syntactic approach is
fast, it often proves inadequate in complex, long contexts. A case study is presented in Table 8. To
address the limitations of this superficial entailment, we adopt a TRUE-based method (Honovich
et al., 2022), which combines the strengths of both syntactic and semantic approaches. Specifically,
we enhance the process by using the TRUE model, a T5-11B model (Raffel et al., 2020) fine-tuned
for the NLI task, to verify, from a semantic perspective, whether a substring match corresponds to
meaningful entailment within document passages. The input to the TRUE model is the concatenation
of a premise and a hypothesis, and the output is an entailment score between 0 and 1, indicating the
degree to which the premise entails the hypothesis. We treat the corresponding documents as the
premise, and to minimize ambiguity, the associated question is concatenated with each gold answer
as the hypothesis. In cases where the TRUE model does not yield a positive entailment score despite
a substring match, we rely on the TRUE judgment as the final label. However, if the substring match
fails, we bypass TRUE calculation, thus reducing the computational cost of relying solely on TRUE
for semantic entailment.
9For EM, the bias is that a q is answerable if an exact match for claims is present in D.
10We show a detailed analysis in Appendices F.2 and F.3.
17</text></page>