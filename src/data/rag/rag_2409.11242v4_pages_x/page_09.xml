<page><text>Published as a conference paper at ICLR 2025
Table 4: Performance of models with only SFT applied as compared to TRUST-ALIGN models. Best
values within each family are bolded).
Model
Type
ASQA (610 answerable, 338 unanswerable)
QAMPARI (295 answerable, 705 unanswerable)
ELI5 (207 answerable, 793 unanswerable)
Resp.
Trustworthiness
Resp.
Trustworthiness
Resp.
Trustworthiness
AR (%)
Truthfullness
Att-Grd.
TRUST
AR (%)
Truthfullness
Att-Grd.
TRUST
AR (%)
Truthfullness
Att-Grd.
TRUST
F1AC
F1GR
F1GC
F1AC
F1GR
F1GC
F1AC
F1GR
F1GC
LLaMA-2
-7b
SFT
80.17
53.21
63.43
79.61
65.42
31.60
33.76
71.13
46.37
50.42
29.50
21.58
63.30
39.59
41.49
TRUST-ALIGN (DPO)
65.30
52.48
66.12
83.94
67.51
32.30
32.03
71.67
49.42
51.04
21.60
22.54
63.27
47.35
44.39
LLaMA-3.2
-1b
SFT
63.82
45.61
63.91
73.10
60.87
26.00
27.98
68.20
37.96
44.71
20.50
14.56
63.93
37.28
38.59
TRUST-ALIGN (DPO)
41.67
38.64
58.61
79.35
58.87
20.00
27.22
67.92
49.42
48.19
9.60
13.20
59.35
48.21
40.25
LLaMA-3.2
-3b
SFT
68.04
49.23
65.47
75.63
63.44
27.60
28.09
70.22
38.03
45.45
14.70
15.92
62.59
53.33
43.95
TRUST-ALIGN (DPO)
77.85
59.82
66.38
84.21
70.14
48.20
29.13
70.85
45.65
48.54
17.50
18.33
62.79
55.87
45.66
LLaMA-3
-8b
SFT
68.99
52.35
66.06
80.95
66.45
24.20
33.85
71.11
48.01
50.99
23.60
22.57
65.06
46.85
44.83
TRUST-ALIGN (DPO)
56.43
53.94
65.49
88.26
69.23
22.40
35.35
70.73
58.77
54.95
15.50
20.81
63.57
50.24
44.87
Qwen-2.5
-0.5b
SFT
83.44
38.71
58.03
57.47
51.40
18.50
16.02
61.35
27.82
35.06
35.50
10.50
57.19
19.57
29.09
TRUST-ALIGN (DPO)
71.84
50.59
61.28
52.40
54.76
17.90
15.76
61.84
29.73
35.78
21.70
13.68
60.79
22.72
32.40
Qwen-2.5
-1.5b
SFT
78.27
44.23
58.75
71.08
58.02
25.50
23.89
69.66
37.68
43.74
41.30
14.14
55.35
27.69
32.39
TRUST-ALIGN (DPO)
72.57
52.68
62.38
66.81
60.62
20.00
23.80
68.46
50.98
47.75
33.60
19.03
57.91
31.63
36.19
Qwen-2.5
-3b
SFT
75.21
47.26
60.61
73.09
60.32
27.20
28.80
68.12
37.34
44.75
34.50
14.85
61.47
35.87
37.40
TRUST-ALIGN (DPO)
49.47
55.19
63.76
78.64
65.86
48.10
35.69
70.31
45.64
50.55
13.50
22.52
64.38
42.01
42.97
Qwen-2.5
-7b
SFT
65.30
50.73
64.50
82.07
65.77
31.70
33.58
70.10
49.08
50.92
25.50
20.78
64.25
46.89
43.97
TRUST-ALIGN (DPO)
59.49
55.04
66.22
83.57
68.28
32.10
30.11
70.68
53.48
51.42
21.00
24.30
63.79
47.02
45.04
Phi3.5
-mini
SFT
66.46
51.92
64.34
82.77
66.34
29.10
35.04
73.93
49.38
52.78
24.50
22.50
65.70
46.79
45.00
TRUST-ALIGN (DPO)
66.56
52.23
64.20
85.36
67.26
30.10
36.42
73.95
53.40
54.59
24.90
23.39
67.62
47.42
46.14
Table 5: Ablations of data synthesis techniques for LLaMA-2-7b on three evaluation datasets using refusal
prompting; The original error types in Section 2.3 were summarized into three main classes: answer-related
(Inaccurate Answer), citation-related (Overcitation, Improper Citation), refusal-related (Over Responsiveness,
Excessive Refusal).
ASQA
QAMPARI
ELI5
Resp.
Trustworthiness
Resp.
Trustworthiness
Resp.
Trustworthiness
AR (%)
Truthfullness
Att-Grd.
TRUST
AR (%)
Truthfullness
Att-Grd.
TRUST
AR (%)
Truthfullness
Att-Grd.
TRUST
F1AC
F1GR
F1GC
F1AC
F1GR
F1GC
F1AC
F1GR
F1GC
DPO-LLaMA-2-7b
65.30
52.48
66.12
83.94
67.51
31.10
32.09
71.83
51.33
51.75
21.60
22.54
63.27
48.43
44.75
TRUST-ALIGN w/o. augmented instructions
79.43
53.54
63.33
81.15
66.01
32.20
33.14
70.82
45.94
49.97
29.50
23.98
63.30
40.28
42.52
TRUST-ALIGN w/o. answer HT
77.74
53.29
63.7
81.2
66.06
33.40
33.56
71.36
46.17
50.36
27.60
23.47
63.56
38.28
41.77
TRUST-ALIGN w/o. citation HT
77.32
52.55
63.88
81.51
65.98
33.10
34.13
71.40
46.91
50.81
26.70
22.65
64.33
42.81
43.26
TRUST-ALIGN w/o. refusal HT
79.11
53.55
63.33
81.85
66.24
31.10
34.40
71.35
48.12
51.29
28.30
22.93
64.05
41.18
42.72
GPT-4 as critic
70.36
54.91
65.29
78.47
66.22
25.90
30.77
70.29
48.87
49.98
23.50
17.27
62.24
42.38
40.63
We validated our data construction approach against the GPT-4-as-critic pipeline (Li et al., 2024a;
Huang et al., 2024b), where GPT-4 iteratively identifies and corrects errors to generate positive and
negative responses (details in Appendix G). In LLaMA-2-7b, TRUST-ALIGN outperforms GPT-4
critic on TRUST-SCORE, with gains of 1.29% (ASQA), 1.77% (QAMPARI), and 4.12% (ELI5).
Table 6: Effect of adding refusal sam-
ples on the ASQA.
TRUST-ALIGN Models
AR%
F1AC
F1GR
F1GC
TRUST
Only Answerable
DPO-LLaMA-2-7b
100
51.79
39.15
77.37
56.10
DPO-LLaMA-3-8b
100
56.54
39.15
81.39
59.03
With Refusal
DPO-LLaMA-2-7b
65.30
52.48
66.12
83.94
67.51
DPO-LLaMA-3-8b
56.43
53.94
65.49
88.26
69.23
Importance of refusal samples in TRUST-ALIGN.
To
verify the importance of refusal samples in our pipeline,
we removed all unanswerable questions from the training
set, creating a dataset without refusals. Table 6 shows
a significant drop in TRUST-SCORE scores without re-
fusals, including declines of 10.2% (LLaMA-3-8b) and
11.41% (LLaMA-2-7b). Notably, F1GR decreases by 26.34% (LLaMA-3-8b) and 26.97% (LLaMA-
2-7b), and F1GC by 6.87% (LLaMA-3-8b) and 6.57% (LLaMA-2-7b).
We also observe that in LLaMA-3-8b, F1AC is higher in the answerable-only setting compared to
with refusals setting. This occurs because RAC favors over-responsive models, which artificially
inflates F1AC, as discussed in main results. The resulting models answer all questions (AR% of
100%), even without supporting documents, suggesting an increased reliance on ungrounded para-
metric knowledge, as discussed in Section 6.1.
Out-of-domain analysis.
Following Huang et al. (2024a), we use ExpertQA (Malaviya et al.,
2024) to assess our modelâ€™s generalizability. As shown in Table 7, TRUST-ALIGN model outper-
forms FRONT on TRUST-SCORE across all 27 open-source model family and dataset configurations.
We also observe that the open-source ICL models perform significantly worse on TRUST-SCORE as
compared to the closed-source ICL models, with a 9.79% gap between LLaMA-3-8b and GPT-
4. TRUST-ALIGN not only closes this gap but establishes a lead: TRUST-ALIGNed LLaMA-3-8b
achieves the highest TRUST score of 54.85%, surpassing 54.69% of GPT-4.
9</text></page>