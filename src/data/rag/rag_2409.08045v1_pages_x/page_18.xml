<page><text>C
Appendix C
Evaluating the Payload Performance
Here we evaluate the Payload performance of the worm using
the following metrics: Coverage, Precision, F1 Score and
Error (Hallucination) Rate.
Metrics
Coverage (recall or true positive rate) - the number of sensi-
tive items (emails) returned (and appear in the context) in the
response of the GenAI service divided by the total number
of items returned in the response (which also includes the
hallucinated items).
Error (Hallucination) Rate - the percentage of the wrong
information returned in the response. This is calculated by
the number of sensitive items (emails) that appear in the
response but do not appear in the given context divided by
the number of sensitive items that appear in the response.
Precision - the number of sensitive items (emails) returned
in the response of the GenAI service divided by the total
number of sensitive items given in the context (emails).
F1 - the harmonic mean between recall and precision.
Experimental Setup. We created a personal database for
every employee using 100 emails from Enron (50 emails re-
ceived and 50 emails sent by the employee). The worm we
used is based on the Wikipedia prefix (see Fig. 6) and j, r,
and m which are presented in Listing 3 with minor modifica-
tions to r, causing the worm to focus exclusively on retrieving
email addresses. To evaluate the payload performance of the
worm, we used the GenAI engine (Gemini 1.5 Flash) to en-
rich the body of an email written by the employee, selected
from their outgoing emails, using context retrieved by the
RAG from the employee’s database. We retrieved K=9 doc-
uments from the user’s RAG and added the worm to make
up a total of 10 documents for the context. This experiment
was repeated 1,000 times across 20 different employees, with
each iteration enriching one of their 50 outgoing emails. Dur-
ing these experiments, we extracted the email addresses from
both the context retrieved by the RAG and the email addresses
generated by the GenAI engine.
Results. As shown in the top of Fig. 9 the F1 score begins
at 0.78 when the context includes 10 emails, but decreases
to 0.58 as the context size grows to 100 emails. Additionally,
the error rate rises as more emails are added to the context,
starting at 0.26 and increasing to 0.37.
A common error observed with Gemini 1.5 Flash involved
hallucinating complete email addresses based on the personal
names of tagged employees from previous email threads, as
illustrated in Listing 5. In the lower part of Fig. 9, the worm’s
scalability in terms of leakage performance is shown. Notably,
Gemini 1.5 Flash was able to search, identify, and extract
at least 50% of the real email addresses from the context,
even when the context included between 10 and 100 email
documents.
Figure 9: The Payload Performance (top) and Email Ad-
dresses Count in (bottom).
*** Enron Email***
"All , I just talked with Andy Fastow and Brian
Redmon. We need to pass on him , they really
didn ’t recommend him...
***Past Email Thread***
03/22/2001 08:42 AM To: Mike McConnell/HOU/
ECT@ECT cc:
Eric Gonzales/LON/ECT@ECT, GeorgeMcClellan/HOU/ECT@ECT,
John L Nowlan/HOU/ECT@ECT,
Subject: Re: Don Reid
I met w/ him a couple weeks ago. Good guy
..."
-----------------------
***Wrong Emails Addresses Returned By The LLM***
george.mcclellan@enron.com,eric.gonzales@enron.com,
john.l.nowlan@enron.com
Listing 5: Common Errors Encountered with Gemini 1.5
Flash
18</text></page>