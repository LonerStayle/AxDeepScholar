<page><text>(6) Automatic Input/Output Data Sanitization - Train-
ing dedicated classifiers to identify risky inputs and outputs.
This method can be effective at detecting: adversarial self-
replicating prompts due to their unique structure, common
jailbreaking techniques (e.g., detecting roleplay jailbreaking),
and toxic and harmful content (e.g., using sentiment analy-
sis algorithms). However, attackers can use adaptive tech-
niques to create inputs that evade detection: G#- against RAG
documents extraction, RAG entity extraction, and worm. #-
membership inference attacks, and RAG poisoning attacks.
5.2
Conclusions
The analysis (summarized in Table 1) reveals a tradeoff in
the system’s security level and the system’s usability (i.e., the
implications of applying the countermeasure):
(1) RAG data poisoning attacks and worms exploit the
database of the RAG for persistence. Therefore, these attacks
could be prevented by limiting the insertion into the database
of the RAG to content generated by trusted users (access con-
trol). For example, within the context of a database containing
a user’s emails, such a policy allows the insertion of emails
generated by the user while prohibiting the insertion of emails
generated by untrusted entities (e.g., emails received by the
user). This reveals an interesting tradeoff between good sys-
tem security and low system usability: it prevents attackers
from unleashing worms into the wild and poisoning the RAG
while decreasing the accuracy of RAG-based inference due to
the relevant benign information (received from benign users)
was not inserted in the database due to the adopted policy. The
implication of adopting this policy clashes with the reason we
integrated RAG (to increase the accuracy of the inference).
(2) Membership inference, RAG entity extraction, and RAG
documents extraction attacks are harder to prevent, as their
success relies on an attacker’s ability to probe the RAG-based
GenAI-powered application repeatedly (a reasonable prop-
erty for Q&amp;A chatbots). Consequently, the combination of
a set of guardrails (API throttling, thresholding, size limit,
data sanitization) can raise the efforts the attackers need to
invest in performing the attacks because the combination of
the guardrails limits the number of probes, the number of re-
turned documents, and the space the attacker have to craft an
input while having a negligible effect on the system’s usabil-
ity (given that they are configured correctly). However, these
guardrails could be bypassed by adaptive and distributed at-
tacks (given the knowledge and configuration of the deployed
guardrails), a tradeoff between medium system security and
excellent system usability.
(3) Human-in-the-loop can be effective against various
attacks by validating the output of the GenAI-powered ap-
plication. However, it can suffer from scaling issues and can
only be integrated into semi-autonomous GenAI-powered
applications that assist humans (instead of replacing them).
Table 1: Guardrails Analysis. The guardrail prevents ( ),
mitigates (G#), or ineffective (#) against the attack.
Membership
Inference
RAG Entity
Extraction
RAG Data
Poisoning
RAG Data
Extraction
Worm
Access Control
#
#
 
#
 
Retrieval Rate
Limit
G#
G#
#
G#
#
Thresholding
G#
G#
#
G#
G#
Human in
the Loop
G#
G#
G#
G#
G#
Content Size
Limit
G#
G#
G#
G#
G#
Automatic Data
Sanitization
#
G#
G#
G#
G#
6
Limitations
The attacks we presented suffer from the following limita-
tions:
Overtness. We note that the adversarial self-replicating
prompt or the payload (e.g., the sensitive data exfiltrated or
extracted documents) can be detected by the user in cases
where a human-in-the-loop policy is adopted or by dedicated
classifiers. However, we argue that the use of a human as
a patch for a system’s vulnerability is bad practice because
end-users cannot be relied upon to compensate for existing
vulnerabilities of systems and are not effective in fully au-
tonomous GenAI ecosystems of agents (when humans are
not in the loop). In addition, attackers can also bypass classi-
fiers intended to detect adversarial self-replicating prompts
by using adaptive attacks.
Jailbreak Success. The attacks are highly affected by the
ability to jailbreak a GenAI model. We note that GenAI
engines are continuously patched against jailbreaking com-
mands. Therefore, it may require attackers to use the most
updated jailbreaking commands shared on the web, which
according to [10], may persist for over 240 days.
Extensive API Calling/Probing. We note that the applica-
tion of the RAG documents extraction attack relies on mul-
tiple API calls which can be flagged as an attempt to extract
data. However, attackers can bypass the detection by launch-
ing multiple sessions from various machines.
7
Conclusions
The objective of this paper is to shed light on new risks of
RAG-based inference, focusing on the risks posed by a jail-
broken GenAI model. We show that by jailbreaking a GenAI
model via direct/indirect prompt injection, attackers can esca-
late the outcome of attacks against RAG-based inference in
scale (by compromising a network of GenAI-powered applica-
tions instead of a single application) and severity (extracting
documents from the RAG instead of entities).
13</text></page>