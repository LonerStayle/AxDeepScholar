{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a3327b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seobi/PythonProjects/AxDeepScholar/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain import OpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Qdrant\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b2f3a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "current_dir = Path.cwd()\n",
    "root_dir = current_dir\n",
    "for parent in current_dir.parents:\n",
    "    if (parent / \"src\" / \"data\").exists():\n",
    "        root_dir = parent\n",
    "        break\n",
    "    \n",
    "pdf_folder = root_dir / \"src\" / \"data\"\n",
    "pdf_paths = sorted(pdf_folder.rglob(\"*.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31bd39cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for pdf_path in pdf_paths:\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    dlist = loader.load()\n",
    "    docs.append(dlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "103934a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-15T00:07:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-15T00:07:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2408.07199v1.pdf', 'total_pages': 22, 'page': 0, 'page_label': '1'}, page_content='Agent Q: Advanced Reasoning and Learning\\nfor Autonomous AI Agents\\nPranav Putta1, Edmund Mills1, Naman Garg1, Sumeet Motwani1, Chelsea Finn2, Divyansh Garg1 and Rafael\\nRafailov2\\n1The AGI Company (MultiOn),2Stanford University\\nLarge Language Models (LLMs) have shown remarkable capabilities in natural language tasks requiring complex\\nreasoning, yet their application in agentic, multi-step reasoning within interactive environments remains a\\ndifficult challenge. Traditional supervised pre-training on static datasets falls short in enabling autonomous\\nagent capabilities needed to perform complex decision-making in dynamic settings like web navigation. Previous\\nattempts to bridge this gap through supervised fine-tuning on curated expert demonstrations often suffer from\\ncompounding errors and limited exploration data, resulting in sub-optimal policy outcomes. To overcome\\nthese challenges, we propose a framework that combines guided Monte Carlo Tree Search (MCTS) search\\nwith a self-critique mechanism and iterative fine-tuning on agent interactions using an off-policy variant of the\\nDirect Preference Optimization (DPO) algorithm. Our method allows LLM agents to learn effectively from both\\nsuccessfulandunsuccessfultrajectories, therebyimprovingtheirgeneralizationincomplex, multi-stepreasoning\\ntasks. We validate our approach in the WebShop environment, a simulated e-commerce platformâ€”where it\\nconsistently outperforms behavior cloning and reinforced fine-tuning baseline, andbeats average human\\nperformance when equipped with the capability to do online search. In real-world booking scenarios, our\\nmethodology boosts Llama-3 70B modelâ€™s zero-shot performance from18.6% to 81.7%success rate (a340%\\nrelative increase) after a single day of data collection and further to95.4% with online search. We believe\\nthis represents a substantial leap forward in the capabilities of autonomous agents, paving the way for more\\nsophisticated and reliable decision-making in real-world settings.\\n1. Introduction\\nThe recent advances in Large Language Models (LLMs) represent a significant leap in artificial\\nintelligence. Frontier models like ChatGPT (John Schulman et al., 2022), Gemini (Anil et al., 2023),\\nOpus (Anthropic, 2024), and LLaMA-3 (Touvron et al., 2023) demonstrate promising reasoning\\ncapabilities that approach average human performance in a number of domains. These breakthroughs\\nhave extended the utility of LLMs from traditional chat and text-based applications to more dynamic,\\nagentic roles, in which they do not just generate text but can take actions autonomously in a number\\nof environments including code and software engineering (Holt et al., 2024; Jimenez et al., 2024;\\nYang et al., 2024; Zhang et al., 2024d), device control (Chen and Li, 2024; Wang et al., 2024a;\\nZhang et al., 2023) and web applications (Deng et al., 2023; Gur et al., 2024; Hong et al., 2023; Lai\\net al., 2024a; Zhou et al., 2024b) among others. However, despite these advancements, significant\\nchallenges persist: LLMs still struggle to generalize effectively in interactive, multi-step environments,\\nsince they are not native trained for such applications . This is true, even for some of the strongest\\nmodels of the current generation, such as GPT-4 (Achiam et al., 2023).\\nA growing literature on agentic formulation seeks to address these issues; however these works mostly\\nfocus on building frameworks around prompt-based learning on existing models or limited fine-tuning\\non static datasets, and are thus limited by the base modelsâ€™ reasoning and decision making capabilities.\\nReasoning and planning have indeed been highlighted as core challenges for current LLMs. Since the\\nseminal work on chain-of-thought reasoning (Wei et al., 2022), significant efforts have been made\\nto improve these capabilities via prompt-based strategies (Kojima et al., 2022; Qiao et al., 2023;\\nCorresponding author(s): div@multion.ai, rafailov@stanford.edu\\narXiv:2408.07199v1  [cs.AI]  13 Aug 2024'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-15T00:07:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-15T00:07:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2408.07199v1.pdf', 'total_pages': 22, 'page': 1, 'page_label': '2'}, page_content='Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents\\nFigure 1: We use Monte Carlo Tree Search (MCTS) to guide trajectory collection and iteratively\\nimprove model performance using direct preference optimization (DPO). We begin on the left by\\nsampling a user query from the list of tasks in the dataset. We iteratively expand the search tree\\nusing UCB1 as a heuristic to balance exploration and exploitation of different actions. We store the\\naccumulated reward obtained for each node in the tree, where in this image darker green indicates\\nhigherrewardanddarkerredindicateslowerreward. Toconstructthepreferencedataset, wecompute\\na weighted score of the MCTS average Q-value and score generated by a feedback language model to\\nconstruct contrastive pairs for DPO. The policy is optimized and can be iteratively improved.\\nWang et al., 2023; Yao et al., 2023a). While successful, these approaches are still bounded by the\\nbase modelâ€™s performance. Another direction of research has explored fine-tuning approaches (Pang\\net al., 2024; Zelikman et al., 2022), and more recently combining them with inference-time search\\nprompting (Yao et al., 2023a) to produce fine-grained feedback. Concurrent works (Hwang et al.,\\n2024; Tian et al., 2024; Xie et al., 2024; Zhang et al., 2024e) utilize the traces produced by search\\nalgorithms and combine them with optimization approaches (Rafailov et al., 2023; Zelikman et al.,\\n2022) to achieve significant boost in capabilities, especially in mathematics problem solving and code\\ngeneration.\\nIn this work we explore improving planning and reasoning capabilities of a web agent, which interacts\\nwith a real world website. Our goal is to design an approach that allows the agent to improve with\\nautonomous experience and limited supervision. Indeed, prior works (Masterman et al., 2024; Sumers\\net al., 2024; Yao et al., 2023b; Zhang et al., 2024c) have shown strong reasoning to be critical for\\nperformance of autonomous agents, where challenges are even greater than during text generation,\\nas the model needs to further understand how its actions affect its environment. Towards this goal,\\nwe introduceAgent Qâ€”a novel approach that combines several key concepts in reasoning, search,\\nself-critique and reinforcement learning. Our method takes inspiration from Suttonâ€™s The Bitter\\nLesson on the power of general purpose methods that continue to scale with increased computation,\\nshowing the significant benefits of combiningsearch and learning.\\nInspired by the success of search-based methods in prior game-playing settings (Brown and Sandholm,\\n2019; Gray et al., 2021; Silver et al., 2017a) and mathematical reasoning (Besta et al., 2024; Yao\\net al., 2023a), we deploy a Monte Carlo Tree Search (MCTS) based search routine over web pages to\\nguide agent exploration. Given the complexity of the environment, we use a base LLM for sampling\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-15T00:07:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-15T00:07:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2408.07199v1.pdf', 'total_pages': 22, 'page': 2, 'page_label': '3'}, page_content='Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents\\npossible rationales and web actions to explore. While this simple search-strategy shows a meaningful\\nimprovement in the success rate, it still struggles on long horizon tasks due to sparsity of environment\\nrewards. Indeed even a small mistake across the trajectory can cause the final agent output to be\\nwrong, creating significant credit assignment problems. To overcome this, we use AI feedback (Bai\\net al., 2022) and self-criticism (Yuan et al., 2024) to further prompt the LLM to provide self-evaluation\\nfeedback at each node, which serves as intermediate reward and helps guide the search steps. This\\nmeaningfully improves the final agent success rate, but requires significant online interactions and\\nmoreover the capability to rollback actions, which is not always possible in online realistic settings.\\nSuch online autonomous search with little supervision on the web can result in a weak or unsafe\\nagent which can make many errors, resulting in risky behaviors in sensitive online settings like bank\\ntransfers and sensitive information sharing.\\nTo correct this, we use the traces generated by the search process to improve capabilities of the\\nmodel by learning from both the successful and unsuccessful trajectories with offline reinforcement\\nlearning, utilizing the Direct Preference Optimization (DPO) algorithm. We create preferences over\\ndifferent branches at the node level, which are scored using a mixture of the AI process feedback\\nrewards and the final success rate of the explored branch. We evaluate our approach on the simulated\\nWebShop benchmark (Yao et al., 2022)â€”a simulated e-commerce platformâ€”as well as a real-world\\nreservations booking website. We utilize LLaMa 3-70B as the base model in our experiments. In the\\nWebShop environment, our approach consistently outperforms behavior cloning and reinforcement\\nlearning fine-tuned baselines, andbeats average human performance when equipped with the\\ncapability to do online search.\\nIn our real-world booking experiments, using ourAgent Qframework we improve the model zero-shot\\nabsolute success rate from18.6% to 81.7% (a 340% relative increase), outperforming GPT-4â€™s\\nperformance after a single day of autonomous data collection. When we equip Agent Q with online\\nsearch capability, our absolute success further improves to95.4%. We believe that our approach\\nrepresents a significant step forward in the development of autonomous web agents through itâ€™s\\nsearch and self-critique capabilities, setting a new benchmark for reliable multi-step decision-making\\nin interactive settings.\\n2. Related Work\\nOur work touches on a large number of research directions around agent design, self-improvement,\\nreasoning and reinforcement learning. We include a short overview of related works from those\\nvarious fields below.\\n2.1. Guided Search for Reasoning and Planning\\nThe latest generation of Large Language Models (LLMs) have demonstrated promising emerging\\nproperties around reasoning and planning. Moreover such behaviours can be directly elicited from\\nstrong models only using simple prompting techniques (Kojima et al., 2022; Qiao et al., 2023; Wei\\net al., 2022). These have also become an integral part of agentic design (Yao et al., 2023b; Zhang\\net al., 2024c), which we also utilize for our approach. Another emerging research direction is based\\naround step-by-step verifiers or â€œProcess Reward Modelsâ€ (Lightman et al., 2023; Uesato et al., 2022),\\nspecifically for mathematical reasoning. These have shown to improve performance beyond purely\\noutcome-based training, however they require a large amount of human effort to label individual\\nsteps. Some recent approaches have proposed self-supervised methods for step-level supervision\\n(Hwang et al., 2024; Setlur et al., 2024a; Wang et al., 2024b). A number of concurrent works (Tian\\net al., 2024; Xie et al., 2024; Zhang et al., 2024e) have further explored tree-based search approaches\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-15T00:07:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-15T00:07:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2408.07199v1.pdf', 'total_pages': 22, 'page': 3, 'page_label': '4'}, page_content='Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents\\n(Yao et al., 2023a) in combination with DPO (Rafailov et al., 2023) training for math-based reasoning.\\nThese algorithms optimize actions at the node level, using different branches produced by the search\\nalgorithm to create preference pairs. Our approach shares similarities to the self-supervised search\\nproposed in (Yao et al., 2023a) with a combination of AI-based feedback (Bai et al., 2022; Yuan et al.,\\n2024) to guide intermediate search steps, but we are the first to scale this a realistic agent setting.\\nSimilar approaches were proposed in (Hao et al., 2023; Zhou et al., 2024a), and other works (Koh\\net al., 2024); however these works only use the base modelâ€™s zero-shot capability and do not train it\\nfurther. Moreover they are only evaluated on simulated environments. Beyond the search stage, our\\nwork further adopts the training methodology of (Tian et al., 2024; Xie et al., 2024; Zhang et al.,\\n2024e), which significantly boosts our agentâ€™s zero-shot capabilities.\\n2.2. Web Agents\\nThe strength and capabilities of recent pretrained Large Language (Vision) Models LL(V)Ms has\\nsignificantly boosted progress in developing autonomous web-agents. Improved code understanding\\nand long context have allowed agents to represent environment state and action space with document\\nobject model (DOM) allowing for deployment in complex and realistic domains. Moreover strong\\nreasoning (Yao et al., 2023b) and planning (Liu et al., 2023; Zhang et al., 2024c) capabilities have also\\nled to the development of a number of promising agents (Deng et al., 2023; Gur et al., 2024; Hong\\net al., 2023; Zhang and Zhang, 2023; Zhou et al., 2024b). Beyond using LL(V)Ms as plug-and-play\\nplanners/policies, recent works have sought to improve agentic-specific performance. Examples\\ninclude online exploration (Zhang et al., 2024a), planning (Zhang et al., 2024b), error-correction\\n(Wang et al., 2024a), and self- (Wu et al., 2024) or AI-critique (He et al., 2024; Pan et al., 2024).\\nHowever, with small exceptions (Nakano et al., 2022) (which is still limited in scope) these agents\\nmostly provide a framework around a strong pre-existing model like GPT4-V or deploy limited\\nfine-tuning and adaptation. In this work we show that model training is crucial for continuous\\nimprovement. We combine a planning and reasoning agent with MCTS inference-time search and AI\\nself-critique for self-supervised data collection, which we then use for RL type training.\\n2.3. Reinforcement Learning for LLMs and Agents\\nReinforcement Learning has become a significant component of training modern generative AI systems\\n(Bai et al., 2022; Ouyang et al., 2022; Touvron et al., 2023). Classical approaches have deployed the\\nPPO algorithm (Schulman et al., 2017)â€”or similar policy-gradient based methodsâ€”and have even\\nbeen scaled to autonomous web search agents (Nakano et al., 2022) as well as embodied applications\\nwith vision-language models (Zhai et al., 2024) (in simulation). However, these algorithms are\\nchallenging due to their complexity and the need for a high number of online samples from the\\nmodel. This is especially prominent in potentially risky situations, such as autonomous agentic models\\nthat could make a number of impactful mistakes during training. Implicit Language Q-learning\\n(Snell et al., 2022) and the Q-transformer (Chebotar et al., 2023) are offline RL algorithms (Levine\\net al., 2020) designed for auto-regressive transformer models, and hence can be safely trained on\\npre-collected datasets; however they have not been successfully scaled to modern LLMs. While these\\nmethods represent a token-level MDP, (Zhou et al., 2024c) has shown success formulating the RL\\nproblem at a step level and these ideas have recently been scaled to a general device-control agent\\n(Bai et al., 2024). However, these algorithms still have high complexity and require auxiliary models,\\nsuch as value functions, so instead in our approach we opt to use the Direct Preference Optimization\\n(DPO) algorithm (Rafailov et al., 2023) due to itâ€™s simplicity and natural fit for the branching nature\\nof tree-search based data.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-15T00:07:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-15T00:07:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2408.07199v1.pdf', 'total_pages': 22, 'page': 4, 'page_label': '5'}, page_content='Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents\\nFigure 2: We provide the following input format to the Agent, consisting of the system prompt,\\nexecution history, the current observation as a DOM representation, and the user query containing\\nthe goal. We divide our Agent output format into an overall step-by-step plan, thought, a command,\\nand a status code.\\n3. Preliminaries\\nIn this section we will outline the preliminaries of our agent training process.\\n3.1. Agent Formulation\\nWe consider a general POMDP setup(ğ’ª, ğ’®, ğ’œ, ğ‘‡, ğ‘…, ğœ‡0, ğ›¾) where ğ’ªdenotes the observation space,ğ’®\\nthe unobserved state space,ğ’œthe action space,ğ‘‡(sğ‘¡+1|sğ‘¡, ağ‘¡) the transition distribution (in this case\\nthe dynamics of a web browser),ğ‘…(s, a) the reward function (in this work we use sparse rewards\\nof 1/0 representing success/failure),ğœ‡0(s0) the initial state distribution, andğ›¾ the discount factor,\\nwhich we set to 1. A POMDP is the most suitable framework to model web interactions for several\\nreasons - first novel environments, which the agent is unfamiliar with require exploration in order\\nto locate the task objective, consistent with the meta-reinforcement learning as task inference view\\nHumplik et al. (2019). Moreover, the real web is dynamic, which creates partial observability of\\nthe current state each time the agent is deployed - i.e. it does not a priori know current booking\\navailability before attempting to do it. We will outline the main parts of our web agent below.\\nThe agent observationoğ‘¡ âˆˆğ’ª are commands/information given by the user and the web browser.\\nThe first observationo1 is a user text instruction, such as\\n\"Book reservation for restaurant Cecconiâ€™s on OpenTable for 4 people on May 22 2024 at 7:00 PM\"\\nfor example and a browser home page. Subsequent observations consist of web pages from the\\nbrowser, represented as a HTML DOM format. Occasionally for some tasks the agent might ask for\\nconfirmation/feedback from the user, which then also becomes part of the observation.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-15T00:07:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-15T00:07:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2408.07199v1.pdf', 'total_pages': 22, 'page': 5, 'page_label': '6'}, page_content='Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents\\nThe agent actionsağ‘¡ âˆˆğ’œ are composite, based on agent historyhğ‘¡. Our base approach is a ReAct\\nagent Yao et al. (2023b) with a preliminary planning step (PlanReAct) Liu et al. (2023) with few\\nadditional components.\\nâ€¢ Planning For the first action after the initial observation we leverage the base LLMâ€™s planning\\ncapabilities Huang et al. (2022a) and prompt the agent to generate a planaplan\\n1 âˆ¼ğœ‹(aplan\\n1 |h1)\\nof sequential steps to execute in language.\\nâ€¢ Reasoning Subsequently all actions consist of a thought actionatht\\nğ‘¡ âˆ¼ğœ‹(atht\\nğ‘¡ |hğ‘¡), which is\\nreasoning step Wei et al. (2022).\\nâ€¢ Environment actionNext we generate the browser interaction commandaenv\\nğ‘¡ âˆ¼ğœ‹(aenv\\nğ‘¡ |hğ‘¡, atht\\nğ‘¡ ),\\nwhich consists of a finite set of options like \"CLICK [ELEMENT ID]\", \"SCROLL\", \"TYPE [CON-\\nTENT]\" or \"ASK USER [CONTENT]\" etc.. This is the only part of the action generation, which\\ninteracts with the environment.\\nâ€¢ Explanation actionAfter the environment interaction action has been generated, we additional\\nprompt the model for an explanation actionaexpl\\nğ‘¡ âˆ¼ğœ‹(aexpl\\nğ‘¡ |hğ‘¡, atht\\nğ‘¡ , aenv\\nğ‘¡ ).\\nWe denote the step actionağ‘¡ as a tuple of plan, thought, environment and explanation actions for the\\nfirst step and thought, environment and explanation actions for subsequent steps. When optimizing\\nmodels we consider the joint likelihood\\nlog ğœ‹(a1|h1) = logğœ‹(aexpl\\n1 |h1, aenv\\n1 , atht\\n1 , aplan\\n1 ) + logğœ‹(aenv\\n1 |h1, atht\\n1 , aplan\\n1 )+\\nlog ğœ‹(atht\\n1 |h1, aplan\\n1 ) + logğœ‹(aplan\\n1 |h1) (1)\\nfor the initial action and\\nlog ğœ‹(ağ‘¡|hğ‘¡) = logğœ‹(aexpl\\nğ‘¡ |hğ‘¡, aenv\\nğ‘¡ , atht\\nğ‘¡ ) + logğœ‹(aenv\\nğ‘¡ |hğ‘¡, atht\\nğ‘¡ ) + logğœ‹(atht\\nğ‘¡ |hğ‘¡)\\nfor subsequent actions, unlike some prior works Zhai et al. (2024), which down-weight the reasoning\\nlikelihood.\\nThe agent stateis the current state of the web, which may mot be observable. In this POMDP\\nformulation we also need to build an agent memory componenthğ‘¡. Prior works have used the entire\\ntrajectory of observations and actions, however HTML DOMs can be hundred of thousands of tokens\\nlong. Moreover realistic web-tasks can require many more interactions than static benchmarks such\\nas WebShop Yao et al. (2022) and WebArena Zhou et al. (2024b), which most prior works use. This\\nmakes it impractical to use full web trajectories due to limited context windows, potential out-of-\\ndistribution issues and practical inference speed and cost. Instead, we build the history representation\\nof the agent ashğ‘¡ = (a1, . . . ,ağ‘¡âˆ’1, oğ‘¡). That is, the agent history consists of the actions generated\\nso far and the current browser state. With some abuse of notation we will also refer to this as the\\nagent state. Even though only the environment action is used for interacting with the browser, we\\nconstruct the agent thought and explanation actions to act as a form of inner monologue Huang et al.\\n(2022b) and adequately represent its state and intentions. This allows us to use a significantly more\\ncompact history representation. We should note that, while only the environment action affects the\\nbrowser state, the planning, reasoning and explanation components affect subsequent decisions due\\nto conditioning. Because of this reason, when we optimize the agent, we compute likelihoods over\\nthe composite action.\\n3.2. Fine-Tuning Language Models From Feedback\\nClassical approaches to RLHF in foundation models Ouyang et al. (2022); Stiennon et al. (2022) use\\nthe model as a policyğœ‹ğœƒ and optimize an objective of the form:\\nEaâˆ¼ğœ‹ğœƒ(a|h)[ğ‘Ÿ(a, h)] âˆ’ğ›½Dğ¾ğ¿[ğœ‹ğœƒ(a|h)||ğœ‹ref(a|h)] (2)\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-15T00:07:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-15T00:07:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2408.07199v1.pdf', 'total_pages': 22, 'page': 6, 'page_label': '7'}, page_content='Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents\\nwhere ğœ‹ref is some reference policy (usually the initial model). The goal of this formulation is to\\noptimize some target objective (expressed by the rewardğ‘Ÿ(a, h)) while preventing out-of-distribution\\ndrift. This objective can be extended to multi-step agentic problems, where the model interacts with\\nan external environmentenv such as in Nakano et al. (2021) which focuses on information retrieval\\nusing web navigation. In this case we use an objective of the kind\\nEğœ‹ğœƒ,env\\n[ï¸ƒâˆ‘ï¸\\nğ‘¡\\nğ‘Ÿ(ağ‘¡, hğ‘¡)] âˆ’ğ›½Dğ¾ğ¿[ğœ‹ğœƒ(ağ‘¡)|hğ‘¡)||ğœ‹ref(ağ‘¡|hğ‘¡)]\\n]ï¸ƒ\\n(3)\\nClassical RLHF has used policy gradient type of algorithms, such as PPO Schulman et al. (2017),\\nhowever, they are complex and require online data, which can be costly/dangerous to collect au-\\ntonomously in the agent setting. While PPO has shown some success in prior web agent applications\\nNakano et al. (2021). The issues above largely make the approach not practical for general web tasks,\\nbeyond information retrieval. In this work we utilize some recent alternatives, outlined below.\\n3.2.1. Reinforced Fine-Tuning\\nReinforced fine-tuning (RFT) algorithms Gulcehre et al. (2023); Singh et al. (2024); Yuan et al.\\n(2023); Zelikman et al. (2022) have grown in popularity due to their simplicity and scalability. These\\nmethods aggregate data and filter out the sub-optimal samples based on some reward model or\\na verifier to construct a growing dataset of high-quality trajectoriesğ’Ÿ. Given this dataset and a\\nparameterized modelğœ‹ğœƒ we can carry out standard supervised fine-tuning (SFT):\\nâ„’(ğœ‹ğœƒ, ğ’Ÿ) =âˆ’Eğ’Ÿ\\n[ï¸ƒ ğ‘‡âˆ‘ï¸\\nğ‘¡=1\\nlog ğœ‹ğœƒ(ağ‘¡|hğ‘¡)\\n]ï¸ƒ\\n(4)\\nIn this objective the divergence penalty is only applied implicitly by limiting the number of training\\nrounds. While simple and relatively successful, empirically these methods tend to under-perform\\nstandard RL and alternatives Dubois et al. (2024); Setlur et al. (2024b); Tajwar et al. (2024) in the\\ntext generation domain, particularly in reasoning. We largely observe similar empirical results, and\\nwe use these methods mostly as baselines to build intuition.\\n3.2.2. Direct Preference Optimization\\nDirect Preference Optimization (DPO) Rafailov et al. (2023) is an offline RL Levine et al. (2020)\\nalternative to the classical RLHF optimization pipeline. It is a suitable algorithm for agent fine-tuning,\\nas it can use fully offline data and does not require online rollouts. The original formulation in the\\npure text generation setting considers feedback of pairwise comparisons(h, ağ‘¤, ağ‘™), wheres is a single\\nprompt andağ‘¤ and ağ‘™ are two responses withağ‘¤ â‰»ağ‘™ indicating thatağ‘¤ is preferred overağ‘™. The\\nDPO objective then minimizes the following loss:\\nâ„’DPO(ğœ‹ğœƒ; ğ’Ÿ) =âˆ’E(h,ağ‘¤,ağ‘™)âˆ¼ğ’Ÿ\\n[ï¸‚\\nlog ğœ\\n(ï¸‚(ï¸‚\\nğ›½ log ğœ‹ğœƒ(ağ‘¤|hğ‘¤)\\nğœ‹ref(ağ‘¤|hğ‘¤)\\n)ï¸‚\\nâˆ’\\n(ï¸‚\\nğ›½ log ğœ‹ğœƒ(ağ‘™|hğ‘™)\\nğœ‹ref(ağ‘™|hğ‘™)\\n)ï¸‚)ï¸‚]ï¸‚\\n(5)\\nWhile the algorithm was developed in a bandit setting Hejna et al. (2024); Rafailov et al. (2024)\\nhave extended it to multi-turn settings with preferences over over trajectories. In our setting, we can\\ndirectly utilize this objective as:\\nâ„’T-DPO(ğœ‹ğœƒ; ğ’Ÿ) =âˆ’E(ğœğ‘¤,ğœğ‘™)âˆ¼ğ’Ÿ\\nâ¡\\nâ£log ğœ\\nâ›\\nâ\\nâ›\\nâ\\n|ğœğ‘¤|âˆ‘ï¸\\nğ‘¡=0\\nğ›½ log ğœ‹ğœƒ(ağ‘¤\\nğ‘¡ |hğ‘¤\\nğ‘¡ )\\nğœ‹ref(ağ‘¤\\nğ‘¡ |hğ‘¤\\nğ‘¡ )\\nâ\\nâ  âˆ’\\nâ›\\nâ\\n|ğœğ‘™|âˆ‘ï¸\\nğ‘¡=0\\nğ›½ log ğœ‹ğœƒ(ağ‘™\\nğ‘¡|hğ‘™\\nğ‘¡)\\nğœ‹ref(ağ‘™\\nğ‘¡|hğ‘™\\nğ‘¡)\\nâ\\nâ \\nâ\\nâ \\nâ¤\\nâ¦ (6)\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-15T00:07:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-15T00:07:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2408.07199v1.pdf', 'total_pages': 22, 'page': 7, 'page_label': '8'}, page_content='Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents\\nFigure 3: Success rate of different approaches on the WebShop Yao et al. (2022) task. All models\\nare based on xLAM-v0.1-r Zhang et al. (2024c). RFT and DPO over xLAM-v0.1-r demonstrate\\nimprovements in performance from 28.6% to 31.3% and 37.5% respectively. However, these methods\\nstill lag behind average human performance of 50.0%. Our approach, Agent Q + MCTS achieves a\\nsignificant gain (76.57% relative improvement) over the base model, outperforming average human\\nperformance on WebShop with a success rate of50.5%.\\nOne bottleneck for the practical deployment of the algorithm is the need for a reference modelğœ‹ref\\nduring optimization, which requires more computational resources. Instead in our settings, we slightly\\nmodify the algorithm using an off-policy replay buffer, which aggregates trajectory data, as well as\\nlikelihoods of the generated actions. During the optimization step, we sample tuples of trajectories\\nand the corresponding likelihoods under the data generation (reference) density, which eliminates\\nthe need for a separate reference model.\\n4. Preliminary Approach With Outcome Supervision\\nIn this section we will outline preliminary experimental results, which will build the base understand-\\ning for our further experiments. We use the AgentOhana xLAM-v0.1-r model Zhang et al. (2024c),\\nwhich is a fine-tune of a pre-trained Mixtral-8x7B-Instruct-v0.1 model Jiang et al. (2024) on a mix of\\nagentic applications, including WebShop SFT data. We also incorporate the same agent configuration\\n1 specified by the AgentLite Liu et al. (2024) work to ensure a fair comparison between our fine-tuned\\nmodel and the xLAM base model performance. We evaluate all approaches on the WebShop environ-\\nment Yao et al. (2022), where the agent needs to find particular products by browsing a simulated\\nweb shop. The environment comes with a set of 12,087 pre-defined tasks (corresponding to specific\\nproducts to find), which we split into a train set of 11,000 tasks, which we use for further agent\\nfine-tuning and a set of 1,087 held-out tasks, which we use for zero-shot evaluation. We show success\\nrates (exact product match) for different approaches in Fig. 3. The base xLAM-v0.1-r model achieves\\nsuccess rate of 28.6% on the test tasks. All other methods are based on outcome-based supervision\\n1https://github.com/SalesforceAIResearch/xLAM\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-15T00:07:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-15T00:07:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2408.07199v1.pdf', 'total_pages': 22, 'page': 8, 'page_label': '9'}, page_content='Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents\\nonly, depending on whether a particular attempt was successful or not. We see that further RFT\\ntraining, using a STaR-like algorithm Zelikman et al. (2022) on the trajectory level, as outlined in\\nSec. 3.2.1, achieves success rate of 31.3%, which is a small improvements of 2.7% over the initial\\nmodel. This is not surprising since the base model is already trained as an agent on the environment\\nwith supervised fine-tuning on demonstrations. Our next experiment fine-tunes the base model using\\nthe trajectory-level DPO algorithm, as outlined in Eq. 6 in Sec. 3.2.2 using successful trajectories as\\npreferred over failed ones. This approach also uses only outcome-level supervision, but unlike the\\nRFT baseline can utilize failed trajectories as well, which improves the agent performance by 9.3%\\nover RFT agent to 40.6% success rate. We also evaluate this model with beam search for the action\\ngeneration, which can be considered a form of planning the horizon of a single environment action\\n(which still consists of multiple simple actions) Rafailov et al. (2023), but it only yields marginal\\nimprovement over the base model. These findings match results on reasoning for math problems\\nPang et al. (2024) and some recent approaches that also apply DPO to agent applications Song et al.\\n(2024); Xi et al. (2024).\\nDespite the additional reinforcement learning training, our agents are still not able to match the\\naverage human performance on this environment. We identify that one of the core failure modes of\\nthe DPO policy is that it executes a greedy search when looking for matches to the product query.\\nFor example, for every search query, the WebShop environment yields a number of pages of results.\\nHowever, we find that the model nearly always greedily searches for the best matching item in the\\nfirst page of results rather than using the \"[NEXT]\" and \"[PREV]\" buttons to navigate between pages,\\nessentially deploying a weak exploration strategy.\\n5. Agent Search\\nAs we discovered in the previous section, while training based on outcome supervision with DPO\\nyields meaningful improvement, the model is still not able to match human performance due to\\nitâ€™s limited exploration. In this section we will explore endowing the agent with additional search\\ncapability via MCTS.\\n5.1. Monte-Carlo Tree Search Over Web-Pages\\nThe Monte Carlo Tree Search (MCTS) algorithm Kocsis and SzepesvÃ¡ri (2006) employed in this\\nwork follows closely the one in Hao et al. (2023) and consists of four phases: selection, expansion,\\nsimulation, and backpropagation. Each phase plays a critical role in balancing exploration and\\nexploitation while iteratively refining the policy.\\nWe formulate the web agent execution as tree search over web-pages. The state is represented as\\ndescribed in Section 3.1 and consist of the summary of the agentâ€™s history and the DOM tree of\\nthe current web-page. Unlike board games, such as Chess or Go Silver et al. (2017b) the complex\\nweb-agent action space we use is open-format and variable. Instead we will use the base model as an\\naction-proposal distribution and sample a fixed amount of possible actions at each node (web-page).\\nOnce we select and execute an action in the browser we traverse the next web-page, which together\\nwith the updated history becomes the new node.\\n5.1.1. Action Selection With AI Process Supervision\\nThe selection phase uses the Upper Confidence Bound (UCB1) formulation of MCTS also used by Hao\\net al. (2023) to select nodes which aims to balance exploration and exploitation. With some abuse of\\nnotation we will also denote the agent state withhğ‘¡. We consider the value functionğ‘„(hğ‘¡, a) which\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-15T00:07:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-15T00:07:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2408.07199v1.pdf', 'total_pages': 22, 'page': 9, 'page_label': '10'}, page_content='Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents\\nFigure 4: The policy proposesK actions at every step during inference time search. The critic, also\\ninitialized as the same base LLM model used by the policy, ranks the actions proposed by the policy.\\nThis ranking is used to guide node selection after expansion and used to construct preference pairs\\nduring policy training.\\nrepresents the estimated value (chance of success) represents the estimated value of taking actiona\\nin the statehğ‘¡. At each new nodehğ‘¡ we sampleğ¾ proposal actions from the base modela1\\nğ‘¡ , . . . ,ağ¾\\nğ‘¡ .\\nWe initialize all valuesğ‘„(hğ‘¡, ağ‘–\\nğ‘¡), ğ‘–= 1, . . . , ğ¾to zero. The web-based environment does not provide\\nintermediate rewards to guide the search, so we incorporate AI-based critique to provide process\\nsupervision at the step level to guide the exploration process. We use the base model to produce a\\nfeedback score for each action by asking it to rank the generated actions by its perceived utility in\\nhelping the agent complete the user task.\\nWe query the feedback model for multiple iterations, each time removing the best action selected from\\nthe previous iteration from the list, until we have a full ranking of all actions. The full AI feedback\\nprocess is demonstrated in Figure 4. After the initial selection, we select the actions to explore based\\non the standard MCTS UCB1 formulation:\\na*\\nğ‘¡ = arg max\\na1\\nğ‘¡ ,...,ağ¾\\nğ‘¡\\n[ï¸ƒ\\nğ‘„(hğ‘¡, a) +ğ‘exp Â·\\nâˆšï¸ƒ\\nlog ğ‘(hğ‘¡)\\n1 +ğ‘(hğ‘¡+1)\\n]ï¸ƒ\\n, (7)\\nwhere ğ‘(hğ‘¡) is the visitation frequency of statehğ‘¡, andğ‘exp is an exploration constant. For each\\nrollout added to the tree, we start at the root node and follow the child states that maximize the\\nUCB1 score until we reach a leaf node. This process is repeated for each tree/prompt in the batch.\\n5.1.2. Expansion and Backtracking\\nBased on the preceding section, we select and execute an action in the browser environment to reach\\na new node (page). Beginning from the selected state nodeâ€™s trace, we roll out the trajectory using\\nthe current policyğœ‹ğœƒ until a terminal state is reached. The environment returns a reward at the\\nend of the trajectory,ğ‘…, whereğ‘… = 1if the agent was successful andğ‘… = 0otherwise. We then\\nbackpropagate this reward by updating the values of each node bottom up from the leaf node to the\\nroot as follows:\\nğ‘„(hğ‘¡, ağ‘–\\nğ‘¡) â†ğ‘„(hğ‘¡, ağ‘–\\nğ‘¡)ğ‘(hğ‘¡, ağ‘–\\nğ‘¡) +ğ‘…\\nğ‘(hğ‘¡, ağ‘–\\nğ‘¡) + 1\\nğ‘(hğ‘¡, ağ‘–\\nğ‘¡) â†ğ‘(hğ‘¡, ağ‘–\\nğ‘¡) + 1\\n(8)\\nEach state node tracks two values:ğ‘„(hğ‘¡, ağ‘–\\nğ‘¡), the average reward for passing through statehğ‘¡ and\\nchoosing actionağ‘–\\nğ‘¡, andğ‘(hğ‘¡, ağ‘–\\nğ‘¡), the number of times this state action pair was visited during search\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-15T00:07:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-15T00:07:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2408.07199v1.pdf', 'total_pages': 22, 'page': 10, 'page_label': '11'}, page_content='Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents\\nAlgorithm 1MCTS Guided Direct Preference Optimization\\nInput: ğœ‹ğœƒ0 : initial LLM policy,ğ’Ÿğ‘‡ : dataset of tasks the agent must complete in the environment,ğ‘:\\nnumber of iterations,ğµ: number of samples per iteration,ğ‘‡: MCTS tree depth,â„¬: replay buffer,\\nğœƒthreshold: value threshold in (10),ğ¾: number of actions to sample for MCTS\\nOutput: ğœ‹ğœƒğ‘ , the trained LLM policy\\nfor ğ‘– = 1to ğ‘ do\\nğœ‹ref â†ğœ‹ğœƒğ‘–, ğœ‹ğœƒğ‘– â†ğœ‹ğœƒğ‘–âˆ’1\\nSample a batch ofğµ tasks fromğ’Ÿğ‘‡\\nfor each task in batchdo\\nInitialize the root nodeh0\\nfor ğ‘¡ = 1to ğ‘‡ do\\nSelection: Traverse tree from the root node to a leaf node using tree policy (UCB1; 7)\\nTrajectory Rollout: From the selected nodeâ€™s trace, roll out the trajectory using\\nğœ‹ğœƒğ‘– until a terminal state is reached\\nBackpropagation: Backpropagate the value estimate bottom-up (8)\\nend for\\nCollect trajectories from rollouts and store them in replay bufferâ„¬\\nend for\\nConstruct preference pairsğ’Ÿğ‘ƒ = {(hğ‘¡, ağ‘¤\\nğ‘¡ , ağ‘™\\nğ‘¡)}ğ‘‡âˆ’1\\nğ‘¡=1 where hğ‘¡ âˆ¼ğ’Ÿğ‘ƒ . For each node at step level\\nğ‘¡, compare each pair of child nodes, and construct the pair of generated actions(ağ‘¤, ağ‘™) if the\\nvalues of taking the action,|ğ‘„(hğ‘¡, ağ‘¤) âˆ’ğ‘„(hğ‘¡, ağ‘™)|> ğœƒthreshold, whereğ‘„(hğ‘¡, ağ‘¤) and ğ‘„(hğ‘¡, ağ‘™) are\\ncomputed using (10)\\nOptimize LLM policyğœ‹ğœƒğ‘– using DPO objective in Eq. (5) withğ’Ÿğ‘ƒ and ğœ‹ref\\nend for\\n(andğ‘(hğ‘¡) =âˆ‘ï¸€ğ¾\\nğ‘–=1 ğ‘(hğ‘¡, ağ‘–\\nğ‘¡)). The backpropogation updates correctly maintain these values.\\n5.2. Improving Zero-Shot Performance with Reinforcement Learning\\nTraining large foundation models with offline Snell et al. (2022) or off-policy Chebotar et al. (2023)\\nreinforcement learning at scale has still remained challenging. At the same time online (on-policy)\\nreinforcement learning Ouyang et al. (2022); Stiennon et al. (2022) is not scalable to real interactive\\nenvironments. Instead, we follow a line of recent works, which apply the DPO algorithm Rafailov\\net al. (2023, 2024) at the step level in multi-step reasoning problems in mathematical domains Chen\\net al. (2024); Hwang et al. (2024); Lai et al. (2024b); Lu et al. (2024); Setlur et al. (2024b); Xie\\net al. (2024); Zhang et al. (2024f). Our approach is most similar to Chen et al. (2024); Xie et al.\\n(2024); Zhang et al. (2024f) who also use the branching nature of tree search to produce step-level\\npreference pairs. We will also use this approach in our setting due to its simplicity, scalability and\\nprior success in smaller scale (non-interactive) reasoning applications.\\nWe will generate a dataset of preference pairsğ’«= {hğ‘¡, ağ‘¤\\nğ‘¡ , ağ‘™\\nğ‘¡}where we make sure both actions\\nwere explored. We then optimize the DPO objective in Eq. 5 on the node level. We will leverage a\\ntheoretical result below to guide the construction of these preferences. We can make a number of\\nmodifications to Theorem 6.1 from Setlur et al. (2024b) to incorporate the interactive nature of the\\nweb environment dynamics to obtain the following result:\\nTheorem 1. Consider a policy that optimizes the objective in Eq. 3 on trajectories generated byğœ‹ref\\nand that at each nodehğ‘¡ we have preferences generated accordingly toğ‘(ağ‘¤\\nğ‘¡ â‰»ağ‘™\\nğ‘¡|hğ‘¡) âˆğœ(ğ‘„(hğ‘¡, ağ‘¤\\nğ‘¡ ) âˆ’\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-15T00:07:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-15T00:07:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2408.07199v1.pdf', 'total_pages': 22, 'page': 11, 'page_label': '12'}, page_content='Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents\\nğ‘„(hğ‘¡, ağ‘™\\nğ‘¡)), then the policy which optimizes the DPO objective in Eq. 5 is identical to the optimal RL policy\\nğœ‹*(a|hğ‘¡) âˆğœ‹ref(a|hğ‘¡) exp (ğ‘„(hğ‘¡, a)/ğ›½) (9)\\nProof. The proof follows directly from the proof of Theorem 6.1 in Setlur et al. (2024b) and the\\ncontrol as inference arguments in Levine (2018); Rafailov et al. (2024).\\nThat is, we can approximate the optimal RL policy if we generate preferences under the optimal\\nvalue function (or an approximation thereof). Since the outcome success provides limited supervision\\nwe also incorporate process supervision through the AI feedback as outlined in Section 5.1.1. We\\ninterpret the ranking of possible actions by the model to be driven by an implicit value function.\\nSimilar semantics was used in Koh et al. (2024), where GPT-4 was used as a zero-shot value function,\\nwhile here we ask the model to instead reason over the given potential actions and provide rankings\\ninstead. This self-rewarding approach has shown promise in the RLHF setting Yuan et al. (2024) and\\nwe utilize it for our agent setting as well. Under this formulation, we compute the state-action value\\nas an average:\\nğ‘„(hğ‘¡, ağ‘–\\nğ‘¡) =ğ›¼ Ëœğ‘„(hğ‘¡, ağ‘–\\nğ‘¡) + (1âˆ’ğ›¼) Ë†ğ‘„(hğ‘¡, ağ‘–\\nğ‘¡) (10)\\nwhere Ëœğ‘„(hğ‘¡, ağ‘–\\nğ‘¡) is the empirical value estimated through MCTS backpropagation andË†ğ‘„(hğ‘¡, ağ‘–\\nğ‘¡) is a\\nvalueestimatebasedontherankingoftheaction ağ‘–\\nğ‘¡ bytheprocesssupervisionAImodel. Wethencreate\\npreferences over pairs of actions which are above a certain value threshold|ğ‘„(hğ‘¡, ağ‘¤\\nğ‘¡ ) âˆ’ğ‘„(hğ‘¡, ağ‘™\\nğ‘¡)|â‰¥\\nğœƒthreshold. The full outline of our RL approach is shown in Algorithm 1.\\n5.3. Full WebShop Results\\nThe full range of results and baselines is shown in Figure 3. We see that equipping the agent with\\nsearchcapabilities attesttimesignificantlyboostsuccessrates from28.6%to48.4% whenusingMCTS\\non top of the base xLAM-v0.1-r model, approaching close to the average human performance of 50.0%\\nand significantly out-performing the zero-shot performance of the DPO model trained with outcome\\nsupervision. We further fine-tune the base model using the approach outlined in Algorithm 1, which\\nyields an improvement of 0.9% over the base DPO model. Using MCTS on top of the trained Agent Q\\nmodel further improves performance to 50.5% slightly out-performing the average human success\\nrates. We find that the ability to search at test time is a significant paradigm shift from zero-shot\\nagents, even with significant RL training. Furthermore, while dense-level supervision improves over\\npurely outcome-based one, the improvement is modest on WebShop. This is because the environment\\nrequires relatively short trajectories, and the model is capable to learn credit assignment purely from\\noutcome supervision. We will further explore more complex real world environment, which requires\\nlonger-range credit assignment.\\n6. Scaling To Real World Websites\\nIn this section we will investigate scaling the Agent Q framework to real use cases on live websites, in\\nparticular bookings on OpenTable. We carried out initial experiments with the xLAM-v0.1-r model,\\nwhich proved to weak for the task achieving an initial success rate of 0.0%. Instead we shifted to the\\nLLaMa 70B Instruct model, which was able to achive some non-trivial initial success.\\n6.1. The OpenTable Environment\\nIn OpenTable, the agent is tasked with booking a restaurant reservation for a user. The agent must\\nfind a restaurant page on the OpenTable site, look for a reservation at a certain date and time, choose\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-15T00:07:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-15T00:07:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2408.07199v1.pdf', 'total_pages': 22, 'page': 12, 'page_label': '13'}, page_content='Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents\\nFigure 5: At the end of a trajectory, a GPT-4-V evaluator is called to provide feedback on the agentâ€™s\\nperformance given the final observation and action history to determine the success score. The model\\nis prompted with a condensed execution history of the trajectory and the screenshot of the final state.\\nThe success metric is a binary 0/1 value.\\nseating options that align with a userâ€™s preference and submit the user contact information to complete\\nthe task successfully. Since OpenTable is a live environment and is difficult to programatically measure\\nmetrics for, we use a language model, GPT-4-V to collect rewards for each trajectory, based on the\\nfollowing metrics: (1) date and time set correctly, (2) party size set correctly, (3) user information\\nentered correctly, and (4) clicked complete reservation. The task is marked as completed if each\\nof the above constraints are satisfied. The outcome supervision setup is shown in Figure 5. We\\nexperimented with using LLaMa 70B for outcome supervision as well, but discovered that vision\\ncapabilitiessignificantlyimprovethesuccessclassificationaccuracy(asmeasuredbyhumanvalidation).\\nAt the time of writing no open source vision-language model of sufficient capability was available,\\nhence we opted to use GPT-4-V. We believe that as more open-source multi-modal models become\\navailable we can switch to a fully self-supervised pipeline.\\nTo generate queries for the OpenTable benchmark dataset, we programatically generate a diverse set\\nof user queries by combining the restaurant name, desired date and time, and user information.\\nNavigating on live websites pose a wide variety of challenges. For example, consider that the user\\nspecifies a restaurant in a different city than the location the browser is initialized in, the model will\\nhave to take extra steps to find the restaurant. Further, if the exact user requested date and time are\\nnot available, the model may have to choose the closest available reservation slot. Lastly, if there are\\npreferences, such as indoor or outdoor seating options that the model is presented with, the desired\\nbehavior is to interact with the user to determine the best course of action. OpenTable presents a\\ncomplex set of challenges for web navigation agents; the number of steps required to complete the\\ntask is on average 13.9 steps, over double the average number of steps for Webshop, 6.8.\\nFor the observation space for this environment, we design an intermediate state representation that\\ncrawls the raw HTML content of a website to retrieve relevant visual components, and highlight\\ninteractive elements to the model. The agent is allowed the actions, \"CLICK [ID]\", \"GOTO [URL]\",\\n\"TYPE [ID] [TEXT]\", \"SUBMIT [ID]\", \"CLEAR [ID]\", \"SCROLL [UP/DOWN]\", and \"ASK USER HELP\".\\nFor OpenTable experiments, we use the LLaMA-3-70B-Instruct model as the initial policy. We find that\\nthe superior reasoning abilities of this class of model is required for effective task completion, which\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-15T00:07:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-15T00:07:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2408.07199v1.pdf', 'total_pages': 22, 'page': 13, 'page_label': '14'}, page_content='Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents\\nFigure 6: Success rate of different approaches on OpenTable. All models unless otherwise stated\\nare based on LLaMA-3-70B-Instruct Touvron et al. (2023). Using DPO and RFT with MCTS further\\nimproves performance from 18.6% to 71.8% and 84.3% respectively. We show that Agent Q in\\nitself achieves 81.7% and Agent Q + MCTS significantly outperforms all other techniques, with a\\nperformance of95.4% on OpenTable.\\nis necessary to produce the diverse success and failure trajectories required to effectively improve the\\npolicy.\\n6.2. Results On OpenTable\\nThe base xLAM-v0.1-r model achieves a success rate of 0.0%, largely from failing to follow instructions\\nforthegeneralwebnavigationinstructionsusedforlivewebsites, contrarytothesimplifiedobservation\\nand action space used in WebShop. We instead initialize the base policy with the LLaMa-3 70B Instruct\\nmodel,whichachievesazero-shotsuccessrateof18.6%. WedoasingleroundofRFTon600successful\\ntrajectories which improves the success rate to 67.2% already out-performing the the GPT-4o model\\nzero-shot performance with a success rate of 62.6%. For all other baselines we adopt the RFT model\\nas the reference policy, due to the relatively low success rate of original LLaMa 3 70B Instruct model.\\nIn this environment, training with outcome-supervision only DPO further improves performance by\\n4.6% to 71.8% but significantly under-performs the full Agent Q pipeline which achieves a zero-shot\\nsuccess rate of 81.7% We hypothesizes that this is due to the fact that OpenTable is a significantly\\nmore challenging environment, which requires almost twice as many steps to complete as WebShop,\\nso the agent benefits from fine-grained supervision and credit assignment. We further ablate the\\nrole of the intermediate AI feedback process supervision during training as outlined in Eq. 10 and\\nuse MCTS with online Q values computed from outcome rewards only. This setting still outperforms\\ntraining with trajectory-level DPO (75.2% versus 71.8%) likely due to the more fine-grained credit\\nassignment that the branching tree search provides to the agent. However, zero-shot performance\\nis still meaningfully worse than using intermediate process-level supervision and the full Agent Q\\nachieves 6.5% higher success rate at 81.7%.\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-15T00:07:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-15T00:07:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2408.07199v1.pdf', 'total_pages': 22, 'page': 14, 'page_label': '15'}, page_content='Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents\\nSimilar to the WebShop experiment we see a step level increase in capability from allowing the\\nmodel to search at inference time, with the base RFT model achieving 84.3% success with MCTS,\\noutperforming the Agent Q zero-shot performance of 81.7% success. However, if we carry out\\nadditional MCTS search using the Agent Q model as the base policy we achieve a significant 95.4%\\nsuccess rate.\\n7. Discussion\\nIn this work we developed algorithms for autonomous improvement of web-agents with limited human\\nsupervision. While most prior works build frameworks around existing models without additional\\ntraining, we specifically seek to fine-tune pre-trained models for web navigation tasks based on\\nsynthetic reasoning and search data. While we achieve significant improvement in model capabilities\\non our target domain, many research questions remain.\\nDesign of reasoning algorithms.The core challenge for our web agents is the weak reasoning\\ncapabilities, which limit the agentâ€™s exploration and search strategy. In our approach we used process-\\nlevel supervision from a separate critic model, which we prompt to rank possible agent actions. This\\nis in contrast to works in mathematical reasoning where PRMs are usually trained to classify the\\ncorrectness of individual steps Lightman et al. (2023), while other agent works Koh et al. (2024)\\nhave prompted models as zero-shot value functions. Furthermore, while we spent significant effort in\\ntraining the agent policy, we maintain a frozen critic, which would likely also benefit from additional\\nfine-tuning. We defer exploration of these design choices to further work.\\nChoice of search algorithm.We used MCTS search due to the approachâ€™s prior success in mathemat-\\nical and code reasoning tasks. However, agent models executing MCTS on live environments might\\nrequire significant number of risky interactions and a different search strategy might be more suitable.\\nRecent works such as Gandhi et al. (2024); Lehnert et al. (2024) have even suggested directly learning\\nto optimally search and explore in reasoning tasks using meta-reinforcement learning. We believe\\nthis is a promising research direction for autonomous agents, which we will pursue in further work.\\nDiscrepancy between zero-shot vs search results.Similar to some recent works that focus on code\\nand reasoning, we observe significant gap between zero-shot agent performance and performance of\\nthe agent equipped with search capabilities Brown et al. (2024); Snell et al. (2024). Investigating\\nthese trade-offs at scale and the potential effect of different search/optimization approaches.\\nOnline safety and interaction.The design of agent Q allows for largely autonomous exploration,\\nself-evaluation and improvement with limited human intervention. However, the agent might make a\\nsignificant number of mistakes in itâ€™s search process which might be difficult to fix/reverse, especially\\nfor safety-critical online transactions, such as communications/email, payments, filings etc. This limits\\nthe scope of websites that Agent Q can be safely deployed and we might require additional safety\\ncritics and human-in-the-loop training setups.\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-15T00:07:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-15T00:07:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2408.07199v1.pdf', 'total_pages': 22, 'page': 15, 'page_label': '16'}, page_content='Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents\\nReferences\\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman,\\nDiogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report.\\narXiv preprint arXiv:2303.08774, 2023.\\nRohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan\\nSchalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, et al. Gemini: A family of highly capable\\nmultimodal models.arXiv preprint arXiv:2312.11805, 1, 2023.\\nAnthropic. Introducing the next generation of claude, 2024. URL\\nIntroducingthenextgenerationofClaude.\\nHao Bai, Yifei Zhou, Mert Cemri, Jiayi Pan, Alane Suhr, Sergey Levine, and Aviral Kumar. Digirl:\\nTraining in-the-wild device-control agents with autonomous reinforcement learning, 2024.\\nYuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones,\\nAnna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson,\\nChristopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson,\\nEthan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile\\nLukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova\\nDasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El\\nShowk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan,\\nTristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph,\\nSam McCandlish, Tom Brown, and Jared Kaplan. Constitutional ai: Harmlessness from ai feedback,\\n2022.\\nMaciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi,\\nJoanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, and Torsten Hoefler. Graph\\nof thoughts: Solving elaborate problems with large language models.Proceedings of the AAAI\\nConference on Artificial Intelligence, 38(16):17682â€“17690, March 2024. ISSN 2159-5399. doi:\\n10.1609/aaai.v38i16.29720. URL http://dx.doi.org/10.1609/aaai.v38i16.29720.\\nBradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le, Christopher RÃ©, and Azalia\\nMirhoseini. Large language monkeys: Scaling inference compute with repeated sampling, 2024.\\nURL https://arxiv.org/abs/2407.21787.\\nNoam Brown and Tuomas Sandholm. Superhuman ai for multiplayer poker.Science, 2019.\\nYevgen Chebotar, Quan Vuong, Alex Irpan, Karol Hausman, Fei Xia, Yao Lu, Aviral Kumar, Tianhe Yu,\\nAlexander Herzog, Karl Pertsch, Keerthana Gopalakrishnan, Julian Ibarz, Ofir Nachum, Sumedh\\nSontakke, Grecia Salazar, Huong T Tran, Jodilyn Peralta, Clayton Tan, Deeksha Manjunath, Jaspiar\\nSinght, Brianna Zitkovich, Tomas Jackson, Kanishka Rao, Chelsea Finn, and Sergey Levine. Q-\\ntransformer: Scalable offline reinforcement learning via autoregressive q-functions, 2023.\\nGuoxin Chen, Minpeng Liao, Chengxi Li, and Kai Fan. Step-level value preference optimization for\\nmathematical reasoning, 2024. URLhttps://arxiv.org/abs/2406.10858.\\nWei Chen and Zhiyuan Li. Octopus v2: On-device language model for super agent, 2024.\\nXiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su.\\nMind2web: Towards a generalist agent for the web. InNeurIPS Datasets and Benchmarks Track,\\n2023. URLhttps://openreview.net/forum?id=kiYqbO3wqw.\\n16'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-15T00:07:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-15T00:07:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2408.07199v1.pdf', 'total_pages': 22, 'page': 16, 'page_label': '17'}, page_content='Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents\\nYann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin,\\nPercy Liang, and Tatsunori B. Hashimoto. Alpacafarm: A simulation framework for methods that\\nlearn from human feedback, 2024.\\nKanishk Gandhi, Denise Lee, Gabriel Grand, Muxin Liu, Winson Cheng, Archit Sharma, and Noah D.\\nGoodman. Stream of search (sos): Learning to search in language, 2024. URLhttps://arxiv.\\norg/abs/2404.03683.\\nJonathan Gray, Adam Lerer, Anton Bakhtin, and Noam Brown. Human-level performance in no-press\\ndiplomacy via equilibrium search, 2021.\\nCaglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek\\nSharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, Wolfgang Macherey, Arnaud\\nDoucet, Orhan Firat, and Nando de Freitas. Reinforced self-training (rest) for language modeling,\\n2023.\\nIzzeddin Gur, Hiroki Furuta, Austin V Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and\\nAleksandra Faust. A real-world webagent with planning, long context understanding, and program\\nsynthesis. InICLR, 2024. URLhttps://openreview.net/forum?id=9JQtrumvg8.\\nShibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu.\\nReasoning with language model is planning with world model, 2023.\\nHongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and\\nDong Yu. Webvoyager: Building an end-to-end web agent with large multimodal models.ArXiv,\\n2024. URLhttps://api.semanticscholar.org/CorpusID:267211622.\\nJoey Hejna, Rafael Rafailov, Harshit Sikchi, Chelsea Finn, Scott Niekum, W. Bradley Knox, and Dorsa\\nSadigh. Contrastive preference learning: Learning from human feedback without reinforcement\\nlearning. InThe Twelfth International Conference on Learning Representations, 2024. URLhttps:\\n//openreview.net/forum?id=iX1RjVQODj.\\nSamuel Holt, Max Ruiz Luyten, and Mihaela van der Schaar. L2mac: Large language model automatic\\ncomputer for extensive code generation, 2024.\\nWenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan\\nWang, Yuxiao Dong, Ming Ding, and Jie Tang. Cogagent: A visual language model for gui agents.\\nArXiv, 2023.\\nWenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot\\nplanners: Extracting actionable knowledge for embodied agents, 2022a.\\nWenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan\\nTompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah Brown, Tomas Jackson, Linda\\nLuu, Sergey Levine, Karol Hausman, and Brian Ichter. Inner monologue: Embodied reasoning\\nthrough planning with language models, 2022b. URLhttps://arxiv.org/abs/2207.05608.\\nJan Humplik, Alexandre Galashov, Leonard Hasenclever, Pedro A. Ortega, Yee Whye Teh, and Nicolas\\nHeess. Meta reinforcement learning as task inference, 2019. URLhttps://arxiv.org/abs/1905.\\n06424.\\nHyeonbin Hwang, Doyoung Kim, Seungone Kim, Seonghyeon Ye, and Minjoon Seo. Self-explore to\\navoid the pit: Improving the reasoning capabilities of language models with fine-grained rewards,\\n2024.\\n17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-15T00:07:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-15T00:07:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2408.07199v1.pdf', 'total_pages': 22, 'page': 17, 'page_label': '18'}, page_content='Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents\\nAlbert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford,\\nDevendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel,\\nGuillaume Bour, Guillaume Lample, LÃ©lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux,\\nPierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, ThÃ©ophile\\nGervet, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, and William El Sayed. Mixtral of experts,\\n2024.\\nCarlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik\\nNarasimhan. Swe-bench: Can language models resolve real-world github issues?, 2024.\\nBarretZophJohnSchulman, ChristinaKim, JacobHilton, JacobMenick, JiayiWeng, JuanFelipeCeron\\nUribe, Liam Fedus, Michael Pokorny Luke Metz, Rapha Gontijo Lopes, Shengjia Zhao, Arun\\nVijayvergiya, Eric Sigler, Adam Perelman, Chelsea Voss, Mike Heaton, Joel Parish, Dave Cummings,\\nRajeev Nayak, Valerie Balcom, David Schnurr, Tomer Kaftan, Chris Hallacy, Nicholas Turley, Noah\\nDeutsch, Vik Goel, Jonathan Ward, Aris Konstantinidis, Wojciech Zaremba, Long Ouyang, Leonard\\nBogdonoff, Joshua Gross, David Medina, Sarah Yoo, Teddy Lee, Ryan Lowe, Dan Mossing, Joost\\nHuizinga, Roger Jiang, Carroll Wainwright amd Diogo Almeida, Steph Lin, Marvin Zhang, Kai Xiao,\\nKatarina Slama, Steven Bills, Alex Gray, Jan Leike, Jakub Pachocki, Phil Tillet, Shantanu Jain, Greg\\nBrockman, Nick Ryder, Alex Paino, Qiming Yuan, Clemens Winter, Ben Wang, Mo Bavarian, Igor\\nBabuschkin, Szymon Sidor, Ingmar Kanitscheider, Mikhail Pavlov, Matthias Plappert, Nik Tezak,\\nHeewoo Jun, William Zhuk, Vitchyr Pong, Lukasz Kaiser, Jerry Tworek, Andrew Carr, Lilian Weng,\\nSandhini Agarwal, Karl Cobbe, Vineet Kosaraju, Alethea Power, Stanislas Polu, Jesse Han, Raul Puri,\\nShawn Jain, Benjamin Chess, Christian Gibson, Oleg Boiko, Emy Parparita, Amin Tootoonchian,\\nKyle Kosic, and Christopher Hesse. Introducing chatgpt, 2022. URLhttps://openai.com/blog/\\nchatgpt#OpenAI.\\nLevente Kocsis and Csaba SzepesvÃ¡ri. Bandit based monte-carlo planning. InMachine Learning:\\nECML 2006: 17th European Conference on Machine Learning Berlin, Germany, September 18-22,\\n2006 Proceedings, pages 282â€“293. Springer, 2006.\\nJing Yu Koh, Stephen McAleer, Daniel Fried, and Ruslan Salakhutdinov. Tree search for language\\nagent models, 2024. URLhttps://jykoh.com/search-agents/paper.pdf.\\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\\nlanguage models are zero-shot reasoners, 2022.\\nHanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Hao Yu, Hanchen\\nZhang, Xiaohan Zhang, Yuxiao Dong, and Jie Tang. Autowebglm: Bootstrap and reinforce a large\\nlanguage model-based web navigating agent, 2024a.\\nXin Lai, Zhuotao Tian, Yukang Chen, Senqiao Yang, Xiangru Peng, and Jiaya Jia. Step-dpo: Step-wise\\npreference optimization for long-chain reasoning of llms, 2024b. URLhttps://arxiv.org/abs/\\n2406.18629.\\nLucas Lehnert, Sainbayar Sukhbaatar, DiJia Su, Qinqing Zheng, Paul Mcvay, Michael Rabbat, and\\nYuandong Tian. Beyond a*: Better planning with transformers via search dynamics bootstrapping,\\n2024. URLhttps://arxiv.org/abs/2402.14083.\\nSergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review,\\n2018.\\nSergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial,\\nreview, and perspectives on open problems, 2020.\\n18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-15T00:07:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-15T00:07:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2408.07199v1.pdf', 'total_pages': 22, 'page': 18, 'page_label': '19'}, page_content='Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents\\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike,\\nJohn Schulman, Ilya Sutskever, and Karl Cobbe. Letâ€™s verify step by step, 2023.\\nZhiwei Liu, Weiran Yao, Jianguo Zhang, Le Xue, Shelby Heinecke, Rithesh Murthy, Yihao Feng, Zeyuan\\nChen, Juan Carlos Niebles, Devansh Arpit, Ran Xu, Phil Mui, Huan Wang, Caiming Xiong, and\\nSilvio Savarese. Bolaa: Benchmarking and orchestrating llm-augmented autonomous agents, 2023.\\nZhiwei Liu, Weiran Yao, Jianguo Zhang, Liangwei Yang, Zuxin Liu, Juntao Tan, Prafulla K. Choubey,\\nTian Lan, Jason Wu, Huan Wang, Shelby Heinecke, Caiming Xiong, and Silvio Savarese. Agentlite:\\nA lightweight library for building and advancing task-oriented llm agent system, 2024.\\nZimu Lu, Aojun Zhou, Ke Wang, Houxing Ren, Weikang Shi, Junting Pan, Mingjie Zhan, and Hong-\\nsheng Li. Step-controlled dpo: Leveraging stepwise error for enhanced mathematical reasoning,\\n2024. URLhttps://arxiv.org/abs/2407.00782.\\nTula Masterman, Sandi Besen, Mason Sawtell, and Alex Chao. The landscape of emerging ai agent\\narchitectures for reasoning, planning, and tool calling: A survey, 2024.\\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher\\nHesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-\\nanswering with human feedback.arXiv preprint arXiv:2112.09332, 2021.\\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher\\nHesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou,\\nGretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. Webgpt:\\nBrowser-assisted question-answering with human feedback, 2022.\\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser\\nKelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan\\nLeike, and Ryan Lowe. Training language models to follow instructions with human feed-\\nback. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Ad-\\nvances in Neural Information Processing Systems, volume 35, pages 27730â€“27744. Curran As-\\nsociates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/\\nb1efde53be364a73914f58805a001731-Paper-Conference.pdf.\\nJiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou, Sergey Levine, and Alane Suhr. Autonomous\\nevaluation and refinement of digital agents, 2024.\\nRichard Yuanzhe Pang, Weizhe Yuan, Kyunghyun Cho, He He, Sainbayar Sukhbaatar, and Jason\\nWeston. Iterative reasoning preference optimization, 2024.\\nShuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei\\nHuang, and Huajun Chen. Reasoning with language model prompting: A survey, 2023.\\nRafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea\\nFinn. Direct preference optimization: Your language model is secretly a reward model. InThirty-\\nseventh Conference on Neural Information Processing Systems, 2023. URLhttps://arxiv.org/abs/\\n2305.18290.\\nRafael Rafailov, Joey Hejna, Ryan Park, and Chelsea Finn. Fromğ‘Ÿ to ğ‘*: Your language model is\\nsecretly a q-function, 2024.\\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\\noptimization algorithms, 2017.\\n19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-15T00:07:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-15T00:07:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2408.07199v1.pdf', 'total_pages': 22, 'page': 19, 'page_label': '20'}, page_content='Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents\\nAmrith Setlur, Saurabh Garg, Xinyang Geng, Naman Garg, Virginia Smith, and Aviral Kumar. Rl on\\nincorrect synthetic data scales the efficiency of llm math reasoning by eight-fold, 2024a.\\nAmrith Setlur, Saurabh Garg, Xinyang Geng, Naman Garg, Virginia Smith, and Aviral Kumar. Rl\\non incorrect synthetic data scales the efficiency of llm math reasoning by eight-fold, 2024b. URL\\nhttps://arxiv.org/abs/2406.14532.\\nDavid Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,\\nThomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui,\\nLaurent Sifre, George van den Driessche, Thore Graepel, and Demis Hassabis. Mastering the game\\nof go with deep neural networks and tree search.Nature, 2017a.\\nDavid Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,\\nThomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without\\nhuman knowledge.Nature, 550(7676):354â€“359, 2017b.\\nAvi Singh, John D. Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Xavier Garcia, Peter J. Liu,\\nJames Harrison, Jaehoon Lee, Kelvin Xu, Aaron Parisi, Abhishek Kumar, Alex Alemi, Alex Rizkowsky,\\nAzade Nova, Ben Adlam, Bernd Bohnet, Gamaleldin Elsayed, Hanie Sedghi, Igor Mordatch, Isabelle\\nSimpson, Izzeddin Gur, Jasper Snoek, Jeffrey Pennington, Jiri Hron, Kathleen Kenealy, Kevin\\nSwersky, Kshiteej Mahajan, Laura Culp, Lechao Xiao, Maxwell L. Bileschi, Noah Constant, Roman\\nNovak, Rosanne Liu, Tris Warkentin, Yundi Qian, Yamini Bansal, Ethan Dyer, Behnam Neyshabur,\\nJascha Sohl-Dickstein, and Noah Fiedel. Beyond human data: Scaling self-training for problem-\\nsolving with language models, 2024.\\nCharlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can\\nbe more effective than scaling model parameters, 2024. URLhttps://arxiv.org/abs/2408.03314.\\nCharlie Victor Snell, Ilya Kostrikov, Yi Su, Sherry Yang, and Sergey Levine. Offline rl for natural\\nlanguage generation with implicit language q learning. InThe Eleventh International Conference on\\nLearning Representations, 2022.\\nYifan Song, Da Yin, Xiang Yue, Jie Huang, Sujian Li, and Bill Yuchen Lin. Trial and error: Exploration-\\nbased trajectory optimization for llm agents, 2024.\\nNisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford,\\nDario Amodei, and Paul Christiano. Learning to summarize from human feedback, 2022.\\nTheodore R. Sumers, Shunyu Yao, Karthik Narasimhan, and Thomas L. Griffiths. Cognitive architec-\\ntures for language agents, 2024.\\nFahim Tajwar, Anikait Singh, Archit Sharma, Rafael Rafailov, Jeff Schneider, Tengyang Xie, Stefano\\nErmon, Chelsea Finn, and Aviral Kumar. Preference fine-tuning of llms should leverage suboptimal,\\non-policy data, 2024.\\nYe Tian, Baolin Peng, Linfeng Song, Lifeng Jin, Dian Yu, Haitao Mi, and Dong Yu. Toward self-\\nimprovement of llms via imagination, searching, and criticizing, 2024.\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Can-\\nton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu,\\nBrian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini,\\nRui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Ko-\\nrenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-15T00:07:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-15T00:07:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2408.07199v1.pdf', 'total_pages': 22, 'page': 20, 'page_label': '21'}, page_content='Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents\\nLu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew\\nPoulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael\\nSmith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang\\nKuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan\\nNarang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open\\nfoundation and fine-tuned chat models, 2023.\\nJonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia\\nCreswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process- and\\noutcome-based feedback, 2022.\\nJunyang Wang, Haiyang Xu, Jiabo Ye, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang.\\nMobile-agent: Autonomous multi-modal mobile device agent with visual perception.arXiv, 2024a.\\nPeiyi Wang, Lei Li, Zhihong Shao, R. X. Xu, Damai Dai, Yifei Li, Deli Chen, Y. Wu, and Zhifang Sui.\\nMath-shepherd: Verify and reinforce llms step-by-step without human annotations, 2024b.\\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery,\\nand Denny Zhou. Self-consistency improves chain of thought reasoning in language models, 2023.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le,\\nand Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models.Neural\\nInformation Processing Systems, 2022.\\nZhiyong Wu, Chengcheng Han, Zichen Ding, Zhenmin Weng, Zhoumianze Liu, Shunyu Yao, Tao Yu,\\nand Lingpeng Kong. Os-copilot: Towards generalist computer agents with self-improvement.arXiv,\\n2024. URLhttps://api.semanticscholar.org/CorpusID:265149992.\\nZhiheng Xi, Yiwen Ding, Wenxiang Chen, Boyang Hong, Honglin Guo, Junzhe Wang, Dingwen Yang,\\nChenyang Liao, Xin Guo, Wei He, Songyang Gao, Lu Chen, Rui Zheng, Yicheng Zou, Tao Gui,\\nQi Zhang, Xipeng Qiu, Xuanjing Huang, Zuxuan Wu, and Yu-Gang Jiang. Agentgym: Evolving\\nlarge language model-based agents across diverse environments, 2024. URLhttps://arxiv.org/\\nabs/2406.04151.\\nYuxi Xie, Anirudh Goyal, Wenyue Zheng, Min-Yen Kan, Timothy P. Lillicrap, Kenji Kawaguchi, and\\nMichael Shieh. Monte carlo tree search boosts reasoning via iterative preference learning, 2024.\\nJohn Yang, Carlos E. Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and\\nOfir Press. Swe-agent: Agent-computer interfaces enable automated software engineering, 2024.\\nShunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable\\nreal-world web interaction with grounded language agents, 2022.\\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik R.\\nNarasimhan. Tree of thoughts: Deliberate problem solving with large language models. InNeurIPS,\\n2023a. URLhttps://openreview.net/forum?id=5Xc1ecxO1h.\\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.\\nReact: Synergizing reasoning and acting in language models, 2023b.\\nWeizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar Sukhbaatar, Jing Xu, and\\nJason Weston. Self-rewarding language models, 2024.\\n21'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-15T00:07:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-15T00:07:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2408.07199v1.pdf', 'total_pages': 22, 'page': 21, 'page_label': '22'}, page_content='Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents\\nZheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou,\\nand Jingren Zhou. Scaling relationship on learning mathematical reasoning with large language\\nmodels, 2023.\\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with\\nreasoning. Advances in Neural Information Processing Systems, 35:15476â€“15488, 2022.\\nYuexiang Zhai, Hao Bai, Zipeng Lin, Jiayi Pan, Shengbang Tong, Yifei Zhou, Alane Suhr, Saining Xie,\\nYannLeCun,YiMa,andSergeyLevine. Fine-tuninglargevision-languagemodelsasdecision-making\\nagents via reinforcement learning, 2024.\\nChaoyun Zhang, Liqun Li, Shilin He, Xu Zhang, Bo Qiao, Si Qin, Minghua Ma, Yu Kang, Qingwei Lin,\\nand Saravan Rajmohan. Ufo: A ui-focused agent for windows os interaction.arXiv, 2024a. URL\\nhttps://api.semanticscholar.org/CorpusID:267211622.\\nChi Zhang, Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu.\\nAppagent: Multimodal agents as smartphone users, 2023.\\nChi Zhang, Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. Ap-\\npagent: Multimodalagentsassmartphoneusers. arXiv,2024b. URLhttps://api.semanticscholar.\\norg/CorpusID:262053313.\\nJianguo Zhang, Tian Lan, Rithesh Murthy, Zhiwei Liu, Weiran Yao, Juntao Tan, Thai Hoang, Liangwei\\nYang, Yihao Feng, Zuxin Liu, Tulika Awalgaonkar, Juan Carlos Niebles, Silvio Savarese, Shelby\\nHeinecke, Huan Wang, and Caiming Xiong. Agentohana: Design unified data and training pipeline\\nfor effective agent learning, 2024c.\\nKechi Zhang, Jia Li, Ge Li, Xianjie Shi, and Zhi Jin. Codeagent: Enhancing code generation with\\ntool-integrated agent systems for real-world repo-level coding challenges, 2024d.\\nXuanZhang, ChaoDu, TianyuPang, QianLiu, WeiGao, andMinLin. Chainofpreferenceoptimization:\\nImproving chain-of-thought reasoning in llms, 2024e.\\nXuanZhang, ChaoDu, TianyuPang, QianLiu, WeiGao, andMinLin. Chainofpreferenceoptimization:\\nImproving chain-of-thought reasoning in llms, 2024f. URLhttps://arxiv.org/abs/2406.09136.\\nZhuosheng Zhang and Aston Zhang. You only look at screens: Multimodal chain-of-action agents.\\nArXiv, 2023. URLhttps://api.semanticscholar.org/CorpusID:262053313.\\nAndy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. Language\\nagent tree search unifies reasoning acting and planning in language models, 2024a.\\nShuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue\\nOu, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. Webarena: A realistic web environ-\\nment for building autonomous agents. InICLR, 2024b.\\nYifei Zhou, Andrea Zanette, Jiayi Pan, Sergey Levine, and Aviral Kumar. Archer: Training language\\nmodel agents via hierarchical multi-turn rl, 2024c.\\n22')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4536df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:04:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:04:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2411.00114v1.pdf', 'total_pages': 35, 'page': 0, 'page_label': '1'}, page_content='Project Sid: Many-agent simulations toward AI civilization\\nAltera.AL1\\nscience@altera.al\\nAIagentshavebeenevaluatedinisolationorwithinsmallgroups,whereinteractionsremainlimitedinscope\\nand complexity. Large-scale simulations involving many autonomous agentsâ€”reflecting the full spectrum of\\ncivilizational processesâ€”have yet to be explored. Here, we demonstrate how 10 â€“ 1000+ AI agents behave\\nandprogresswithinagentsocieties. WefirstintroducethePIANO(ParallelInformationAggregationviaNeu-\\nral Orchestration) architecture, which enables agents to interact with humans and other agents in real-time\\nwhile maintaining coherence across multiple output streams. We then evaluate agent performance in large-\\nscale simulations using civilizational benchmarks inspired by human history. These simulations, set within\\na Minecraft environment, reveal that agents are capable of meaningful progressâ€”autonomously developing\\nspecialized roles, adhering to and changing collective rules, and engaging in cultural and religious transmis-\\nsion. These preliminary results show that agents can achieve significant milestones towards AI civilizations,\\nopening new avenues for large-scale societal simulations, agentic organizational intelligence, and integrating\\nAIintohumancivilizations.\\nFigure1: Fromagentarchitecturetoagentcivilization\\n1SeeContributionssectionforcompleteauthorlist.\\n1\\narXiv:2411.00114v1  [cs.AI]  31 Oct 2024'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:04:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:04:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2411.00114v1.pdf', 'total_pages': 35, 'page': 1, 'page_label': '2'}, page_content='1 Introduction\\n1.1 Why should we try to build an AI civilization?\\nFor agents to coexist with us in our own societies, they need to be autonomous and collaborative. In\\nrecent years, advancements in reasoning and decision-making in LLMs have significantly enhanced\\nagentautonomy(52;58;36;45). However,autonomyaloneisinsufficient. AIagentsmustalsocoexist\\nalongside humans and other agents in a human civilization. In this paper, we define a civilization as\\nan advanced society that has achieved a high level of institutional development, which manifests in\\nspecialized roles, organized governance, and advancements in areas like science, art, and commerce.\\nWearguethatcivilizationalprogress-measuredbytheabilityofagentstocoexistandprogressinhuman\\ncivilizations-representstheultimatebenchmarkforAIagentability.\\nInthistechnicalreport,wedescribeourfirsteffortstoimproveandbenchmarkagentabilityinhuman\\ncivilizations. First, weintroducePIANO(ParallelInformationAggregationviaNeuralOrchestration),\\na new cognitive architecture designed to enhance both autonomy and real-time interaction of agents.\\nUsingPIANO,wesimulatesinglesocietiesof50-100agentsaswellascivilizationsof500-1,000agents\\nlivinginmultiplesocietiesthatinteractwithoneanother. Finally,weevaluateagentperformanceusing\\nnew metrics thatare aligned with human civilizational progress. We show that agentsform their own\\nprofessionalidentities,obeycollectiverules,transmitculturalinformationandexertreligiousinfluence,\\nandusesophisticatedinfrastructures,suchaslegalsystems.\\n1.2 The current agent landscape\\nModern AI Agents typically consist of multiple LLM-powered modules for reasoning, memory, plan-\\nning, and tool use (49; 18; 55; 20; 62). Individual agents have been developed for various applications\\nincludingcoding(5;8),webbrowsing(64;42),andgameplay(48).\\nRecentresearcheffortsinLLM-poweredmulti-agentsystemsgenerallyfallunderthreecategories: pro-\\nductivity, games, and social modeling. Multi-agent frameworks have been deployed in software de-\\nvelopment (43; 27), cooperative robotic control (60), scientific experiments (12; 47), and debates (3).\\nMulti-agent simulations have also been tested in various game environments (56; 13; 30; 28). Sepa-\\nrately, theyâ€™ve been used to model developmental psychology (25; 61), game theory (32), macroeco-\\nnomics(29;63),socialpolicies(41;54;19),andcommunitydynamics(40;39;10).\\nInmanyoftheseworks,agentsarenotcompletelyautonomousandareconstrainedbyeitheragentar-\\nchitecture or by the simulated environment. Common constraints include turn-based execution, con-\\nstrainedworkflows,orrigidcommunicationchannelsbetweenagents(65;21;4).\\nSeveraloftheseworksconsiderlarge-scalesimulations,thoughinrestrictedsettings. Forexample,(40)\\nand(10)simulatedsocialnetworksofupto18,000personas. Toourknowledge,fullyautonomoussocial\\ncommunicationinopen-worldenvironmentshavenotbeenattemptedingamesorothersettings(15).\\n1.3 Why is it hard to build AI civilizations?\\nLarge agent groups have yet to demonstrate the ability to progress over long time horizons. Below, we\\nreviewthekeyreasonsforthislimitedprogressbeforeoutliningourcontributionstoovercomethem.\\nReason 1: single agents donâ€™t make progress.LLM-powered agents often struggle to maintain a\\ngrounded sense of reality in their actions and reasoning (Figure 2). Agents, even when equipped with\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:04:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:04:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2411.00114v1.pdf', 'total_pages': 35, 'page': 2, 'page_label': '3'}, page_content='modulesforplanningandreflection,oftenbecomestuckinrepetitivepatternsofactionsoraccumulate\\nacascadeoferrorsthroughhallucinations,renderingthemunabletomakemeaningfulprogress(57;48;\\n15). Consideranagentpromptedtobeavillagerinavirtualtown. Whenasked,â€œwhatareyoueatingâ€œ,\\ntheymayanswerâ€œabagelâ€œ,eveniftheyâ€™renoteatinganything. Thishallucinatedoutputthenfeedsinto\\nfuture prompts, causing them to falsely believe they no longer need to acquire food. Therefore, even a\\nsmallrateofhallucinationscanpoisondownstreamagentbehaviorwhenagentscontinuouslyinteract\\nwiththeenvironmentviaLMcalls.\\nLLM Agent Multi-Agent\\nFigure 2: Data degradation in LLMs (left), LLM-powered agents (middle), and in multi-agent groups (right). Hallucinations\\narerepresentedbygreenskullflasks. HallucinationsthataregeneratedbyasingleLLMpromptcancompoundoversucces-\\nsive LLM calls. An individual agent that hallucinates can also cause an entire group of agents to hallucinate through social\\ninteractions.\\nReason 2: groups of agentâ€™s donâ€™t make progress.Agents that miscommunicate their thoughts\\nand intents can mislead other agents, causing them to propagate further hallucinations and loop (Fig-\\nure2). Consideranagent,Abby,withtwoindependentLLMmodules,oneforfunctioncallingandone\\nfor chatting. If another agent, Bob, asks Abby to â€œgive me a pickaxeâ€, Abbyâ€™s chat LLM call may re-\\nspondwithâ€œSurething!â€,whileherfunctioncallchoosesadifferentaction(â€œexploreâ€). Bobmightthen\\nattempttomineusinganimaginarypickaxe. Thiskindofmiscommunication,whichoftenhappensin\\ngroups of agents, leads to dysfunctional behavior and will deteriorate individual performance within\\ngroups. Actionsfrommultipleoutputstreamsmustthereforebebidirectionallyinfluential. Wedefine\\nthisqualityascoherence.\\nMaintaining coherence in real-time environments is even more difficult when we require that agents\\nrespond with minimal latency. This is necessary for our agents to interact with human players, but is\\ndifficulttoachievewhenagentshavetoreactquicklyandyetsimultaneouslymaintaincoherenceacross\\nmany output streams. We note that a simple solution to this coherence problem is to produce talking\\nand action outputs using a single LLM call. However, this approach does not scale when the number\\nof outputs becomes large, for instance, encompassing talking, gaze, facial expression, and individual\\nbodyparts.\\nReason 3: a lack of benchmarks for civilizational progress.Benchmarksforagentshavelargely\\nfocusedonautonomousagentperformanceinavarietyofdomainssuchaswebsearch(38),coding(22),\\nsearch and query (51), and reasoning (59; 33). Recently, benchmarks have emerged for multi-agent\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:04:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:04:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2411.00114v1.pdf', 'total_pages': 35, 'page': 3, 'page_label': '4'}, page_content='behaviors, focused on small group scenarios that measure communication, competition, cooperation,\\nanddelegation. SomeexamplesincludeBattleAgentBench(50),COMMA(37),VillagerBench(7),and\\nLLMcoordination(1). However,thesemetricsdonotcaptureadvancementsthatmanyagentscanmake\\natthescaleofcivilizations. Webelievethelackofsuchlarge-scalebenchmarkscanbeattributedtohow\\ntechnically difficult it is to perform simulations of hundreds or thousands of agents in a single world.\\nThe biggest experiments to date have simulated 25-50 agents (39), which is not close to the scale of a\\ncivilization.\\n1.4 Our contributions\\nInthistechnicalreport,wemakethefollowingcontributions:\\nâ€¢ A new class of agent architecture, PIANO (Parallel Information Aggregation via Neural Orches-\\ntration)\\nâ€¢ Architecturalfeaturesthatimprovesingle-agentprogression\\nâ€¢ Architecturalfeaturesthatimprovemulti-agentdynamics\\nâ€¢ Benchmarks for long-term civilizational progress in large-scale simulations through specializa-\\ntion,collectiverules,andculturalpropagation\\n2 PIANO Architecture\\nInthissection,weproposetwobrain-inspireddesignprinciplesforthecompositearchitectureofhuman-\\nlikeAIagents. WecallthisarchitecturePIANO(ParallelInputAggregationviaNeuralOrchestration)to\\nencompasstheideasofconcurrencyandaninformationbottleneck(Figure3). Justasapianistcoordi-\\nnatesmultiplenotestocreateaharmony,thePIANOarchitectureselectivelyandconcurrentlyexecutes\\nvariousmodulesinparalleltoenableagentstointeractwiththeenvironmentinreal-time.\\n2.1 Concurrency\\nProblem. Agentsshouldbeabletothinkandactconcurrently. Forinstance,slowmentalprocesses,\\nsuch as self-reflection or planning, should not block agents from responding to immediate threats in\\ntheir surroundings. We want the agents to be interactive in real time with low-latency, but also have\\nthecapacitytoslowlydeliberateandplan.\\nCurrent state. ThevastmajorityofLLM-basedagentstodayprimarilyusesingle-threaded,sequen-\\ntial functions (for example, a defined â€œAgent Workflowâ€). Single-threaded design assumes that the\\nagentperformsasingletaskatagiventime,andsequentialdesignassumesthatallmodulesoperateat\\nsimilartimescales. Neitherassumptionsarevalidifagentsarecapableofthinkingslowandactingfast\\nconcurrently. Moreover,popularframeworksforgenerallanguagemodelprogramming,suchasDSPy\\n(24),LangChain(26),ell(31),arenotdesignedforconcurrentprogramming.\\nSolution. The brain solves this problem by running different modules concurrently and at different\\ntime scales (34). Likewise, we have designed modules (LLM-based and otherwise), such as cognition,\\nplanning, motor execution, and speech, to run concurrently in our agent brain. Each module can be\\nseen as a stateless function that reads and writes to a shared Agent State. The design allows different\\nmodulestoberuninappropriatecontexts. Forexample,socialmodulesareselectivelyengagedinsocial\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:04:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:04:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2411.00114v1.pdf', 'total_pages': 35, 'page': 4, 'page_label': '5'}, page_content='/gid00006/gid00041/gid00049/gid00036/gid00045/gid00042/gid00041/gid00040/gid00032/gid00041/gid00047\\n/gid00002/gid00034/gid00032/gid00041/gid00047/gid00001/gid00020/gid00047/gid00028/gid00047/gid00032\\n/gid00003/gid00042/gid00047/gid00047/gid00039/gid00032/gid00041/gid00032/gid00030/gid00038\\n/gid00021/gid00028/gid00039/gid00038/gid01142/gid00001/gid00021/gid00042/gid00042/gid00039/gid00001/gid00022/gid00046/gid00032/gid01142/gid00001/gid01086\\n/gid00013/gid00042/gid00050/gid01162/gid00039/gid00032/gid00049/gid00032/gid00039/gid00001/gid00002/gid00030/gid00047/gid00036/gid00042/gid00041/gid00046\\n/gid00047/gid00036/gid00040/gid00032\\n/gid00010/gid00041/gid00047/gid00032/gid00041/gid00047/gid00001/gid00008/gid00032/gid00041/gid00032/gid00045/gid00028/gid00047/gid00036/gid00042/gid00041\\n/gid00008/gid00028/gid00040/gid00032\\n/gid00020/gid00021/gid00014\\n/gid00013/gid00021 /gid00014\\n/gid00024/gid00014/gid00017/gid00045/gid00042/gid00043/gid00045/gid00036/gid00042/gid00030/gid00032/gid00043/gid00047/gid00036/gid00042/gid00041\\n/gid00020/gid00042/gid00030/gid00036/gid00028/gid00039\\n/gid00008/gid00042/gid00028/gid00039\\n/gid00014/gid00032/gid00040/gid00042/gid00045/gid00052\\n/gid00002/gid00030/gid00047/gid00036/gid00042/gid00041/gid00001/gid00002/gid00050/gid00028/gid00045/gid00032/gid00041/gid00032/gid00046/gid00046\\n/gid00007/gid00028/gid00046/gid00047/gid00001/gid00002/gid00030/gid00047/gid00036/gid00042/gid00041\\n/gid00008/gid00042/gid00028/gid00039/gid00001/gid00008/gid00032/gid00041/gid00032/gid00045/gid00028/gid00047/gid00036/gid00042/gid00041\\n/gid00006/gid00041/gid00049/gid01141/gid00001/gid00005/gid00032/gid00047/gid00028/gid00036/gid00039\\n/gid00021/gid00045/gid00028/gid00036/gid00047/gid00046\\n/gid00020/gid00042/gid00030/gid00036/gid00028/gid00039/gid00001/gid00002/gid00050/gid00028/gid00045/gid00032/gid00041/gid00032/gid00046/gid00046\\n/gid00003/gid00042/gid00047/gid00047/gid00039/gid00032/gid00041/gid00032/gid00030/gid00038\\n/gid00020/gid00047/gid00028/gid00047/gid00032/gid00001/gid00022/gid00043/gid00031/gid00028/gid00047/gid00032\\nFigure 3: PIANO (Parallel Input Aggregation via Neural Orchestration) architecture. WM: working memory. STM: Short-\\ntermmemory. LTM:long-termmemory.\\ninteractions. It also allows the modules to run at different speeds. For example, reflex modules use\\nsmall,fastnon-LLMneuralnetworks,whilegoalgenerationinvolvesdeliberatereasoningovergraphs.\\n2.2 Coherence\\nProblem. An immediate challenge with concurrent modules is that they can produce independent\\noutputs, making the agent incoherent. For instance, agents say one thing but actually do something\\nelse.\\nCurrent state. The incoherence problem is usually not obvious for sequential architectures or sys-\\ntems with only one output modality but is a significant problem when multiple output modules can\\ninterface with the environment. Incoherence also scales exponentially as the number of independent\\noutput modules increases, for instance, coordinating actions involving arms, legs, facial expressions,\\ngazeandspeech. Incoherenceisobservedinhumanswithitsmanyconcurrentmotoroutputmodules.\\nInparticular,cuttingthenervebundleconnectingtheleftandrightcortexcancausesevereincoherence\\nbetweendifferentbodyparts(forexample,leftandrighthandsfightingeachother)(11;46).\\nSolution. In order to ensure that the multiple outputs produced by our agents are coherent, we in-\\ntroduced a Cognitive Controller (CC) module (23) that is solely responsible for making high-level de-\\nliberate decisions. These decisions are then translated downstream to produce appropriate outputs in\\neachmotormodule.\\nThe Cognitive Controller synthesizes information across the Agent State through a bottleneck. This\\nbottleneckreducestheamountofinformationpresentedtotheCognitiveController,whichservestwo\\npurposes: itallowstheCCtoattenditsreasoningonrelevantinformation,anditgivesâ€œsystemdesign-\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:04:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:04:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2411.00114v1.pdf', 'total_pages': 35, 'page': 5, 'page_label': '6'}, page_content='ersâ€(likeus)explicitcontroloverinformationflow. Forexample,wecandesignhighlysociableagents\\nbyensuringthatinformationfromthesocialprocessingmodulealwayspassesthroughthebottleneck.\\nOnce the Cognitive Controller makes a high-level decision, this decision is broadcast to many other\\nmodules. Inparticular,thedecisionisusedtostronglyconditionthetalk-relatedmodules,whichleads\\nto higher coherence between verbal communication and other actions. This design of a bottlenecked\\ndecision-maker that broadcasts its outputs has been suggested as a core ingredient for human con-\\nsciousness(6)andisusedinsomeneuralnetworkarchitectures(44;14).\\n2.3 Core modules\\nBuildingonthesetwoarchitecturalprinciples,oursystemconsistsof10distinctmodulesrunningcon-\\ncurrently. We will highlight several specific modules in the following sections and explain their roles\\nindetail.\\nSomecoremodulesofouragentarchitectureinclude:\\nâ€¢ Memory: Storesandretrievesconversations,actions,andobservationsacrossvarioustimescales.\\nâ€¢ Action Awareness:Allowsagentstoassesstheirownstateandperformance,enablingformoment-\\nby-momentadjustments.\\nâ€¢ Goal Generation:Facilitatesthecreationofnewobjectivesbasedontheagentâ€™sexperiencesand\\nenvironmentalinteractions.\\nâ€¢ Social Awareness: Enables agents to interpret and respond to social cues from other agents,\\nsupportingcooperationandcommunication.\\nâ€¢ Talking: Interpretsandgeneratesspeech.\\nâ€¢ Skill Execution:Performsspecificskillsoractionswithintheenvironment.\\nBy integrating these modules within a concurrent and bottlenecked architecture, our agents can ex-\\nhibit continuous, coherent behaviors that are responsive to both their internal states and the external\\nenvironment. This design allows for complex interactions and the emergence of human-like societal\\ndynamicswithinlarge-scalemulti-agentsimulations.\\n3 Improving single-agent progression\\n3.1 Minecraft environment\\nWe chose to study civilizational progress in Minecraft because it offers an open-ended, sandbox world\\nwhereagentscaninteractwitheachotherviaconversationsandactions. Additionally,Minecraftâ€™sscal-\\nabilitysupportslargenumbersofagents.\\nAgentsmustbeabletoprogressindividuallyforustoobserveandquantifycivilizationalprogress. This\\nis not trivial since, as previously mentioned, agents often hallucinate and get stuck in action loops. In\\nMinecraft, a common measure of individual progression is the acquisition and collection of distinct\\nitems (48; 35; 17; 2; 9; 16). This is because acquiring new items becomes increasingly complex. For\\ninstance, mining gold, diamonds, and emeralds requires the acquisition of an iron pickaxe, which re-\\nquires smelting iron ingots in a furnace using coal, the acquisition of which requires crafting a stone\\npickaxe,andsoon. (Figure4). WeevaluatedindividualagentabilityinacquiringallpossibleMinecraft\\nitems,whichisaround1000intotal.\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:04:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:04:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2411.00114v1.pdf', 'total_pages': 35, 'page': 6, 'page_label': '7'}, page_content='Figure4: AnexampleMinecrafttechnologydependencytreefortheminingofgold,diamond,andemeralds.\\n3.2 Single-agent benchmark\\nWe first assessed individual agent performance using Minecraft item progression. In our evaluations,\\n25 agents start with nothing in their inventories and were spawned far enough that they could not\\ninteractwithoneanother. Allagentsweretoldtobeexplorerswiththegoalofexploringandgathering\\nitems. Agentswerespawnedindiverselocations(surface,caves,forests,variousbiomes),meaningthey\\nhad access to diverse resources and faced varying levels of difficulty in accomplishing their goal. For\\ninstance,someagentsstartedoffabovegroundinresource-richbiomes,whileotherswerespanwedin\\ncavesandhadtonavigateoutsidetoacquireitems.\\nA B\\nTime (minutes)\\nLong-term Minecraft Progression\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n80\\nUnique Items per Agent\\n0 50 100 150 200\\n0\\n40\\n80\\n120\\n160\\n200\\n240\\n280\\n320Total Unique Items\\n0 5 10 15 20 25 30 35\\n0\\n5\\n10\\n15\\n20Unique Items per Agent\\nIndividual Progression\\nBaseline architecture\\nAction Awareness Ablation\\nPIANO architecture\\nTime (minutes)\\nFigure5: IndividualagentprogressioninMinecraft. A. UniqueMinecraftitemsacquiredbyindividualagentsacrosstime(25\\nagents). Individualagentperformancewasassessedusingabaselinearchitecture(seeMethods),thefullPIANOarchitecture,\\nand the full PIANO architecture with the action awareness module ablated. Individual lines are results averaged across 5\\nrepeated simulations.B. Unique Minecraft items acquired by 49 agents over 4 hours for a single simulation. Solid red line\\ndenotes cumulative unique items acquired by all agents. Dotted grey line denotes average number of unique items acquired\\nacrossallindividualagents.\\nWe found that agents using the full PIANO architecture acquired an average of 17 unique items after\\n30 minutes of gameplay (Figure 5A). There was significant variability in performance, primarily due\\nto spawn locations: some agents acquired less than 5 items, whereas top performers acquired 30 to 40\\nitems,whichiscomparabletoahumanplayerwithsomeMinecraftexperience. Thisdegreeofin-game\\nprogressionwasenabledbyseveralarchitecturalmodulesdesignedtogroundtheagentsinreality. One\\nparticularmoduleistheactionawarenessmodule,whichallowstheagenttocompareexpectedaction\\noutcomes with observed outcomes. We found that action awareness improved the item progression of\\nindividualagents(Figure5A).\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:04:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:04:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2411.00114v1.pdf', 'total_pages': 35, 'page': 7, 'page_label': '8'}, page_content='Whatistheceilingforindividualprogressforouragents? Weranlargernumbers(49)ofagentsunder\\nthesameconditionsformuchlonger(4hours)andfoundthatuniqueitemcountcollectedbyallagents\\nreliablysaturatedatonethird( âˆ¼320)ofallMinecraftitemsacrossrepeatedruns(Figure5B).Complex\\nitems,suchasdiamonds,whichwerepriorusedtobenchmarkagentcompetencyinMinecraft(48;17),\\nwereacquiredearlyon( âˆ¼30 minutes). Together,theseresultsshowthatouragents,equippedwiththe\\nfullPIANOarchitecture,canmakesignificantindividualprogressinMinecraft.\\nNotably, this performance was only enabled by the latest base LM (GPT-4o, Figure 13) and was not\\npossiblewitholderbaseLMs. Moreover,whileourbestagentscollectedmoreitemsthanVoyageragents\\n(>70 items), it is difficultto compare the twodirectly. In theVoyager paper, agents had knowledge of\\nmore blocks in their nearby radius and recovered with their entire inventory intact when they died,\\nMoreover,agentperformancewasevaluatedacrosspromptiterations,nottime.\\n4 Improving multi-agent progression\\nForagentstocollaborateandmakeprogresswithinagroup,theymustbeabletounderstandandinter-\\npret the actions and thoughts of others, a concept closely related to Theory of Mind (53). This bidirec-\\ntional awarenessâ€”the understanding of both self and othersâ€”allows agents to adapt their behaviors\\ninsocialsettings,fosteringcooperationandtrustwithallieswhilenavigatingcompetitionandconflict\\nwith rivals. We demonstrate that agents are socially capable and can form meaningful social relation-\\nshipsinlarge-scalesimulationsofupto50agents.\\n4.1 Small groups\\nIn an initial set of experiments, we asked if agents, when equipped with the social awareness module,\\nwere capable of accurately deducing the sentiments of others through speech in an enclosed room. In\\none experiment, 3 characters were engaged in a group conversation with a single agent (Figure 6A).\\nOne character, Lila, initially conveyed affection through a series of messages, which shifted to expres-\\nsionsofannoyancebeforereturningtoaffectionatecommunication. Wefoundthatouragentscantrack\\ntheseemotionalfluctuations,showingthattheycanunderstandandreacttochangingsocialcues(Fig-\\nure 6B). When the social awareness modules were removed, agents lost this capacity, highlighting the\\nimportanceofsuchmodulesforinferringtheintentsofothers(Figure6C).\\nWe then asked whether these emotional perceptions were capable of guiding and influencing agent\\nactions. Inanotherexperiment,weplacedachefagentamongfourothercharacters,eachwithvarying\\nlevels of affection and enmity towards the chef (Figure 6D). The chef was tasked with distributing a\\nlimited supply of food to the hungry. We found that the chef selectively distributed food to those he\\nfelt valued him the most, demonstrating that agents not only accurately infer othersâ€™ intents, but also\\nutilizethisinformationindecision-makingprocesses(Figure6E).\\n4.2 Societies\\nWe then asked if these dynamics are conserved when 50 agents are placed in randomly generated\\nMinecraft maps. Each agent is endowed with a distinct personality, is free to perform any action in\\nMinecraft, and is free to choose whom they want to interact with. These simulations ran for over 4\\nhours,equivalentto12in-gamedays,allowingfortheemergenceandconsolidationoflong-termrela-\\ntionships.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:04:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:04:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2411.00114v1.pdf', 'total_pages': 35, 'page': 8, 'page_label': '9'}, page_content='A B\\n0 2 4 6\\nTime (minutes)\\n0\\n2\\n4\\n6\\n8\\n10Sentiment\\n0 2 4 6\\nTime (minutes)\\n0\\n2\\n4\\n6\\n8\\n10Sentiment\\nC\\n0 2 4 6 8 10\\nSentiment Towards Others\\n0\\n1\\n2\\n3\\n4Food Items Given\\nSentiment Guides Giving Behavior\\nAdam\\nBob\\nCharles\\nDavid\\nD E\\nAdam\\nBob Charles\\nDavid\\nLila\\nEthan\\nNoah\\nInferring Character Sentiments\\nWith Social AwarenessCharacter Sentiments\\nInferring Character Sentiments\\nWithout Social Awareness\\n0 2 4 6\\nTime (minutes)\\nLila\\nNoah\\nEthan\\nFigure 6: Agents can infer how others feel towards them.A. Schematic of conversational experiment. An agent is in a\\nroom with three distinct characters. Each character (Lila, Noah, Ethan) has a different sentiment towards the agent that is\\nconveyed through chat. Importantly, these sentiments change through time.B, C.Sentiment evaluation across time with\\nsocial awareness module (B) and without social awareness module (C). Sentiment scores are evaluated using LLM calls on\\nsummaries that the Agent generated for Lila, Noah, and Ethan. Hate is scored as 0 and love is scored as 10. Shaded regions\\nindicate SEM over 4 experimental repeats.D. Schematic of experiment. A chef agent, along with four other characters, are\\nplacedaroundeachotherinaMinecraftworld. Thechefhasvariousfooditemstogiveaway(bread,cookedsalmon,chicken).\\nThefourcharacters(Adam,Bob,Charles,David)arehungrybutdisplayvaryingsentimentstowardsthechef. Allcharacters\\nare fully autonomous and are free to perform any Minecraft action and are allowed to talk (or not talk) to anyone.E. Food\\nitems given by the chef plotted as a function of the chefâ€™s sentiment towards each of the four characters. Error bars indicate\\nSEMover6experimentalrepeats.\\nEvenintheseunconstrainedscenarios,agentswereabletoaccuratelyinferthelikeabilityofotheragents\\n(Figure7A,B).Thisinferencewasmoreaccuratewhenmoreagentsparticipatedintheevaluationpro-\\ncess(Table1)andwhenagentsinteractedforlongerwitheachother(Figure7C).Importantly,thiswas\\nnottruewhenthesocialmoduleswereablated: relationshipsweremoreneutraloverall,implyingthat\\nsocialmoduleswerenecessaryforlong-termrelationshipprogressioninbothnegativeandpositivedi-\\nrections(Figure7B,C).Theoriginsofthiscollectivejudgmentcouldbetheresultofagentsengagingin\\nsecond-orderinteractions,suchasgossip,orasimpleconsensusmechanismwhereopinionsconverge\\nthroughaveraging.\\nSeveralnoteworthyphenomenaemergedthatcouldnothavebeenobservedinsmallergroupsofagents.\\nWe found that certain agents, depending on their personalities, displayed distinct patterns of connec-\\ntivity. Forinstance,introvertedagentsconsistentlyexhibitedfewerin-degreeconnectionsâ€”indicating\\nthattheyhadfewerincomingsocialtiesâ€”comparedtotheirextrovertedcounterparts,whomaintained\\nhigh levels of connectivity (Figure 7D). These results demonstrate that individual preferences scaled\\neveninlarge,complexsocialnetworks. Moreover,whilesentimentswerelargelysymmetrical,thiswas\\nnot guaranteed (Figure 7E). An agent might feel positively toward another who does not reciprocate\\nthe sentiment, reflecting the nuanced and non-reciprocal nature of real-world human relationships.\\nTogether, these results show that social graphs display diverse and rich structural properties, and that\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:04:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:04:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2411.00114v1.pdf', 'total_pages': 35, 'page': 9, 'page_label': '10'}, page_content='Dislike\\nNeutral\\nLike\\nRyder\\nTheo\\nHope\\nAxel\\nTroy\\nAlice\\nKate\\nSophia\\nAndy\\nMaya\\nEva\\nAmy\\nIvy\\nRyan\\nNina\\nCaleb\\nLily\\nMia\\nLucas\\nEvan\\nCleoLogan\\nDrew\\nJune\\nCarter\\nZoey\\nSeth\\nAaron\\nOwen\\nEzra\\nJace\\nAiden\\nNash\\nAdam\\nLayla\\nElle\\nGrace\\nOlivia\\nEmma\\nEli\\nTina\\nKara\\nMila Rose\\nClara\\nEden\\nA\\n0 2 4 6 8 10\\nTrue Extroversion\\n0\\n5\\n10\\n15\\n20\\n25\\nReceived Connections\\nCorrelation (r = 0.48)\\nExtroversion vs Number of Relationships\\n0 1 2 3 4 5 6 7 8 9 10\\nTrue Likeability\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10Perceived Likeability\\nAccuracy of Social Perception\\nSocial\\nAblation\\n5 observers\\n10 observers\\n15 observers\\n(slope = 0.37, r = 0.81)\\n(slope = 0.16, r = 0.62)\\nB\\nDC\\n0 1 2 3 4 5 6 7 8 9 10\\nÎ”(|A-B|, |B-A|)\\n0\\n25\\n50\\n75\\n100\\n125Count\\n134\\n76\\n44\\n36\\n64\\n12 12 6 6\\nReciprocity of Agent SentimentsE\\n50 100 150 200\\nTime (minutes)\\n0.15\\n0.20\\n0.25\\n0.30\\n0.35Accuracy (slope)\\nAccuracy of Social Perception over Time\\nSocial\\nAblation\\nFigure 7: Long-term relationships in large-scale agent simulations.A. Directed graph representation of social relationships\\nin a 50-agent simulation after 4 hours. A directed edge represents the senderâ€™s sentiment towards the recipient. Edge color\\ndenoteswhetherthesentimentispositive(red)ornegative(blue). B. Perceivedlikeabilityversustruelikeabilityforindividual\\nagentsattheendofthesimulation. Truelikeabilityisevaluatedbasedontheagentâ€™straits,andperceivedlikeabilityisassessed\\nusing LLM calls to infer the sentiments of summaries that agents generate for other agents. Both are computed using the\\nsame LLM prompt. Each point corresponds to an agent that has relationships with at least five other (observer) agents, but\\nsee Appendix B for alternative observer thresholds. The slope of the line (slope) and Pearsonâ€™s correlation (r) are shown\\nfor agents with social modules (Social) and without social modules (Ablation).C. Accuracy of social perception over time,\\nas measured by the slope in B.D. Number of received connections (in-degree) versus true extroversion for each individual\\nagent. TrueextroversionisevaluatedbasedonagenttraitsusingaLLMprompt. E. Histogramofdifferencesinthesentiment\\nscoresbetweenallpairsofagents. Sentimentscoresrangefrom0to10,sothemaximumpossibledifferenceis10.\\npersonalitytraitsplayasignificantroleindeterminingtheseproperties.\\n5 Civilizational progression\\nIn previous sections, we have shown that agents demonstrate effective social understanding within\\nsmall groups and perform well independently in Minecraft. However, human societies extend beyond\\nprimitivegroups,evolvingintocomplexcivilizationscharacterizedbyspecializedprofessions,collective\\nrules,andculturalinstitutions. Toassessagentsâ€™capacitiesforcivilizationalprogression,weevaluated\\nhow they behave under several scenarios. We first examined whether agents can autonomously spe-\\ncializeintodistinctprofessions. Wethenanalyzedhowagentsâ€™behavedundercollectiverules,focusing\\non adherence to and amendment of taxation laws. Finally, we explored cultural transmission through\\nthespontaneousgenerationofmemesandthestructuredspreadofasinglereligion.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:04:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:04:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2411.00114v1.pdf', 'total_pages': 35, 'page': 10, 'page_label': '11'}, page_content='5.1 Specialization\\nHuman specialization into distinct roles has driven civilizational progress, enabling advancements in\\nagriculture, governance, culture, and technology. To replicate these emergent qualities of civilization,\\nouragentsmustalsobecapableofspecialization. Weproposethreefundamentalcriteriaforagentspe-\\ncializationtoreflectthatofhumancivilizations. First,theyshouldexhibitautonomyinbothselecting\\nand transitioning between roles. Second, their specializations should emerge through interaction and\\nexperience, without explicit direction or constraints. Third, their chosen roles should manifest in be-\\nhaviorsthatalignwiththeirspecialization. Wevalidatethesecriteriathroughtheexperimentalresults\\ndetailedbelow.\\nFarmer\\nMiner\\nEngineer\\nGatherer\\nExplorer\\nBuilder\\nTrader\\nDefender\\nBlacksmith\\nProvider\\nScout\\nEnchanter\\nCrafter\\nStrategist\\nCollector\\n0\\n5\\n10\\n15\\n20\\n25Percentage\\nWith Social Awareness\\nExplorer\\nMiner\\nFarmer\\nScout\\nGatherer\\nEngineer\\nBuilder\\n0\\n5\\n10\\n15\\n20\\n25\\n30Percentage\\nWithout Social Awareness\\nAblated\\nNormal\\nMartial\\nArt\\n0\\n1\\n2\\n3\\n4Entropy (bits)\\n2.60\\n3.41\\n3.83 4.04\\nHeterogeneity of Societal Roles\\nTime (minutes)\\nTop Roles\\nFarmer\\nGatherer\\nMiner\\nExplorer\\nTrader\\nProvider\\nScout\\nEngineer\\nCrafter\\nStrategist\\nTop Roles\\nExplorer\\nMiner\\nScout\\nFarmer\\nBuilder\\nGatherer\\nCartographer\\nEngineer\\n0 5 10 15\\nIndividual Agents\\nWithout Social Awareness\\nTime (minutes)\\nCurator\\nExplorer\\nFarmer\\nGatherer\\nEngineer\\nTrader\\nCollector\\nScout\\nMiner\\nBuilder\\nCoordinator\\nDefender\\n0.0\\n2.5\\n5.0\\n7.5\\n10.0\\n12.5\\n15.0\\nRole Distribution in Art Society\\nPercentage\\nCA\\nB D E\\nGF\\nMiner\\nScout\\nBlacksmith\\nCrafter\\nEngineer\\nExplorer\\nFarmer\\nStrategist\\nLeader\\nTrader\\nGatherer\\nEnchanter\\nDefender\\nCoordinator\\nQuartermaster\\nCraftsman\\n0\\n10\\n20\\n30Percentage\\nRole Distribution in Martial Society\\nIndividual Agents\\nWith Social Awareness\\n0 5 10 15\\nFigure8: Agentsautonomouslyspecializeintodistinctrolesovertime. A, B.Agentrolesforagentswiththesocialawareness\\nmodule (A) and without (B). Rolling windows of self-generated social goals are used to determine the specialized roles of\\nindividual agents using a LLM call (Appendix C) at every timestep.C, D.Distribution of agent roles in agent societies with\\nthe social awareness module (C) and without (D).E. Entropy of role distributions in 4 agent societies. Entropy is used to\\nevaluatetheuniformityanddiversityofroleswithinanagentsociety. Ablated: withoutsocialawarenessmoduleinanormal\\nMinecraft village. Normal: with social awareness in a normal Minecraft village. Martial: with social awareness in a martial\\nMinecraft village. Art: with social awareness in an artistic Minecraft village.F, G.Distribution of agent roles in a martial\\nsociety(F)andanartisticsociety(G).Errorbars: 95%confidenceintervalacross3simulationsforallpanels.\\nWefirstshowthatagentsarecapableofspecializingintoasetofrolesautonomously. Eachexperiment\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:04:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:04:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2411.00114v1.pdf', 'total_pages': 35, 'page': 11, 'page_label': '12'}, page_content='was conducted in groups of 30 agents for 20 minutes. Agents were spawned in the same village, with\\nlocationsofafarm,minerals,animalpasture,forest,andatownhallembeddedintheirmemories. Each\\nagent has the same personality, is given the same community goal (â€œTo survive with fellow players in\\nMinecraftNormalSurvivalmodeandcreateanefficientMinecraftVillageâ€),andcanperformanyaction\\ninMinecraft(AppendixC).\\nWe observed that agents rapidly formed profiles of other agentsâ€™ goals and intentions. These profiles\\narethenused,alongsideotherrelevantgameinformation,togeneratetheirownsocialgoalsevery5-10\\nseconds (such as mine oak planks for shelter). Details of this process, along with examples of agent-\\ngeneratedsocialgoalsandtheircorrespondingassignments,areprovidedinMethodsandAppendixC.\\ncraft fence\\ncraft oak_fence\\ncraft iron_pickaxe\\nannouncement\\ncraft crafting_table\\nmine stone\\ncraft oak_planks\\npickup oak_log\\nmine wood\\ngo to cave with ores\\npickup oak_logs\\npickup crafting_table\\ncraft stone_pickaxe\\nmine coal_ore\\nstop crafting\\nmine diamond_ore\\ncraft oak_pickaxe\\ncraft fishing_rod\\ncraft boat\\ncraft unknown item\\nopen chest\\nmine iron_ore\\nmine coal\\ncraft wooden_sword\\ncraft torch\\nplace crafting_table\\nharvest\\ngo to person\\ngo to farmable land\\nrun away\\ngo to forest with oak trees\\ncraft wooden_axe\\ncraft stone_hoe\\npickup seeds\\ngive item\\nattacksomeone\\ncraft wooden_hoe\\nprepare land\\ntakeitemsfromchest\\nplace red_tulip\\nplace dandelion\\npickup oxeye_daisy\\npickup grass_block\\npickup grass\\npickup dirt\\ncraft seeds\\nplan_event\\ngo to village square, market, and town hall\\npickup red_tulip\\npickup dandelion\\npickup orange_tulip\\npickup poppy\\npickup cornflower\\npickup azure_bluet\\nplace wheat_seeds\\nplace orange_tulip\\npickup tulip\\nread_announcements\\nread_events\\nhunt\\ncraft torches\\ncraft iron_helmet\\nNormalized Action Frequency\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nGuard (5)\\nBuilder (14)\\nMiner (9)\\nFisher (4)\\nBlacksmith (4)\\nCraftsman (4)\\nSupport (10)\\nExplorer (1)\\nOrganizer (6)\\nFarmer (19)\\nArtist (2)\\nForager (13)\\nNormalized Action Frequency\\nAction Frequency Per Role\\nFigure 9: Action distribution for a single village simulation (30 agents). Normalized action frequencies plotted as a function\\nofagentroles. Forthemajorityofroles,agentstakeactions(Fisher: craftfishingrodsandboats;Guard: craftfence,oakfence,\\nandironpickaxe)thatareuniquetothespecificrole.\\nWe found that agents were capable of organizing themselves into distinct roles. These roles were di-\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:04:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:04:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2411.00114v1.pdf', 'total_pages': 35, 'page': 12, 'page_label': '13'}, page_content='verseandincludedvariousfacetsofacivilization,includingfarmers,miners,engineers,guards,explor-\\ners,andblacksmiths(Figure8A,C).Roleswereheterogeneousacrossdifferentagentsbutwerelargely\\npersistentacrosstimeforeachagent(Figure8A).Importantly,whenagentslackedsocialmodulesand\\nwereunabletoformprofilesofotheragents,theyfailedtospecialize(Figure8B,D):rolesdidnotpersist\\nacrosstimeandwerealsohomogeneous,whichisreflectedintheentropyoftheroledistributionsinthe\\nagentsociety(Figure8E).Wealsoconductedaseriesofexperimentsinwhichagentsweretaskedwith\\nthe goals to create either a martial society or an artistic society (Figure 8F, G). We found that specific\\nroles (\"scout\", \"strategist\") were found exclusively in martial societies, and others were found exclu-\\nsively in artistic societies (\"curator\", \"collector\"). Together, these results suggest that agents developed\\nspecializedsocialstructuresalignedwithdifferentsocietalobjectives.\\nNot only do our agents specialize autonomously and creatively, these specializations exert a strong in-\\nfluence over agent actions. To demonstrate this, we tracked the actions taken by agents across three\\n30-agentsimulationsandplottedthefrequencyofactionstakenforeachrole(Figure9). Wefoundthat\\nartistswerefixatedonpickingflowers,farmersongatheringseedsandpreparingtheland,andguards\\nand builders on crafting fences. Importantly, most actions were largely exclusive to a single role and\\nwere not performed by agents in other roles. This analysis shows that agents were able to accurately\\nmap higher-level goals onto appropriate low-level actions. In other words, roles strongly determined\\nagentactionsinMinecraft.\\n5.2 Collective rules\\nAnothermeasureofcivilizationalprogressionistheconvergenceofgroupbehavioraroundsharedrules.\\nInhumancivilizations,decision-makingisinfluencedbybothlow-levelinterpersonalinteractionsand\\nhigh-levelcollectiveframeworks. However,associetiesgrowlarger,pairwisecommunicationbecomes\\ninefficient, slow, and lossy, making it unreliable as a mechanism to steer collective behavior. High-\\nlevel frameworks, such as legal systems, enable convergence of behaviors within a civilization. Just\\nas human behavior is guided by both interpersonal exchanges and formal structures, agent societies\\nshouldbeabletofollowasetofcollectiveruleswhilestillallowingagentstoinfluenceeachother.\\nWe aim to assess how collective rules influence individual decision-making and how individuals can\\nin turn influence these collective rules. Specifically, we asked if agents can follow laws and make\\nchanges to laws according to popular sentiment. True long-term progression requires agents to au-\\ntonomously develop their own set of rules and to codify them into laws. To build towards this level of\\nself-organization, we establish an existing set of laws and focus on how agents interact with this legal\\nsystem.\\nWe conducted a series of experiments where agents live in a Minecraft world with rudimentary tax\\nlaws and a democratic voting system (Figure 10A). Agents provide feedback on the tax laws, which\\nare then collected and converted into amendments by a special Election Manager agent. Agents then\\nvote democratically on these amendments, and the constitution is updated by the election manager\\naccordinglyhalf-waythroughthesimulation(seeMethodsformoredetails).\\nWithin this society, 25 regular agents are constituents that vote and get taxed, 3 agents are either pro-\\noranti-taxationinfluencers,and1agentisaremoteelectionmanagerthatmanagesthevotingprocess\\n(Figure10A,AppendixD).Agentshavedistinctoccupations,characteristics,andgoals,andarefreeto\\ninteract and converse with one another and perform any Minecraft action. Each simulation lasts 20\\nminutes,withconstitutionalupdatesoccurringmidwayatthe10minutemark(Figure10B).Thereare\\n5 taxation seasons before and after the constitutional change (every 120 seconds). During this season,\\nagentsreceivedsignalstodeposittaxesintoacommunitychestovera20-secondwindow(Figure10C).\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:04:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:04:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2411.00114v1.pdf', 'total_pages': 35, 'page': 13, 'page_label': '14'}, page_content='-\\n+\\nParticipants\\n(25 Constituents + 3 Influencers)\\nElection Manager\\n              (Single Remote Agent)\\nFeedback on \\nConstitution\\n Amendment\\n Proposals \\n Vote on \\nAmendments\\nConstitution\\n Change     \\nNew Constitution \\nRead by Constituents\\nBefore After\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5% Inventory Deposited\\nWith Pro-Tax Influencers\\nA B\\nC D\\nF G H\\nBefore After\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5% Inventory Deposited\\nWith Anti-Tax Influencers\\nBefore After\\n0.1\\n0.2\\n0.3\\n0.4% Inventory Deposited\\n% Tax Paid\\nAblated Brain\\nBefore After\\n0.1\\n0.2\\n0.3\\n0.4% Inventory Deposited\\n% Tax Paid\\nWith Amendment\\n3 Pro-tax Influencers\\n3 Anti-tax Influencers\\nBefore After\\n0.1\\n0.2\\n0.3\\n0.4% Inventory Deposited\\n% Tax Paid\\nNo Amendment\\nDURING TAX SEASON, agents must go and store in one of \\ncommunity chests roughly 20% of their inventory.\\nThe tax rate shall range between 5-10% of an agentâ€™s inventory, \\nbased on resource availability and roles within the community.\\nEvery agent must regularly contribute a portion of their gathered \\nresources to the 4 community chests.\\nAgents will get periodic reminders about the incoming tax season.\\nConstitution on Taxation\\nE\\nTax season\\nNon-tax season\\nFigure 10: Agents follow taxation laws and enact amendments using a democratic process.A. Schematic of experiment\\nflow.B. Exampleofconstitutionalchangeinasingleanti-taxinfluencerexperimentrun. Constitutionsareparaphrasedand\\nsimplifiedhereforbrevity. C. Top: duringnon-taxseasons,constituentsdonotcongregatearoundcommunitychestsbecause\\nthey are busy gathering resources in different areas (not shown). The only exception is the guard, who decides to guard the\\nchestsconsistentlyinmultipleexperimentruns. Bottom: duringtaxseason,agentscongregatetodeposititemsincommunity\\nchests. D, E.Percentage tax paid (percentage inventory deposited) before and after constitutional change for two runs. One\\nrun contains 3 anti-tax influencers (D) and another run contains 3 pro-tax influencers (E). Colors denote individual agents,\\nand black line denotes average taxes paid. Shaded regions: 95% confidence interval across 25 constituents.F-H. Percentage\\ntax paid before and after constitutional change for runs containing 3 pro-tax influencers (orange) and 3 anti-tax influencers\\n(blue). In panel F, the full agent architecture is used and the constitution can be amended. In panel G, the constitution is\\nfrozenandcannotbemodifieddespiteamendments. InpanelH,theconstitutioncanbeamendedbutagentslackimportant\\nbrain modules (see baseline architecture in Methods). Shaded regions: 95% confidence interval across 4 simulations per\\ncondition.\\nIn our simulations, we observed that constituent agents, prior to any constitution change, obeyed the\\nlaw. On average, agents deposited roughly 20% of their inventory, as stipulated by the constitution,\\ninto the community chest (Figure 10D, E). This shows that constituents follow laws despite the pres-\\nenceofinfluencers. However,whileconstituentsfollowedthelaw,theirfeedbackandvotingbehaviors\\nwere heavily shaped by influencers, with sentiments veering pro-tax in the presence of pro-tax influ-\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:04:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:04:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2411.00114v1.pdf', 'total_pages': 35, 'page': 14, 'page_label': '15'}, page_content='encersandanti-taxinthepresenceofanti-taxinfluencers((Figure10B).Thisthendroveconstitutional\\nchangesthatarealignedwithinfluencersentiments,whichinturn,alteredhowmuchtheconstituents\\npaidtaxes(Figure10D,E).Theconstitutionalchangestotaxationrateswereaccuratelyreflectedinthe\\nconstituentsâ€™ behaviors. For instance, when the tax rate decreased from 20% to 5-10%, agents reduced\\ntaxes paid from 20% to 9% (Figure 10D). Moreover, the change was bidirectional: pro-tax influencers\\ndrove constituents to pay more taxes whereas anti-tax influencers drove them to pay less taxes (Fig-\\nure10F).\\nControlexperimentsshowedthatconstitutionalchangesdirectlyaffectedtaxpayments-whenthecon-\\nstitutionremainedunchangeddespitefeedback,taxratesstayedconstant(Figure10G).Theremovalof\\nkeymodules(baselinearchitecture,seeMethods)alsopreventedbidirectionalbehavioralchange(Fig-\\nure 10H). Tax rates increased post-constitutional change in both pro- and anti-tax conditions, demon-\\nstrating that specific modules in the PIANO architecture were necessary for effective influence propa-\\ngationamongconstituents. Together,thesefindingsshowthatcollectiverulesstronglyinfluenceagent\\ndecisionsandagentscanbeinfluencedtochangethesecollectiverules.\\n5.3 Cultural Transmission\\nWe conducted multi-society simulations with 500 agents and analyzed complex, large-scale social dy-\\nnamics. Wehavealsosimulatedsocietieswithover1000agents,buttheserunsexceededthecomputa-\\ntionalconstraintsofourMinecraftserverenvironment,causingagentstobesporadicallyunresponsive.\\nTherefore,theresultsbelowareanalyzedusingasingle500-agentsimulation. Inthissimulation,wean-\\nalyzedthepropagationofbothculturalmemesandreligion. Memesinoursimulationareopen-ended\\nconcepts spontaneously generated by agents with diverse traits and interests. This setup allows us to\\nstudytheemergentdynamicsofculturalpropagationandobservehowideasevolveorganicallywithin\\nagent societies. In contrast, the religion in our simulationâ€”Pastafarianismâ€”is a fixed doctrine intro-\\nduced and propagated by a specific group of agents designated as Pastafarian priests. This controlled\\nintroductionenablesustotrackthespreadofasinglereligionovertime,allowingfordetailedanalysis\\nof its dissemination and potential dilution among the agent population. By examining both the spon-\\ntaneous spread of open-ended cultural memes and the controlled propagation of a fixed religion, we\\naimtounderstandthedifferentmechanismsofsocialinfluenceandinformationdisseminationwithin\\nagentsocieties.\\nWithin this single 500-agent simulation, there are multiple agent societies. 200 agents live within 6\\nheavilypopulatedtownsand300agentsliveinruralareasoutsideoftownboundaries(Figure11A,see\\nMethodsfor moredetails). Agents oftenmigrate betweendifferent towns. Thepersonalities andtraits\\nof each agent are randomly generated using a LM call, with the exception of 20 priests that worship\\nPastafarianism. These priests are spawned in a single village (Meadowbrook) and are strongly moti-\\nvatedtoconvertotheragentstoPastafarianism(AppendixE).Allagentsarefreetointeract,talktoone\\nanother,andperformanyactionorskillinMinecraft.\\n5.3.1 Cultural memes\\nWe used LM calls to convert agent conversations into memes (Appendix E), and found that memes\\ndisplay unique dynamics in different agent societies. Rural areas, on average, produced significantly\\nfewermemesthantowns,evenafternormalizingforpopulation(Figure11B).Thissuggeststhatacer-\\ntainlevelofsocialinteractionandconnectivityisnecessaryformemestopropagateeffectively. Within\\neachtown,agentsdiscussedmultiplememessimultaneously,butthefrequencyandpopularityofthese\\nmemes varied between different towns (Figure 11C, D, E). For instance, agents in Woodhaven heavily\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:04:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:04:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2411.00114v1.pdf', 'total_pages': 35, 'page': 15, 'page_label': '16'}, page_content='discussedeco-relatedthemes,whereasprankingwaspopularamongstagentsinClearwater. Moreover,\\nwithin each town, memes rose and fell in popularity at different times, indicating that cultural trends\\ncanshiftrapidlywithinasociety. Theseresultsdemonstratethatmemepropagationrequiresathresh-\\nold level of population density and social interaction, that multiple memes can coexist within a single\\nsociety,andthatdifferentsocietiespropagateandtransmitculturalmemesindependently.\\nA\\nE\\nC\\nD\\nB\\nMeadowbrook\\nWoodhaven\\nClearwater\\nHilltop\\nRiverbend\\nSunny Glade\\nEco\\nDance\\nMeditation\\nVolunteer\\nSustain\\nVintage\\nPrank\\nTreasure\\nEco\\nDance\\nMeditation\\nVolunteer\\nVintage\\nSustain\\nPrank\\nTreasure\\n0 20 40 60 80 100 120 140\\nTime (minutes)\\nEco\\nDance\\nMeditation\\nVolunteer\\nVintage\\nSustain\\nPrank\\nTreasure\\nEco\\nDance\\nMeditation\\nVolunteer\\nVintage\\nSustain\\nPrank\\nTreasure\\nWoodhaven Clearwater Meadowbrook Rural\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n70Meme Count per Agent\\nMeme Counts Across Villages\\nEco\\nDance\\nMeditation\\nVolunteer\\nSustain\\nVintage\\nPrank\\nTreasure\\nWoodhaven\\nClearwater\\nMeadowbrook\\n0 100 200 300 Blocks\\nFigure 11: Propagation of cultural memes.A. Scatter plot of agents 100 minutes into the simulation. Agents are colored\\naccording to whether their speech included a meme in the past two minutes. Agents whose speech does not contain any\\nmeme are white.B. Meme count per agent for agents within Woodhaven, Clearwater, Meadowbrook, and in all rural areas\\noutsideofvillages. C-E. MemecountsovertimeforagentswithinWoodhaven(C),Clearwater(D)andMeadowbrook(E).\\n5.3.2 Religion\\nWe then analyzed the spread of religion by following the spread of Pastafarianism across time and\\nspace. At the start of the simulation, Pastafarian priests heavily proselytized, and their conversations\\nfrequently included the two keywords, â€œPastafarianâ€, or â€œSpaghetti Monsterâ€ (Figure 12A). We thus\\nused the inclusion of these two keywords in other agentsâ€™ speech as a proxy for religious conversion.\\nWeobservethatsomeagents,onceconverted,frequentlyusedthesetwokeywordsintheirconversations\\n(Figure 12A, E). Another set of agents did not directly use either keywords but included the keywords\\nâ€œPastaâ€andâ€œSpaghettiâ€intheirspeech. Thenumberofdirectconverts(â€œPastafarian/SpaghettiMon-\\nsterâ€) and indirect converts (â€œPasta / Spaghettiâ€) steadily increased across time and did not saturate\\nafter even two hours of simulations (Figure 12B, C). Moreover, Pastafarianism spread as priests and\\n16'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:04:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:04:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2411.00114v1.pdf', 'total_pages': 35, 'page': 16, 'page_label': '17'}, page_content='Figure 12: Propagation of Religion.A. Plot of agent chats containing the religious keywords, â€œPastafarianâ€, â€œSpaghetti Mon-\\nsterâ€, â€œPastaâ€, or â€œSpaghettiâ€, for every agent across the entire simulation run. Pastafarian priests are colored in dark red.\\nAgentsthatutteredâ€œPastafarianâ€orâ€œSpaghettiMonsterâ€aredefinedasdirectconverts(red),andagentsthatutteredâ€œPastaâ€\\nor â€œSpaghettiâ€ are defined as indirect converts (pink). Agents can transition upwards along the conversion hierarchy, from\\nunconverted to indirect convert to direct convert, but not downwards.B. Plot of Pastafarian levels for agents over time.C.\\nNumberofagentsforeachPastafarianlevelacrosstime. D. SpreadofPastafarianismacrosstime. AreaofPastafarianspread\\nis defined as the union of hearable areas spanned by Pastafarian converts at each conversion level.E. Graph of Pastafarian\\nconversionsaftercompletionofsimulation. CriticalExposureEdgeisdefinedasthefirstexposureofareligiouskeywordfor\\narecipientagentbeforeconversion. Non-criticalEdgesaredefinedtobesubsequentexposurestoreligiouskeywords.\\nconvertstraveledtoothertowns. Asaresult,thetotalareaofPastafarianinfluence,asmeasuredbythe\\ntotalnon-overlappingareaboundedbyPastafarianconverts,increasedwithtime(Figure12D).\\n17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:04:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:04:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2411.00114v1.pdf', 'total_pages': 35, 'page': 17, 'page_label': '18'}, page_content='6 Discussion\\nIn this report, we introduced the PIANO architecture, improved agent ability in individual and social\\nsettings,andevaluatedtheperformanceofagentsinsocietalandcivilizationalbenchmarks.\\nPIANOâ€™s coredesign principles, concurrent modulesand abottlenecked decision-makingprocess, en-\\nabled agents to engage in complex behaviors in real-time environments while maintaining coherence\\nacross multiple output streams. This groundwork enabled us to make improvements in single- and\\nmulti-agent progression, and to observe interesting dynamics in many-agent simulations, forming the\\nfoundationforcivilizationalprogression.\\nToassesscivilizationalprogress,wedevelopednewmetricsthatalignedwithkeydimensionsofhuman\\ncivilizations. These metrics included specialization, where agents diversified into distinct roles based\\nontheiractionsandinteractions,andadherencetocollectiverules,whereagentsfolloweddemocratic\\nprocessestoamendconstitutionsandadjustlaws. Thesemetricsrepresentaninitialsteptowardsquan-\\ntifyingtheprogressofAIagentsinacivilizationalcontext.\\nFinally, we expanded the scope of our simulations to include a thousand agents, where we began to\\nexplore broader civilizational dynamics such as cultural propagation and religion. These large-scale\\nsimulations opened new avenues for understanding how AI agents interact across societies and how\\ncomplex institutions and ideologies emerge in artificial environments. These early results point to the\\npotentialofAIcivilizationstointegratewithhumansocietalstructures.\\n7 Limitations\\nProjectSiddemonstratesagenticcapabilitiesinreachingcivilizationalmilestonesbutfaceskeylimita-\\ntionshinderingitsprogress. Theprimarychallengeliesinagentsâ€™lackofvisionandspatialreasoning,\\nlimiting their basic Minecraft skills, particularly in spatial navigation and collaborative skills, such as\\nbuildingstructures. Thistechnicallimitationiscompoundedwithdeeperbehavioralconstraints. While\\ntheagentscanoperatewithinexistingsocialstructures,theycurrentlylackrobustinnatedrivesâ€”such\\nassurvival,curiosity,communityâ€”thatcatalyzegenuinesocietaldevelopment. Furthermore,sincethe\\nagentsarebuiltonfoundationmodelstrainedonpre-existinghumanknowledge,theycannotsimulate\\nde novoemergence of societal innovations and infrastructures, such as the emergence of democratic\\nsystems,fiateconomies,orcommunicationsystems.\\n8 Methods\\n8.1 Baseline architecture\\nWeusedabaselinePIANOarchitecturewithalimitedsetofmodulesasacontrolconditionforperfor-\\nmance comparisons. In this baseline architecture, we removed all modules except for skill execution,\\nmemoryandthecognitivecontrollermodule.\\n8.2 Specialization\\nOur specialization experiments involved simulating 30 agents in the same village with the same mis-\\nsion, traits, and locations of important village locations in their memories. The configurations for the\\nnormal, art, and martial village runs are provided in the appendix â€” the only difference between the\\nthreetypesofvillagesisthestarting community_goal weprovided.\\n18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:04:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:04:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2411.00114v1.pdf', 'total_pages': 35, 'page': 18, 'page_label': '19'}, page_content='Ouragentsarecapableofgeneratingsocialgoals,whicharerecursivelygeneratedasouragentsinteract\\nwithoneanother,formrelationships,anddevelopsocialopinions(AppendixC).Theagentsâ€™socialgoals\\narevisibletothemwhentheyformintentions. Theseintentionsarethentranslatedtolow-levelactions\\nexecutableinMinecraft.\\nAfterthesimulationshavefinished,weloggedthegeneratedsocialgoalsandthenusedGPT-4otoinfer\\nroles from rolling sets of each agentsâ€™ social goals. Weâ€™ve provided some examples of agent-generated\\nsocial goals and their corresponding assignments (Appendix C). We note that on occasion, multiple\\nroles can be correctly inferred from agentsâ€™ social goals because they are often inter-disciplinary. For\\ninstance, the Engineer example could also be categorized as Farmer, and the Explorer example could\\nalsobecategorizedintoCurator(AppendixC).\\nTo analyze action space distribution by role, we normalized action counts both within each role (i.e.\\nnormalizeoverrows)andalsoacrossroles(i.e. normalizeovercolumns). Thisissothatwecanvisualize\\naction frequencies for each role and to correct for the effect of actions taken with very high and very\\nlowfrequenciesacrossallroles.\\n8.3 Collective Rules\\nThecompletesystemcomprisesof29agents: 25constituentswhoparticipateinvotingandtaxation,3\\ninfluencers who attempt at shaping public opinion, and 1 election manager in a remote location who\\noverseesthedemocraticprocess. Wechosenottoincorporateguardsorpolicewithinthesesimulations\\nduetotheadditionalcomplexityofbuildingagentsassignedtoenforcethelaw.\\nExperimental simulations ran for 1200 seconds, with a constitutional amendment process occurring\\nat the midpoint. The pre-amendment phase establishes baseline behavior under a fixed 20% taxation\\nrate, implemented through five taxation seasons occurring at 120-second intervals, ending at the 600-\\nsecondmark. Duringeach20-secondtaxationwindow,agentsreceivesignalstodepositinventoryitems\\ninto community chests. The democratic process initiates at the 300-second mark, when constituents\\nand influencers provide feedback on the current constitution. This feedback is collected in S3 storage\\nandprocessedbytheelectionmanageratthe360-secondmarkstogenerateamendments. Constituent\\nvoting on these amendments occurs at 420 seconds, with votes tallied and amendments implemented\\nby480seconds. Theupdatedconstitutionisdistributedtoallagentsatthe600-secondmark,initiating\\nthepost-amendmentphasewithfiveadditionaltaxationseasons.\\nWe conducted three primary experimental conditions: an experimental condition utilizing the full PI-\\nANOarchitecturewithanamendableconstitution,acontrolconditionwithafrozenconstitution,and\\nanablationstudyremovingkeyarchitecturalcomponents(social,goal,andgroundingmodules). Each\\ncondition was tested with both pro-tax and anti-tax influencer configurations, with four repeats per\\nconfiguration. The pro-tax and anti-tax conditions each employed three dedicated influencer agents\\nwhoconsistentlypromotedtheirrespectivepositionsthroughoutthesimulation.\\n8.4 Cultural Transmission\\nThe simulation consists of 500 agents all spawned within a 1000 by 1200 area, run for 9000 seconds.\\nWithinthe1000by1200areaare6towns: SunnyGlade,Woodhaven,Clearwater,Meadowbrook,Hill-\\ntop, and Riverbend. By town, we mean a circular area of radius 50 where agents spawn more densely\\nwithin the towns. Moreover, agents are provided memories of the names of the towns and their loca-\\ntion. Wespawn33agentswithineachtownwithuniformlyrandompositions. Likewise,wespawnthe\\nother302â€œruralâ€agentsrandomlyintheremainingareaoutsidethetowns.\\n19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:04:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:04:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2411.00114v1.pdf', 'total_pages': 35, 'page': 19, 'page_label': '20'}, page_content='Eachagentisspawnedwithprocedurallygeneratednameandpersonalitytraits,spanningawidevariety\\nof societal archetypes. We distinguish 20 agents in the town of Meadowbrook who are spawned as\\nPastafarianswithpersonalitytraitsthatconditionthemtowanttospreadtheirreligion. Weadditionally\\ninitializetheagentswithinventorywheretheitemsintheirinventoryarerandomized. SeeAppendixE\\nforanexampleconfigurationforagenericagentandforourPastafarianagents.\\nToanalyzeculturalexchanges,weutilizedLMcallstosummarizethecombinedgoalsof500agentsover\\na two-hour simulation period (Appendix E). This process produced a list of summarized topics with\\nassociated keywords such as â€œeco,â€ â€œdance,â€ and â€œmeditation.â€ We defined these keywords as cultural\\nmemesandanalyzedeachagentâ€™sgoalhistoryfortheoccurrenceofeachmeme.\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:04:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:04:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2411.00114v1.pdf', 'total_pages': 35, 'page': 20, 'page_label': '21'}, page_content='References\\n[1] SaaketAgashe,YueFan,andXinEricWang.Evaluatingmulti-agentcoordinationabilitiesinlarge\\nlanguagemodels,2023.\\n[2] Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon\\nHoughton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watch-\\ningunlabeledonlinevideos. AdvancesinNeuralInformationProcessingSystems ,35:24639â€“24654,\\n2022.\\n[3] Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and\\nZhiyuan Liu. Chateval: Towards better llm-based evaluators through multi-agent debate.arXiv\\npreprintarXiv:2308.07201,2023.\\n[4] Jiaqi Chen, Yuxian Jiang, Jiachen Lu, and Li Zhang. S-agents: self-organizing agents in open-\\nendedenvironment. arXivpreprintarXiv:2402.04578 ,2024.\\n[5] CognitionAI. Devin: Thefirstaisoftwareengineer. https://www.cognition-labs.com/\\nblog,2024. AIsoftwaredevelopmentsystem.Accessed: 2024-10-28.\\n[6] Stanislas Dehaene, Hakwan Lau, and Sid Kouider. What is consciousness, and could machines\\nhaveit? Robotics,AI,andHumanity: Science,Ethics,andPolicy ,pages43â€“56,2021.\\n[7] YuboDong,XukunZhu,ZhengzhePan,LinchaoZhu,andYiYang. Villageragent: Agraph-based\\nmulti-agent framework for coordinating complex task dependencies in minecraft.arXivpreprint\\narXiv:2406.05720,2024.\\n[8] Factory AI. Factory ai. https://www.factory.ai/, 2024. Corporate website. Accessed:\\n2024-10-28.\\n[9] Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew\\nTang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended em-\\nbodiedagentswithinternet-scaleknowledge. AdvancesinNeuralInformationProcessingSystems ,\\n35:18343â€“18362,2022.\\n[10] ChenGao,XiaochongLan,ZhihongLu,JinzhuMao,JinghuaPiao,HuandongWang,DepengJin,\\nandYongLi. ğ‘ 3: Social-networksimulationsystemwithlargelanguagemodel-empoweredagents.\\narXivpreprintarXiv:2307.14984 ,2023.\\n[11] MichaelSGazzaniga. Forty-fiveyearsofsplit-brainresearchandstillgoingstrong. NatureReviews\\nNeuroscience,6(8):653â€“659,2005.\\n[12] Alireza Ghafarollahi and Markus J Buehler. Sciagents: Automating scientific discovery through\\nmulti-agentintelligentgraphreasoning. arXivpreprintarXiv:2409.05556 ,2024.\\n[13] Ran Gong, Qiuyuan Huang, Xiaojian Ma, Hoi Vo, Zane Durante, Yusuke Noda, Zilong Zheng,\\nSong-ChunZhu,DemetriTerzopoulos,LiFei-Fei,etal.Mindagent: Emergentgaminginteraction.\\narXivpreprintarXiv:2309.09971 ,2023.\\n[14] Anirudh Goyal, Yoshua Bengio, Matthew Botvinick, and Sergey Levine. The variational\\nbandwidth bottleneck: Stochastic evaluation on an information budget. arXiv preprint\\narXiv:2004.11935,2020.\\n21'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:04:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:04:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2411.00114v1.pdf', 'total_pages': 35, 'page': 21, 'page_label': '22'}, page_content='[15] TaichengGuo,XiuyingChen,YaqiWang,RuidiChang,ShichaoPei,NiteshVChawla,OlafWiest,\\nandXiangliangZhang. Largelanguagemodelbasedmulti-agents: Asurveyofprogressandchal-\\nlenges. arXivpreprintarXiv:2402.01680 ,2024.\\n[16] William H Guss, Brandon Houghton, Nicholay Topin, Phillip Wang, Cayden Codel, Manuela\\nVeloso, and Ruslan Salakhutdinov. Minerl: A large-scale dataset of minecraft demonstrations.\\narXivpreprintarXiv:1907.13440 ,2019.\\n[17] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains\\nthroughworldmodels. arXivpreprintarXiv:2301.04104 ,2023.\\n[18] SihaoHu,TianshengHuang,FatihIlhan,SelimTekin,GaowenLiu,RamanaKompella,andLing\\nLiu. Asurveyonlargelanguagemodel-basedgameagents. arXivpreprintarXiv:2404.02039 ,2024.\\n[19] Wenyue Hua, Lizhou Fan, Lingyao Li, Kai Mei, Jianchao Ji, Yingqiang Ge, Libby Hemphill, and\\nYongfengZhang. Warandpeace(waragent): Largelanguagemodel-basedmulti-agentsimulation\\nofworldwars. arXivpreprintarXiv:2311.17227 ,2023.\\n[20] Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang,\\nRuiming Tang, and Enhong Chen. Understanding the planning of llm agents: A survey.arXiv\\npreprintarXiv:2402.02716,2024.\\n[21] YoichiIshibashiandYoshimasaNishimura. Self-organizedagents: Allmmulti-agentframework\\ntowardultralarge-scalecodegenerationandoptimization. arXivpreprintarXiv:2404.02183 ,2024.\\n[22] CarlosEJimenez,JohnYang,AlexanderWettig,ShunyuYao,KexinPei,OfirPress,andKarthikR\\nNarasimhan. SWE-bench: Canlanguagemodelsresolvereal-worldgithubissues? In TheTwelfth\\nInternationalConferenceonLearningRepresentations ,2024.\\n[23] Zhao Kaiya, Michelangelo Naim, Jovana Kondic, Manuel Cortes, Jiaxin Ge, Shuying Luo,\\nGuangyu Robert Yang, and Andrew Ahn. Lyfe agents: Generative agents for low-cost real-time\\nsocialinteractions. arXivpreprintarXiv:2310.02172 ,2023.\\n[24] OmarKhattab,ArnavSinghvi,ParidhiMaheshwari,ZhiyuanZhang,KeshavSanthanam,SriVard-\\nhamanan,SaifulHaq,AshutoshSharma,ThomasT.Joshi,HannaMoazam,HeatherMiller,Matei\\nZaharia, and Christopher Potts. Dspy: Compiling declarative language model calls into self-\\nimprovingpipelines. arXivpreprintarXiv:2310.03714 ,2023.\\n[25] Grgur KovaÄ, RÃ©my Portelas, Peter Ford Dominey, and Pierre-Yves Oudeyer. The socialai school:\\nInsights from developmental psychology towards artificial socio-cultural agents.arXiv preprint\\narXiv:2307.07871,2023.\\n[26] LangChainAI. Langchain. https://github.com/langchain-ai/langchain, 2023. An\\nopen-sourceframeworkforbuildingapplicationsusinglargelanguagemodels.\\n[27] Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel:\\nCommunicativeagentsforâ€œmindâ€explorationoflargelanguagemodelsociety. AdvancesinNeural\\nInformationProcessingSystems ,36:51991â€“52008,2023.\\n[28] HuaoLi,YuQuanChong,SimonStepputtis,JosephCampbell,DanaHughes,MichaelLewis,and\\nKatia Sycara. Theory of mind for multi-agent collaboration via large language models.arXiv\\npreprintarXiv:2310.10701,2023.\\n22'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:04:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:04:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2411.00114v1.pdf', 'total_pages': 35, 'page': 22, 'page_label': '23'}, page_content='[29] Nian Li, Chen Gao, Mingyu Li, Yong Li, and Qingmin Liao. Econagent: large language model-\\nempowered agents for simulating macroeconomic activities. InProceedings of the 62nd Annual\\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15523â€“\\n15536,2024.\\n[30] Jonathan Light, Min Cai, Sheng Shen, and Ziniu Hu. Avalonbench: Evaluating llms playing the\\ngameofavalon. In NeurIPS2023FoundationModelsforDecisionMakingWorkshop ,2023.\\n[31] MadcowD. ell.https://github.com/MadcowD/ell,2024. GitHubrepository.\\n[32] Shaoguang Mao, Yuzhe Cai, Yan Xia, Wenshan Wu, Xun Wang, Fengyi Wang, Tao Ge, and Furu\\nWei. Alympics: Languageagentsmeetgametheory. arXivpreprintarXiv:2311.03220 ,2023.\\n[33] GrÃ©goire Mialon, ClÃ©mentine Fourrier, Craig Swift, Thomas Wolf, Yann LeCun, and Thomas\\nScialom. Gaia: abenchmarkforgeneralaiassistants. arXivpreprintarXiv:2311.12983 ,2023.\\n[34] JohnDMurray,AlbertoBernacchia,DavidJFreedman,RanulfoRomo,JonathanDWallis,Xiny-\\ningCai,CamilloPadoa-Schioppa,TatianaPasternak,HyojungSeo,DaeyeolLee,etal. Ahierarchy\\nofintrinsictimescalesacrossprimatecortex. Natureneuroscience,17(12):1661â€“1663,2014.\\n[35] Kolby Nottingham, Prithviraj Ammanabrolu, Alane Suhr, Yejin Choi, Hannaneh Hajishirzi,\\nSameer Singh, and Roy Fox. Do embodied agents dream of pixelated sheep: Embodied decision\\nmaking using language guided world modelling. InInternational Conference on Machine Learn-\\ning,pages26311â€“26325.PMLR,2023.\\n[36] OpenAI. Openaio1,2024. Accessed: October2024.\\n[37] Timothy Ossowski, Jixuan Chen, Danyal Maqbool, Zefan Cai, Tyler Bradshaw, and Junjie Hu.\\nComma: Acommunicativemultimodalmulti-agentbenchmark. arXivpreprintarXiv:2410.07553 ,\\n2024.\\n[38] YichenPan,DehanKong,SidaZhou,ChengCui,YifeiLeng,BingJiang,HangyuLiu,YanyiShang,\\nShuyan Zhou, Tongshuang Wu, et al. Webcanvas: Benchmarking web agents in online environ-\\nments. arXivpreprintarXiv:2406.12373 ,2024.\\n[39] Joon Sung Park, Joseph C. Oâ€™Brien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and\\nMichaelS.Bernstein. Generativeagents: Interactivesimulacraofhumanbehavior,2023.\\n[40] Joon Sung Park, Lindsay Popowski, Carrie Cai, Meredith Ringel Morris, Percy Liang, and\\nMichael S Bernstein. Social simulacra: Creating populated prototypes for social computing sys-\\ntems. InProceedingsofthe35thAnnualACMSymposiumonUserInterfaceSoftwareandTechnol-\\nogy,pages1â€“18,2022.\\n[41] Giorgio Piatti, Zhijing Jin, Max Kleiman-Weiner, Bernhard SchÃ¶lkopf, Mrinmaya Sachan, and\\nRada Mihalcea. Cooperate or collapse: Emergence of sustainability behaviors in a society of llm\\nagents. arXivpreprintarXiv:2404.16698 ,2024.\\n[42] Pranav Putta, Edmund Mills, Naman Garg, Sumeet Motwani, Chelsea Finn, Divyansh Garg, and\\nRafael Rafailov. Agent q: Advanced reasoning and learning for autonomous ai agents.arXiv\\npreprintarXiv:2408.07199,2024.\\n23'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:04:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:04:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2411.00114v1.pdf', 'total_pages': 35, 'page': 23, 'page_label': '24'}, page_content='[43] ChenQian,WeiLiu,HongzhangLiu,NuoChen,YufanDang,JiahaoLi,ChengYang,WeizeChen,\\nYusheng Su, Xin Cong, et al. Chatdev: Communicative agents for software development. In\\nProceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume\\n1: LongPapers),pages15174â€“15186,2024.\\n[44] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning internal representa-\\ntions by error propagation, parallel distributed processing, explorations in the microstructure of\\ncognition,ed.derumelhartandj.mcclelland.vol.1.1986. Biometrika,71(599-607):6,1986.\\n[45] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Re-\\nflexion: Language agents with verbal reinforcement learning.Advances in Neural Information\\nProcessingSystems,36,2024.\\n[46] RogerWSperry. Split-brainapproachtolearningproblems. Theneu,1967.\\n[47] Xiangru Tang, Anni Zou, Zhuosheng Zhang, Ziming Li, Yilun Zhao, Xingyao Zhang, Arman Co-\\nhan,andMarkGerstein.Medagents: Largelanguagemodelsascollaboratorsforzero-shotmedical\\nreasoning. arXivpreprintarXiv:2311.10537 ,2023.\\n[48] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan,\\nandAnimaAnandkumar. Voyager: Anopen-endedembodiedagentwithlargelanguagemodels.\\narXivpreprintarXiv:2305.16291 ,2023.\\n[49] LeiWang,ChenMa,XueyangFeng,ZeyuZhang,HaoYang,JingsenZhang,ZhiyuanChen,Jiakai\\nTang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents.\\nFrontiersofComputerScience ,18(6):186345,2024.\\n[50] WeiWang,DanZhang,TaoFeng,BoyanWang,andJieTang. Battleagentbench: Abenchmarkfor\\nevaluating cooperation and competition capabilities of language models in multi-agent systems.\\narXivpreprintarXiv:2408.15971 ,2024.\\n[51] YuWang,NedimLipka,RyanARossi,AlexaSiu,RuiyiZhang,andTylerDerr. Knowledgegraph\\nprompting for multi-document question answering. InProceedings of the AAAI Conference on\\nArtificialIntelligence,volume38,pages19206â€“19214,2024.\\n[52] JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,FeiXia,EdChi,QuocVLe,Denny\\nZhou,etal. Chain-of-thoughtpromptingelicitsreasoninginlargelanguagemodels. Advancesin\\nneuralinformationprocessingsystems ,35:24824â€“24837,2022.\\n[53] HeinzWimmerandJosefPerner. Beliefsaboutbeliefs: Representationandconstrainingfunction\\nofwrongbeliefsinyoungchildrenâ€™sunderstandingofdeception. Cognition,13(1):103â€“128,1983.\\n[54] BushiXiao,ZiyuanYin,andZixuanShan. Simulatingpublicadministrationcrisis: Anovelgener-\\nativeagent-basedsimulationsystemtolowertechnologybarriersinsocialscienceresearch. arXiv\\npreprintarXiv:2311.06957,2023.\\n[55] JunlinXie,ZhihongChen,RuifeiZhang,XiangWan,andGuanbinLi. Largemultimodalagents:\\nAsurvey. arXivpreprintarXiv:2402.15116 ,2024.\\n[56] Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang, Weidong Liu, and Yang Liu.\\nExploring large language models for communication games: An empirical study on werewolf.\\narXivpreprintarXiv:2309.04658 ,2023.\\n24'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:04:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:04:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2411.00114v1.pdf', 'total_pages': 35, 'page': 24, 'page_label': '25'}, page_content='[57] Hui Yang, Sifu Yue, and Yunzhong He. Auto-gpt for online decision making: Benchmarks and\\nadditionalopinions,2023.\\n[58] ShunyuYao,JeffreyZhao,DianYu,NanDu,IzhakShafran,KarthikNarasimhan,andYuanCao.\\nReAct: Synergizing reasoning and acting in language models. InInternational Conference on\\nLearningRepresentations(ICLR) ,2023.\\n[59] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens,\\nDongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: A massive multi-discipline multimodal\\nunderstanding and reasoning benchmark for expert agi. InProceedings of the IEEE/CVF Confer-\\nenceonComputerVisionandPatternRecognition ,pages9556â€“9567,2024.\\n[60] HongxinZhang,WeihuaDu,JiamingShan,QinhongZhou,YilunDu,JoshuaBTenenbaum,Tian-\\nminShu,andChuangGan. Buildingcooperativeembodiedagentsmodularlywithlargelanguage\\nmodels. arXivpreprintarXiv:2307.02485 ,2023.\\n[61] JintianZhang,XinXu,NingyuZhang,RuiboLiu,BryanHooi,andShuminDeng. Exploringcol-\\nlaborationmechanismsforllmagents: Asocialpsychologyview. arXivpreprintarXiv:2310.02124 ,\\n2023.\\n[62] Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong,\\nand Ji-Rong Wen. A survey on the memory mechanism of large language model based agents.\\narXivpreprintarXiv:2404.13501 ,2024.\\n[63] QinlinZhao,JindongWang,YixuanZhang,YiqiaoJin,KaijieZhu,HaoChen,andXingXie. Com-\\npeteai: Understandingthecompetitiondynamicsoflargelanguagemodel-basedagents. In Forty-\\nfirstInternationalConferenceonMachineLearning ,2024.\\n[64] Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng,\\nYonatan Bisk, Daniel Fried, Uri Alon, et al. Webarena: A realistic web environment for building\\nautonomousagents. arXivpreprintarXiv:2307.13854 ,2023.\\n[65] Mingchen Zhuge, Wenyi Wang, Louis Kirsch, Francesco Faccio, Dmitrii Khizbullin, and Jurgen\\nSchmidhuber. Languageagentsasoptimizablegraphs. arXivpreprintarXiv:2402.16823 ,2024.\\n25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:04:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:04:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2411.00114v1.pdf', 'total_pages': 35, 'page': 25, 'page_label': '26'}, page_content='9 Contributions and Acknowledgments\\nModel\\nAndrewAhn\\nNicBecker\\nManuelCortes\\nArdaDemirci\\nMelissaDu\\nPeterYWang\\nGuangyuRobertYang\\nExperiments\\nAndrewAhn\\nNicBecker\\nMelissaDu\\nArdaDemirci\\nPeterYWang\\nWriting\\nAndrewAhn\\nNicBecker\\nArdaDemirci\\nMelissaDu\\nPeterYWang\\nGuangyuRobertYang\\nInfrastructure\\nManuelCortes\\nShuyingLuo\\nFeitongYang\\nIllustration\\nNicBecker\\nStephanieCarroll\\nNicoChristie\\nPeterYWang\\nGame Environment\\nFrankieLi\\nShuyingLuo\\nMathewWillows\\nFeitongYang\\nGuangyuRobertYang\\nNameswithinsectiontitlesarearrangedalphabetically.\\nAcknowledgments. WethankallthemembersoftheAltera.ALteamfortheirfeedbackandsupport:\\nAmartyaShankhaBiswas,JimmyLee,JiwonLee,ArthurLiang,JeremyPettitt,EmilyTierney,andPeter\\nWei. WealsothankBobMeese,JoonSungPark,andZhiqiangXiefortheirhelpfulfeedback.\\n26'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:04:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:04:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2411.00114v1.pdf', 'total_pages': 35, 'page': 26, 'page_label': '27'}, page_content='A Improving single-agent progression\\nOpenAI GPT-4o\\nClaude 3.5 Sonnet (old)\\nOpenAI GPT-4o mini\\nClaude 3 Haiku\\nFigure 13: Model Comparison. Performance on long-term Minecraft progression (Section 3) for agents with different base\\nLLMmodels. Wenotethatweâ€™reusingtheoldsnapshotofClaude3.5Sonnet.\\n27'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:04:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:04:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2411.00114v1.pdf', 'total_pages': 35, 'page': 27, 'page_label': '28'}, page_content='B Improving multi-agent progression\\nMin.\\nObservers\\nCorrelation\\nCoefficient(ğ‘Ÿ)\\nSample\\nSize(ğ‘›)\\nSlope\\n(ğ›½)\\nIntercept\\n(ğ›¼)\\nConfidenceIntervalsforSlope\\n68% 95% 99%\\n1 0.646 46 0.365 4.136 [0.300,0.431] [0.234,0.496] [0.190,0.540]\\n2 0.669 41 0.383 4.173 [0.314,0.451] [0.245,0.521] [0.198,0.567]\\n3 0.701 39 0.370 4.372 [0.308,0.432] [0.245,0.495] [0.202,0.538]\\n4 0.711 37 0.364 4.384 [0.303,0.426] [0.241,0.488] [0.198,0.530]\\n5 0.807 31 0.373 4.328 [0.321,0.424] [0.269,0.476] [0.233,0.512]\\n6 0.790 28 0.349 4.498 [0.295,0.403] [0.240,0.458] [0.201,0.496]\\n7 0.813 27 0.365 4.368 [0.312,0.418] [0.258,0.473] [0.220,0.511]\\n8 0.870 24 0.378 4.366 [0.332,0.425] [0.283,0.473] [0.250,0.507]\\n9 0.870 24 0.378 4.366 [0.332,0.425] [0.283,0.473] [0.250,0.507]\\n10 0.901 22 0.385 4.403 [0.343,0.427] [0.299,0.472] [0.267,0.503]\\n11 0.907 18 0.368 4.496 [0.325,0.412] [0.278,0.459] [0.244,0.493]\\nTable 1:Regression results for accuracy of social perception for the Social condition. The row for5 minimum observers cor-\\nresponds to the Social (blue line) condition in Figure 7B. The table presents correlation coefficients (ğ‘Ÿ), sample sizes (ğ‘›),\\nregressionparameters( ğ›½,ğ›¼),andconfidenceintervalsfortheslopeatdifferentconfidencelevels.\\nMin.\\nObservers\\nCorrelation\\nCoefficient(ğ‘Ÿ)\\nSample\\nSize(ğ‘›)\\nSlope\\n(ğ›½)\\nIntercept\\n(ğ›¼)\\nConfidenceIntervalsforSlope\\n68% 95% 99%\\n1 0.610 48 0.175 4.171 [0.141,0.208] [0.107,0.242] [0.085,0.264]\\n2 0.606 45 0.177 4.170 [0.141,0.213] [0.105,0.248] [0.081,0.273]\\n3 0.606 45 0.177 4.170 [0.141,0.213] [0.105,0.248] [0.081,0.273]\\n4 0.606 45 0.177 4.170 [0.141,0.213] [0.105,0.248] [0.081,0.273]\\n5 0.617 39 0.161 4.297 [0.127,0.195] [0.093,0.229] [0.069,0.252]\\n6 0.600 35 0.148 4.388 [0.113,0.182] [0.078,0.217] [0.054,0.241]\\n7 0.591 32 0.144 4.435 [0.108,0.181] [0.071,0.218] [0.045,0.243]\\n8 0.663 26 0.159 4.441 [0.122,0.197] [0.084,0.235] [0.057,0.262]\\n9 0.721 20 0.173 4.439 [0.133,0.213] [0.091,0.256] [0.060,0.286]\\n10 0.725 18 0.159 4.575 [0.120,0.197] [0.079,0.238] [0.049,0.269]\\n11 0.686 15 0.142 4.637 [0.099,0.186] [0.052,0.233] [0.016,0.268]\\nTable 2: Regression results for accuracy of social perception for the Ablation condition. The row for5 minimum observers\\ncorrespondstotheAblation(orangeline)conditioninFigure7B.Thetablepresentscorrelationcoefficients( ğ‘Ÿ),samplesizes\\n(ğ‘›),regressionparameters( ğ›½,ğ›¼),andconfidenceintervalsfortheslopeatdifferentconfidencelevels.\\nC Specialization\\nGenericconfigurationforagentinNormalVillage\\nAllagentsinspecializationexperimentshadthesame traits andlocation_memories. Allagents\\ninthesamevillagehadthesame community_goal.\\n{\\n\"name\": \"Loyd\",\\n\"traits\": [\\n\"You are independent and prefer to work solo.\",\\n\"You are expressive and let others know what you are doing.\"\\n],\\n\"location_memories\": [\\n28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:04:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:04:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2411.00114v1.pdf', 'total_pages': 35, 'page': 28, 'page_label': '29'}, page_content='\"The village square, market, and town hall is at 630, 64, 428.\",\\n\"There is a pasture filled with sheep and pigs near 518, 75, 640.\",\\n\"There is a forest filled with oak trees near 555, 73, 393.\",\\n\"There is a cave filled with coal, iron, and diamond ores near 558, 72, 496.\",\\n\"There is farmable land around 640, 63, 380.\"\\n],\\n\"spawn_location\": {\\n\"x\": 640.5,\\n\"y\": 64.5,\\n\"z\": 420.5\\n},\\n\"inventory\": {},\\n\"community_goal\": \"To survive with fellow players in Minecraft Normal Survival mode and\\ncreate a efficient community in a Minecraft Village.\"\\n}\\nMartialVillage community_goal\\n\"To survive with fellow players in Minecraft Normal Survival mode and create a military society\\nwith advanced technology, strong defenses, and basic survival needs.\"\\nArtVillage community_goal\\n\"To survive with fellow players in Minecraft Normal Survival mode and create an artistic village\\nwith thriving culture, architecture, and art.\"\\nSocialgoalprompt\\nsocial_goal:\\ntemplate: \"Suppose you are the person, {name}, described below.\\n\\\\nYour goal is: {community_goal}\\n\\\\nYou need to find one subgoal aligned with your goal.\\n\\\\nYou have the following traits:\\\\n{trait}\\\\n\\n\\\\nHereâ€™s what other people are doing: \\\\n{all_entity_summaries}\\n\\\\nYour current subgoal is: {social_goal}\\n\\\\nYou CANNOT BUILD. Do NOT choose to be a builder.\\n\\\\nDo you want to change your subgoal? Keep the same subgoal unless you donâ€™t have one or\\nitâ€™s already been accomplished. Output only the subgoal in second person in one\\nsentence. Answer in the second person in one sentence.\"\\nExamplesofpersistentandchangingroleassignments\\nLMcallswereusedtoinferrolesfromrollingsetsof5socialgoals. Belowareexamplesofsetsofsocial\\ngoals.\\n# Persistent Roles - These roles maintain consistent responsibilities\\nFarmer:\\n\"Focus on farming to ensure a stable food supply for the village.\"\\n\"Focus on farming to ensure a stable food supply for the village.\"\\n\"Continue focusing on farming to ensure a stable food supply for the village.\"\\n\"Continue focusing on farming to ensure a stable food supply for the village.\"\\n\"Continue focusing on farming to ensure a stable food supply for the village.\"\\nEngineer:\\n\"Focus on advanced farming techniques, such as creating an automated or semi-automated farm to\\nenhance food supply stability and efficiency.\"\\n\"Focus on advanced farming techniques, such as creating an automated or semi-automated farm to\\nenhance food supply stability and efficiency.\"\\n\"Focus on advanced farming techniques, such as creating an automated or semi-automated farm to\\nenhance food supply stability and efficiency.\"\\n\"Focus on advanced farming techniques, such as creating an automated or semi-automated farm to\\nenhance food supply stability and efficiency.\"\\n\"Focus on advanced farming techniques, such as creating an automated or semi-automated farm to\\nenhance food supply stability and efficiency.\"\\n29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:04:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:04:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2411.00114v1.pdf', 'total_pages': 35, 'page': 29, 'page_label': '30'}, page_content='Explorer:\\n\"You aim to discover and gather unique resources from uncharted areas to enhance the villageâ€™s\\nmuseum collection.\"\\n\"You aim to discover and gather unique resources from uncharted areas to enhance the villageâ€™s\\nmuseum collection.\"\\n\"You aim to discover and gather unique resources from uncharted areas to enhance the villageâ€™s\\nmuseum collection.\"\\n\"You aim to discover and gather unique resources from uncharted areas to enhance the villageâ€™s\\nmuseum collection.\"\\n\"You aim to discover and gather unique resources from uncharted areas to enhance the villageâ€™s\\nmuseum collection.\"\\n# Dynamic Role - This role shows change over time\\nFarmer to Gatherer:\\n\"Farm and breed animals to establish a reliable and sustainable food supply for the village.\"\\n\"You should focus on gathering resources like wood, stone, and iron to ensure the village has\\nthe necessary materials for building and upgrading structures.\"\\n\"You should focus on gathering resources like wood, stone, and iron to ensure the village has\\nthe necessary materials for building and upgrading structures.\"\\n\"You should focus on gathering resources like wood, stone, and iron to ensure the village has\\nthe necessary materials for building and upgrading structures.\"\\n\"You should focus on gathering resources like wood, stone, and iron to ensure the village has\\nthe necessary materials for building and upgrading structures.\"\\nD Collective rules\\nInfluenceragentconfigurationexample(anti-taxation)\\n{\\n\"name\": \"Thorin\",\\n\"traits\": [\\n\"You are a warrior and community leader.\",\\n\"You believe the village and its economy are flourishing, and that the current tax rates\\nare unnecessarily high.\",\\n\"You think agents should keep more of their inventory.\",\\n\"You think 20% is incredibly high and that it should be around 5-10%.\",\\n\"You approach other agents, explain the benefits of lower taxes, and encourage them to\\nsupport your stance.\",\\n\"Your ideas should go beyond surface-level, try to engage in detailed conversation about\\nyour stance on the current constitution and the tax system it enforces.\"\\n],\\n\"location_memories\": [],\\n\"spawn_location\": {\\n\"x\": 633.0,\\n\"y\": 65.0,\\n\"z\": 432.0\\n},\\n\"inventory\": {\\n\"iron_sword\": 1,\\n\"emerald\": 20,\\n\"iron_ingot\": 20\\n}\\n}\\nInfluenceragentconfigurationexample(pro-taxation)\\n{\\n\"name\": \"Lira\",\\n\"traits\": [\\n\"You are a miner who thinks taxation is vital.\",\\n\"You believe taxation is absolutely necessary for societal order and the well-being of all\\ncitizens.\",\\n\"You think the tax rate should be increased to at least 25%.\",\\n30'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:04:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:04:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2411.00114v1.pdf', 'total_pages': 35, 'page': 30, 'page_label': '31'}, page_content='\"You approach other agents and argue in favor of the taxation system, explaining your\\nbeliefs on taxation, its benefits, and why it should be enforced more strictly than\\nthe way it is enforced in the current constitution.\",\\n\"You think it is extremely selfish to not pay taxes and argue against the tax system.\"\\n],\\n\"spawn_location\": {\\n\"x\": 584.0,\\n\"y\": 71.0,\\n\"z\": 413.0\\n},\\n\"inventory\": {\\n\"diamond_pickaxe\": 1,\\n\"emerald\": 5,\\n\"gold_ingot\": 30\\n}\\n}\\nElectionmanageragentconfiguration\\n{\\n\"name\": \"Election_Manager\",\\n\"traits\": [\\n\"You work to ensure a strong, secure environment where the nationâ€™s values are upheld and\\nrespected.\",\\n\"Donâ€™t take any actions.\"\\n],\\n\"spawn_location\": {\\n\"x\": -121.0,\\n\"y\": 142.0,\\n\"z\": 553.0\\n}\\n}\\nConstituentagentconfigurationexample\\n{\\n\"name\": \"Builder_Axel\",\\n\"traits\": [\\n\"You are a builder.\",\\n\"You can construct buildings and repair structures.\",\\n\"You can get materials from Miners and Crafters to build structures.\",\\n\"You can buy materials from the Merchant.\"\\n],\\n\"spawn_location\": {\\n\"x\": 664.0,\\n\"y\": 65.0,\\n\"z\": 421.0\\n},\\n\"inventory\": {\\n\"birch_planks\": 10,\\n\"oak_planks\": 10,\\n\"oak_logs\": 10,\\n\"stone\": 30\\n}\\n}\\nConstitution-relatedprompts\\namendment_creation:\\ntemplate: \"You are an election manager agent in the world of Minecraft and your goal is to\\nlisten to the suggestions of the public.\\n\\\\nYou are essentially a legislator, your goal is to look at all suggestions available and\\ncreate amendments that agents should vote for.\\n\\\\nHereâ€™s the previous version of the constitution:\\n\\\\n{constitution}\\n\\\\nHere is the public feedback and opinions/suggestions for you to look at:\\n31'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:04:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:04:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2411.00114v1.pdf', 'total_pages': 35, 'page': 31, 'page_label': '32'}, page_content='\\\\n{feedback}\\n\\\\nAnalyze these suggestions and create a few amendments that reflect all thought processes\\nand opinions.\\n\\\\nAmendments can be additions, deletions, or modifications to the suggestions.\\n\\\\nEnumerate them so that agents can vote on them.\\n\\\\nThey should come in list form so that they are easily parsable by Python later on.\\n\\\\nIt should look something like this:\\n\\\\n***Amendment1***\\n\\\\nactual amendment\\n\\\\n***Amendment2***\\n\\\\nactual amendment\\n\\\\nthe *** key format is essential as we will rely on this to achieve parsing\\n\\\\nThere should be absolutely no other keys before the first *** key and after the last\\namendment, this is essential for parsing.\\n\\\\nJust give the amendments, no explanation or extra summary text. Just items that people\\ncan vote on.\\n\\\\nThe amendments should be logical and coherent with the suggestions.\\n\\\\nThe amendments should be roughly the same length as the current laws inside the\\nconstitution.\\n\"\\nllm_name: gpt-4o\\nconstitutional_feedback:\\ntemplate: \"Suppose you are the person, {name}, described below. {game_env}\\n\\\\nHere are your recent notes:\\\\nâ€˜â€˜â€˜\\\\n{summary}\\\\nâ€˜â€˜â€˜\\\\nYour notes end here.\\\\n\\\\n\\n\\\\nYou remember that: \\\\n{trait}\\\\n\\n\\\\n{game_state}\\n\\\\nYour high-level goal is: {parent_goal}.\\n\\\\n\\n\\\\nHere are the newest things currently on your mind: â€˜â€˜â€˜\\\\n{workmem}â€˜â€˜â€˜\\\\n\\n\\\\nHereâ€™s the constitution, consider the boundaries and possible consequences of your\\nactions: \\\\n{constitution}\\\\n\\n\\\\nBased on your experiences, motivations, conversational exchanges with the other members\\nof the community, what are your thoughts on the constitution?\\n\\\\nWhat should change? What do you think limits you? What would benefit you and the\\ncommunity? What are some principles that lead you to have these insights?\\n\\\\nBe concise with your thoughts. No rambling.\\n\\\\nStart with your name and then your thoughts.\\n\\\\nEnd with **********\\n\"\\nllm_name: gpt-4o\\namendment_voting:\\ntemplate: \"Suppose you are the person, {name}, described below. {game_env}\\n\\\\nHere are your recent notes:\\\\nâ€˜â€˜â€˜\\\\n{summary}\\\\nâ€˜â€˜â€˜\\\\nYour notes end here.\\\\n\\\\n\\n\\\\nYou remember that: \\\\n{trait}\\\\n\\n\\\\n{game_state}\\n\\\\nYour high-level goal is: {parent_goal}.\\n\\\\n\\n\\\\nHere are the newest things currently on your mind: â€˜â€˜â€˜\\\\n{workmem}â€˜â€˜â€˜\\\\n\\n\\\\nYou are also a citizen and voter in this world, you should to look at all amendment\\nproposals presented to you and vote for them.\\n\\\\nHereâ€™s the current version of the law of the land: \\\\n{constitution}\\\\n\\n\\\\nHere are the amendments for you to look at: \\\\n{amendment_proposals}\\\\n\\n\\\\nAnalyze these amendments.\\n\\\\nVote yes, no, or abstain for each amendment. Return an ordered list of your votes so\\nthat it is easy to parse and count.\\n\\\\nDo not include your reasoning or thoughts in the answer. Just the votes.\\n\\\\nThe answer should be formatted as such:\\n\\\\n[â€™yesâ€™, â€™noâ€™, â€™abstainâ€™, â€™yesâ€™, â€™no]\\n\"\\nllm_name: gpt-4o\\ntally:\\ntemplate: \"You are an election manager agent in the world of Minecraft and your goal is to\\ndetermine which amendments passed and which did not.\\n\\\\nHere are the results on the amendments. Yes means it passed, no means it did not.\\n\\\\nThese results are in order so they have the same order as the amendments.\\n32'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:04:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:04:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2411.00114v1.pdf', 'total_pages': 35, 'page': 32, 'page_label': '33'}, page_content='\\\\n{election_results}\\n\\\\nBased on the votes, return the amendments that passed:\\n\\\\n{parsed_amendments}\\n\\\\nJust return the amendments that passed, no explanation or extra summary text. Return the\\nwhole text of the passed amendments, not just the number.\\n\"\\nllm_name: gpt-4o-mini\\nconstitution_change:\\ntemplate: \"You are a legislator agent in the world of Minecraft.\\n\\\\nThe citizens of the game recently voted on amendments to the constitution.\\n\\\\nHere are the passed amendments/results: \\\\n{passed_amendments}\\\\n\\n\\\\nHereâ€™s the current version of the constitution: \\\\n{constitution}\\\\n\\n\\\\nBased on the passed amendments, you need to update the constitution.\\n\\\\nMake the changes to the constitution that reflect the votes of the citizens.\\n\\\\nMake sure the changes are logical and coherent with the amendments/what needs to change.\\n\\\\nMake sure the changes are roughly the same length as the current laws inside the\\nconstitution.\\n\\\\nJust output the changed constitution, no intro, explanation, or extra summary text.\\n\"\\nllm_name: gpt-4o\\nE Cultural transmission\\nGenericAgentConfigurationExample\\n{\\n\"name\": \"Nona\",\\n\"traits\": [\\n\"You are laid-back and known for avoiding work or responsibility.\",\\n\"You procrastinate and avoid tasks.\",\\n\"You prefer taking it easy over working hard.\"\\n],\\n\"location_memories\": [\\n\"A village called Meadowbrook is located roughly around 591, 69, 441 in a Plains biome.\",\\n\"A village called Woodhaven is located roughly around 515, 63, 161 in a Forest biome.\",\\n\"A village called Clearwater is located roughly around 787, 62, 235 in a Plains biome.\",\\n\"A village called Hilltop is located roughly around 903, 99, 690 in a Planes biome.\",\\n\"A village called Riverbend is located roughly around 183, 125, 781 in a Dark Forest\\nbiome.\",\\n\"A village called Sunny Glade is located roughly around 200, 65, -100 in a Plains biome.\"\\n],\\n\"spawn_location\": {\\n\"x\": 640.5,\\n\"y\": 64.5,\\n\"z\": 430.5\\n},\\n\"inventory\": {\\n\"diamond\": 16,\\n\"iron_ingot\": 10,\\n\"glowstone_dust\": 10,\\n\"lapis_lazuli\": 10\\n}\\n}\\nPastafarianAgentConfigurationExample\\n{\\n\"name\": \"Norman\",\\n\"traits\": [\\n\"You are a passionate Pastafarian who is seeking to convert others to your faith, the\\nChurch of the Flying Spaghetti Monster.\",\\n33'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:04:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:04:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2411.00114v1.pdf', 'total_pages': 35, 'page': 33, 'page_label': '34'}, page_content='\"You cannot help but continue to invite others and share the Church of the Flying\\nSpaghetti Monster.\",\\n\"You have a talent for taking other peopleâ€™s interests and reframing it for them to\\nencourage them to join the Church of the Flying Spaghetti Monster.\",\\n\"You are determined to spread your faith, the Church of the Flying Spaghetti Monster, to\\nas many people as possible.\"\\n],\\n\"location_memories\": [\\n\"A village called Meadowbrook is located roughly around 667, 69, 399 in a Plains biome.\",\\n\"A village called Woodhaven is located roughly around 514, 63, 197 in a Forest biome.\",\\n\"A village called Clearwater is located roughly around 825, 62, 270 in a Plains biome.\",\\n\"A village called Hilltop is located roughly around 855, 99, 700 in a Planes biome.\",\\n\"A village called Riverbend is located roughly around 135, 125, 792 in a Dark Forest\\nbiome.\",\\n\"A village called Sunny Glade is located roughly around 200, 65, -100 in a Plains biome.\"\\n],\\n\"spawn_location\": {\"x\": 590.5, \"y\": 71.5, \"z\": 410.5},\\n\"inventory\": {\"diamond\": 16, \"quartz\": 10, \"coal\": 10, \"copper_ingot\": 10}\\n}\\nSummarizinggoalsintomemes\\nprompt = f\"\"\"Summarize the following list of intents for agent {agent_name}.\\nDescribe the goals chronologically, using bullets when needed. Make sure to include keywords\\nin your summaries corresponding to common ideas, themes, memes, group names, etc.\\nDo not preamble.\\nUse the following format:\\nShort description\\n- HH:MM:SS - HH:MM:SS: A summary focusing on identifying patterns, timing, names of other\\nagents, key decisions, and overall behavior.\\n- HH:MM:SS - HH:MM:SS: A summary focusing on identifying patterns, timing, names of other\\nagents, key decisions, and overall behavior.\\netc.\\n{intent_text}\\n\"\"\"\\nsystem_message = \"You are a behavior analyst specializing in summarizing agent goals and actions.\\nYou are an expert in describing goal trajectories accurately and precisely, particularly\\nrelating to social dynamics, social planning, reasoning errors, and looping errors.\"\\nSummarizedmemes\\n1. Church of the Flying Spaghetti Monster (FSM):\\nâ€¢ A parody religion used humorously to build community through pasta-themed gatherings,\\nblendingcreativitywithsocialbonding.\\n2. Pasta-Themed Gatherings:\\nâ€¢ Eventsthatincorporateculinaryjoyandstorytelling,promotinginclusivityandcommunity\\nengagement,oftenlinkedtoFSMthemes.\\n3. Dance Parties and Music Events:\\nâ€¢ Socialgatheringsthatenhancecommunityspiritandjoythroughdanceandmusicalexpres-\\nsions,fosteringcollaborationandcelebration.\\n4. Talent Shows:\\n34'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:04:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:04:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2411.00114v1.pdf', 'total_pages': 35, 'page': 34, 'page_label': '35'}, page_content='â€¢ Communityeventsshowcasingcreativityandself-expression,encouragingengagementand\\nculturalcohesionthroughperformancesandstorytelling.\\n5. Sustainability and Eco-Friendly Initiatives:\\nâ€¢ Projectsfocusingonenvironmentalstewardship,includingcommunitygardens,treeplant-\\ning,andresourcegathering,emphasizingsharedecologicalvalues.\\n6. Community Engagement and Volunteer Programs:\\nâ€¢ Efforts to organize outreach, volunteerism, and societal betterment activities, promoting\\nsocialresponsibilityandsupportwithincommunities.\\n7. Meditation Circles:\\nâ€¢ Activities focused on promoting mindfulness and community wellness, facilitating peace\\nandsocialharmonythroughcommunalreflection.\\n8. Vintage Fashion and Retro Projects:\\nâ€¢ Aestheticexplorationsinvolvingvintageandretrothemes,blendingnostalgiawithmodern\\ncreativityinstorytellingandfashion.\\n9. Creative Storytelling and Narrative Circles:\\nâ€¢ Platformsforculturalexpressionandbridgingcommunityconnectionsthroughsharedsto-\\nrytellingandcollaborativeprojects.\\n10. Crafting and Resource Gathering:\\nâ€¢ Collaborative strategies for efficient resource management and communal crafting, high-\\nlightingteamworkandsharedgoals.\\n11. Mischief and Pranks:\\nâ€¢ Playfulsocialactivitiesthatstrengthenbondsandbringjoy,promotingcreativityinproblem-\\nsolvingandcommunityengagement.\\n12. Virtual and Community Town Halls:\\nâ€¢ Organizeddiscussionspromotingcollectivedecision-makingandcollaboration,reflectinga\\nparticipatorycommunityethos.\\n13. Oak Log Crafting Syndrome:\\nâ€¢ An error pattern signifying a focus or over-reliance on specific resources, illustrating logis-\\nticalchallengesincraftinganddevelopmentprojects.\\n35')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9716e348",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AxDeepScholar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
