from docling.document_converter import DocumentConverter
import fitz, pdfplumber, layoutparser as lp, cv2, os

import numpy as np
os.environ["DOC_ACCELERATOR_DEVICE"] = "cpu"
os.environ["DOC_ACCELERATOR_BACKEND"] = "cpu"
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["OMP_NUM_THREADS"] = "1"
os.environ["OPENBLAS_NUM_THREADS"] = "1"
os.environ["MKL_NUM_THREADS"] = "1"
os.environ["DOCLING_PICTURE_DESCRIPTIONS"] = "" 

# â€”â€”â€” 1ï¸âƒ£ í…ìŠ¤íŠ¸ ë ˆì´ì–´ í™•ì¸ â€”â€”â€”
def has_text_layer(pdf_path: str) -> bool:
    from PyPDF2 import PdfReader
    reader = PdfReader(pdf_path)
    for pg in reader.pages[:3]:
        txt = pg.extract_text()
        if txt and len(txt.strip()) > 50:
            return True
    return False

# â€”â€”â€” 2ï¸âƒ£ ì¢Œí‘œ ê¸°ë°˜ í…ìŠ¤íŠ¸ ì •ë ¬ (PyMuPDF) â€”â€”â€”
def extract_text_mupdf_clean_strict(pdf_path: str):
    """
    - í˜ì´ì§€ ìƒ/í•˜/ëª¨ì„œë¦¬/ì‘ì€ í°íŠ¸ ì œê±°
    - 'ë¶™ì–´ë²„ë¦° ë‹¨ì–´'ë¥¼ x-ì¢Œí‘œ ê°„ê²© ê¸°ë°˜ìœ¼ë¡œ ìŠ¤í˜ì´ìŠ¤ ë³µì›
    - ë‘ë‹¨ ë¬¸ì„œëŠ” ì™¼ìª½ ì¹¼ëŸ¼ ì „ì²´ â†’ ì˜¤ë¥¸ìª½ ì¹¼ëŸ¼ ì „ì²´ ìˆœì„œ ìœ ì§€ (A1 A2 A3 B1 B2 B3)
    """
    import fitz, numpy as np, re

    doc = fitz.open(pdf_path)
    all_pages = []

    for page in doc:
        page_w, page_h = page.rect.width, page.rect.height
        data = page.get_text("dict")
        if not data or "blocks" not in data:
            continue

        # 1) ë¼ì¸ ë‹¨ìœ„ë¡œ ìŠ¤íŒ¬ì„ ëª¨ìœ¼ë©´ì„œ, í˜ì´ì§€ ì—¬ë°±/í°íŠ¸ í¬ê¸°/ë³¸ë¬¸ í”„ë ˆì„ìœ¼ë¡œ 1ì°¨ í•„í„°
        line_items = []  # [(y_top, x_min, line_text), ...]
        for b in data["blocks"]:
            for line in b.get("lines", []):
                spans = []
                for span in line.get("spans", []):
                    txt = span.get("text", "")
                    if not txt:
                        continue
                    x0, y0, x1, y1 = span["bbox"]
                    fsz = span.get("size", 0)

                    # ìƒë‹¨/í•˜ë‹¨ 10% ì»· + ëª¨ì„œë¦¬ ì»· + ë„ˆë¬´ ì‘ì€ í°íŠ¸ ì»·
                    if y0 < page_h * 0.20 or y1 > page_h * 0.80:
                        continue
                    if x0 < page_w * 0.10 or x1 > page_w * 0.90:
                        continue
                    if fsz < 8:
                        continue

                    spans.append((x0, y0, x1, y1, fsz, txt))

                if not spans:
                    continue

                # 2) ê°™ì€ ë¼ì¸ì˜ ìŠ¤íŒ¬ë“¤ì„ x0 ê¸°ì¤€ ì •ë ¬í•˜ê³ , ê°„ê²©ìœ¼ë¡œ ìŠ¤í˜ì´ìŠ¤ ë³µì›
                spans.sort(key=lambda s: s[0])  # x0
                rebuilt_parts = []
                prev_x1 = None
                prev_fsz = None

                for x0, y0, x1, y1, fsz, txt in spans:
                    # ë„ì–´ì“°ê¸° ë³µì› íœ´ë¦¬ìŠ¤í‹±
                    # - ìŠ¤íŒ¬ ê°„ gap > max(0.5*font_size, 2.0) ì´ë©´ ê³µë°± ì‚½ì…
                    # - ë‹¨ì–´ ëì´ í•˜ì´í”ˆìœ¼ë¡œ ëŠê¸´ ê²½ìš°ëŠ” ê³µë°± ì‚½ì…í•˜ì§€ ì•ŠìŒ (ì¤„ë°”ê¿ˆ ì—°ê²°)
                    gap = 0 if prev_x1 is None else (x0 - prev_x1)
                    need_space = (prev_x1 is not None) and (gap > max(0.5 * (prev_fsz or fsz), 2.0))

                    if need_space and not (rebuilt_parts and rebuilt_parts[-1].endswith("-")):
                        rebuilt_parts.append(" ")

                    rebuilt_parts.append(txt)
                    prev_x1 = x1
                    prev_fsz = fsz

                line_text = "".join(rebuilt_parts).strip()

                # ë¼ì¸ ë í•˜ì´í”ˆ ì²˜ë¦¬: ì¤„ë°”ê¿ˆ í•˜ì´í”ˆìœ¼ë¡œ ì´ì–´ì§„ ê²½ìš° ë‹¨ì–´ ë¶™ì´ê¸°
                # e.g., "investi-" + next line "gation" â†’ "investigation"
                line_text = re.sub(r"-\s*$", "", line_text)

                # ë¼ì¸ì˜ ëŒ€í‘œ ì¢Œí‘œ(ìœ„ìª½ y, ìµœì†Œ x) ì €ì¥
                y_top = min(s[1] for s in spans)
                x_min = min(s[0] for s in spans)

                # ë„ˆë¬´ ì§§ì€ ì¡ìŒ ë¼ì¸ì€ ìŠ¤í‚µ
                if len(line_text) < 3:
                    continue
                line_items.append((y_top, x_min, line_text))

        if not line_items:
            continue

        # 3) ì¢Œ/ìš° ì¹¼ëŸ¼ ë¶„ë¦¬ í›„, "ì™¼ìª½ ì „ë¶€ â†’ ì˜¤ë¥¸ìª½ ì „ë¶€" ìˆœì„œ ìœ ì§€
        #    (ìš”ì²­í•˜ì‹  A1 A2 A3 B1 B2 B3 ìˆœì„œ)
        mid_x = page_w / 2.0
        left_lines  = [(y, x, t) for (y, x, t) in line_items if x < mid_x]
        right_lines = [(y, x, t) for (y, x, t) in line_items if x >= mid_x]

        left_lines.sort(key=lambda r: (round(r[0], 1), round(r[1], 1)))
        right_lines.sort(key=lambda r: (round(r[0], 1), round(r[1], 1)))

        # 4) ë¼ì¸ í•©ì¹˜ê¸° + ê³µë°±/ì¤‘ë³µ ì •ë¦¬
        def join_lines(lines):
            # ë¼ì¸ ì‚¬ì´ ë¶ˆí•„ìš”í•œ ì¤‘ë³µ ê³µë°± ì •ë¦¬
            text = "\n".join(t for (_, _, t) in lines)
            text = re.sub(r"[ \t]{2,}", " ", text)
            text = re.sub(r"\n{3,}", "\n\n", text)
            return text.strip()

        page_text = "\n".join([
            join_lines(left_lines),
            join_lines(right_lines)
        ]).strip()

        # ë„ˆë¬´ ë¹ˆì•½í•œ í˜ì´ì§€ëŠ” ì œì™¸
        if len(page_text) >= 30:
            all_pages.append(page_text)

    return "\n\n".join(all_pages)

# def extract_text_mupdf_clean_strict(pdf_path: str):
#     """
#     ë‘ë‹¨(column) ë¬¸ì„œëŠ” ê·¸ëŒ€ë¡œ ì™¼ìª½â†’ì˜¤ë¥¸ìª½ ìˆœì„œë¡œ ì½ë˜,
#     ë³¸ë¬¸ ë°”ê¹¥(ìƒë‹¨/í•˜ë‹¨/ëª¨ì„œë¦¬/ì‘ì€ í°íŠ¸) ì¡ìŒì€ ì œê±°.
#     """
#     import fitz
#     import numpy as np

#     doc = fitz.open(pdf_path)
#     pages = []
#     header_texts = set()
#     footer_texts = set()

#     # 1ï¸âƒ£ í—¤ë”/í‘¸í„° í›„ë³´ í…ìŠ¤íŠ¸ ì¶”ì •
#     for page in doc[:min(3, len(doc))]:
#         blocks = page.get_text("blocks")
#         blocks = sorted(blocks, key=lambda b: b[1])  # yì¢Œí‘œ
#         if not blocks:
#             continue
#         top_text = blocks[0][4].strip()
#         bottom_text = blocks[-1][4].strip()
#         if len(top_text) < 120:
#             header_texts.add(top_text)
#         if len(bottom_text) < 120:
#             footer_texts.add(bottom_text)

#     # 2ï¸âƒ£ ë³¸ë¬¸ íŒŒì‹±
#     for page in doc:
#         page_w, page_h = page.rect.width, page.rect.height
#         blocks = page.get_text("dict")["blocks"]
#         filtered_blocks = []

#         # ğŸ”¹ ë¸”ë¡ ë‹¨ìœ„ë¡œ í•„í„°ë§ (í°íŠ¸í¬ê¸° + ì¢Œí‘œ)
#         for b in blocks:
#             for line in b.get("lines", []):
#                 for span in line.get("spans", []):
#                     text = span["text"].strip()
#                     if not text:
#                         continue
#                     x0, y0, x1, y1 = span["bbox"]
                    
#                     font_size = span.get("size", 0)
#                     if y0 < page_h * 0.15 or y1 > page_h * 0.85:
#                         continue

#                     # ë„ˆë¬´ ì‘ì€ í°íŠ¸ëŠ” ì œê±° (ê°ì£¼, í•™íšŒëª… ë“±)
#                     if font_size < 8:
#                         continue

#                     # í˜ì´ì§€ ë°”ê¹¥ìª½ ì¡ì˜ì—­ (ìƒë‹¨Â·í•˜ë‹¨Â·ëª¨ì„œë¦¬)
#                     margin_x_ratio = (x0 / page_w, x1 / page_w)
#                     margin_y_ratio = (y0 / page_h, y1 / page_h)
#                     if margin_y_ratio[0] < 0.06 or margin_y_ratio[1] > 0.94:
#                         continue
#                     if margin_x_ratio[0] < 0.04 or margin_x_ratio[1] > 0.96:
#                         continue

#                     filtered_blocks.append((x0, y0, x1, y1, text))

#         # ë‘ë‹¨ ê°ì§€ ë° ì •ë ¬ (ê¸°ì¡´ ë°©ì‹ ê·¸ëŒ€ë¡œ ìœ ì§€)
#         if not filtered_blocks:
#             continue
#         widths = [b[2]-b[0] for b in filtered_blocks]
#         median_width = np.median(widths)
#         two_column = median_width < page_w * 0.45

#         left_blocks = [b for b in filtered_blocks if b[0] < page_w / 2]
#         right_blocks = [b for b in filtered_blocks if b[0] >= page_w / 2]

#         def sort_blocks(blks):
#             return sorted(blks, key=lambda b: (round(b[1], 1), round(b[0], 1)))

#         lines = []
#         for blk in sort_blocks(left_blocks) + sort_blocks(right_blocks):
#             lines.append(blk[4].strip())

#         text = "\n".join(lines)
#         if len(text.strip()) > 30:
#             pages.append(text)

#     return "\n\n".join(pages)

# â€”â€”â€” 3ï¸âƒ£ í‘œ ë° ì„¸ë°€í•œ ì¤„ ë³´ì • (pdfplumber) â€”â€”â€”
def extract_text_pdfplumber(pdf_path: str):
    all_text = []
    tables = []
    with pdfplumber.open(pdf_path) as pdf:
        for i, page in enumerate(pdf.pages):
            chars = sorted(page.chars, key=lambda c: (round(c["top"], 1), c["x0"]))
            prev_y = None
            txt = ""
            for ch in chars:
                if prev_y is None or abs(ch["top"] - prev_y) > 5:
                    txt += "\n"
                txt += ch["text"]
                prev_y = ch["top"]
            all_text.append(txt)
            for tbl in page.extract_tables():
                # í‘œë¥¼ ë§ˆí¬ë‹¤ìš´ ë“± ë¬¸ìì—´ë¡œ ì €ì¥
                import pandas as pd
                df = pd.DataFrame(tbl[1:], columns=tbl[0])
                tables.append({"page": i, "content": df.to_markdown(index=False)})
    return "\n\n".join(all_text), tables


# â€”â€”â€” 4ï¸âƒ£ í˜ì´ì§€ë³„ êµ¬ì¡° ê°ì§€ (í…ìŠ¤íŠ¸ vs ì´ë¯¸ì§€ vs í‘œ ë“±) â€”â€”â€”
def analyze_page_type(pdf_path):
    """í˜ì´ì§€ë³„ë¡œ í…ìŠ¤íŠ¸/ì´ë¯¸ì§€/í‘œ ë¹„ìœ¨ ë¶„ì„"""
    doc = fitz.open(pdf_path)
    page_types = []
    for i, page in enumerate(doc):
        img_count = len(page.get_images())
        text_blocks = page.get_text("blocks")
        text_len = sum(len(b[4].strip()) for b in text_blocks if b[4].strip())
        text_lower = (page.get_text("text") or "").lower()
        
        if img_count > 0 and text_len > 400:
            ptype = "text+image"
        elif img_count > 0 and text_len < 100:
            ptype = "image_only"
        elif "table" in text_lower or "í‘œ" in text_lower:
            ptype = "text+table"
        else:
            ptype = "text_only"

        page_types.append({"page": i, "type": ptype, "img_count": img_count, "text_len": text_len})
    return page_types


docling = DocumentConverter()
def parse_docling_only(pdf_path: str, page_numbers: list[int] | None = None):
    """
    ìµœì‹  Docling APIì—ì„œëŠ” page_numbers ì¸ìë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŒ.
    ë”°ë¼ì„œ ì „ì²´ ë¬¸ì„œë¥¼ ë³€í™˜í•˜ê³  ì´í›„ í•„ìš”í•œ í˜ì´ì§€ë§Œ í•„í„°ë§.
    """
    result = docling.convert(pdf_path)
    doc_struct = result.document

    # page_numbersê°€ ìˆì„ ê²½ìš° â€” í•´ë‹¹ í˜ì´ì§€ ë°ì´í„°ë§Œ í•„í„°ë§
    if page_numbers is not None and hasattr(doc_struct, "sections"):
        selected_sections = [
            sec for sec in doc_struct.sections
            if getattr(sec, "page_number", None) in page_numbers
        ]
        # ì„ì‹œ êµ¬ì¡°ì²´ í‰ë‚´ â€” sectionë§Œ êµì²´
        doc_struct.sections = selected_sections

    return doc_struct
# â€”â€”â€” 5ï¸âƒ£ í˜ì´ì§€ë³„ë¡œ íŒŒì„œ ì„ íƒ ì‹¤í–‰ â€”â€”â€”
def parse_page_adaptively(pdf_path):
    """í˜ì´ì§€ ë‹¨ìœ„ ì ì‘í˜• íŒŒì‹± (ë¹ˆ í˜ì´ì§€ ëˆ„ë½ ë°©ì§€)"""
    page_summaries = analyze_page_type(pdf_path)
    results = []

    import pandas as pd

    for info in page_summaries:
        page_idx = info["page"]
        ptype = info["type"]
        base_meta = {"page": page_idx, "source": os.path.basename(pdf_path)}

        try:
            if ptype == "text_only":
                # âœ… Docling sectionì´ ì—†ë”ë¼ë„ mupdf fallback ì¶”ê°€
                doc_struct = parse_docling_only(pdf_path, page_numbers=[page_idx + 1])
                section_added = False
                for sec in getattr(doc_struct, "sections", []):
                    results.append({
                        "page_content": sec.text.strip(),
                        "metadata": {**base_meta, "type": "text"}
                    })
                    section_added = True
                if not section_added:
                    # ğŸ”» íŠ¹ì • í˜ì´ì§€ë§Œ ì¶”ì¶œ
                    doc = fitz.open(pdf_path)
                    page = doc[page_idx]
                    txt = page.get_text("text")
                    results.append({
                        "page_content": txt.strip(),
                        "metadata": {**base_meta, "type": "text-fallback"}
                    })

            elif ptype == "text+table":
                with pdfplumber.open(pdf_path) as pdf:
                    page = pdf.pages[page_idx]
                    text = page.extract_text() or ""
                    tables = page.extract_tables()
                    if tables:
                        for tbl in tables:
                            df = pd.DataFrame(tbl[1:], columns=tbl[0])
                            results.append({
                                "page_content": df.to_markdown(index=False),
                                "metadata": {**base_meta, "type": "table"}
                            })
                    if text.strip():
                        results.append({
                            "page_content": text.strip(),
                            "metadata": {**base_meta, "type": "text+table"}
                        })
                    else:
                        # âœ… í‘œë§Œ ìˆê³  í…ìŠ¤íŠ¸ ì—†ëŠ” ê²½ìš°ë¼ë„ dummy í…ìŠ¤íŠ¸ ì¶”ê°€
                        results.append({
                            "page_content": "[No text on this page]",
                            "metadata": {**base_meta, "type": "table-only"}
                        })
            
            elif ptype in ["text+image","image_only"] :
                doc_struct = parse_docling_only(pdf_path, page_numbers=[page_idx + 1])
                section_added = False
                for sec in getattr(doc_struct, "sections", []):
                    results.append({
                        "page_content": sec.text.strip(),
                        "metadata": {**base_meta, "type": "text"}
                    })
                    section_added = True
                if not section_added:
                    # íŠ¹ì • í˜ì´ì§€ë§Œ ì¶”ì¶œ
                    doc = fitz.open(pdf_path)
                    page = doc[page_idx]
                    txt = page.get_text("text")
                    results.append({
                        "page_content": txt.strip(),
                        "metadata": {**base_meta, "type": "text-fallback"}
                    })
        except Exception as e:
            #  í˜ì´ì§€ë³„ ì—ëŸ¬ ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰
            results.append({
                "page_content": f"[Page {page_idx} skipped due to error: {str(e)}]",
                "metadata": {**base_meta, "type": "error"}
            })
    return results


# â€”â€”â€” ë©”ì¸ íŒŒì‹± í•¨ìˆ˜ (Docling ê¸°ë°˜) â€”â€”â€”
def parse_docling_with_fallback(pdf_path: str):
    """
    Doclingì„ ë©”ì¸ íŒŒì„œë¡œ ì‚¬ìš©í•˜ë˜, fallback ë° ë³´ì • ë³‘í•© ë¡œì§ í¬í•¨
    """
    docs = []
    

    if not has_text_layer(pdf_path):
        from pdf2image import convert_from_path
        import pytesseract

        images = convert_from_path(pdf_path)
        for pi, img in enumerate(images):
            txt = pytesseract.image_to_string(img, lang="eng+kor")
            docs.append({
                "page_content": txt,
                "metadata": {"source": os.path.basename(pdf_path), "page": pi, "type": "ocr"}
            })
        return docs
    
    converted = parse_docling_only(pdf_path)
    doc_struct = converted.document

    for sec in doc_struct.sections:
        docs.append({
            "page_content": sec.text,
            "metadata": {"source": os.path.basename(pdf_path), "type": "section", "title": sec.title if hasattr(sec, "title") else None}
        })

    for tbl in doc_struct.tables:
        docs.append({
            "page_content": tbl.html or tbl.markdown or tbl.csv, 
            "metadata": {"source": os.path.basename(pdf_path), "type": "table", "page": tbl.page_number}
        })

    if not docs:
        t1 = extract_text_mupdf_clean_strict(pdf_path)
        t2, tbls = extract_text_pdfplumber(pdf_path)
        
        merged = t1 if len(t1) > len(t2) else t2
        docs.append({
            "page_content": merged,
            "metadata": {"source": os.path.basename(pdf_path), "type": "fallback"}
        })
        for tm in tbls:
            docs.append({
                "page_content": tm["content"],
                "metadata": {"source": os.path.basename(pdf_path), "type": "table", "page": tm["page"]}
            })

    return docs


# â€”â€”â€” 6ï¸âƒ£ ì „ì²´ ë¬¸ì„œ íŒŒì‹± ì»¨íŠ¸ë¡¤ëŸ¬ â€”â€”â€”
def parse_with_docling(pdf_path: str):
    """
    PDFë¥¼ í˜ì´ì§€ë³„ë¡œ ë¶„ì„í•˜ì—¬
    Docling / pdfplumber / layoutparser / OCR ì„ ìë™ ì ìš©
    """
    docs = []

    if not has_text_layer(pdf_path):
        # ì „ì²´ê°€ ì´ë¯¸ì§€ PDFì¼ ê²½ìš° OCR
        from pdf2image import convert_from_path
        import pytesseract
        images = convert_from_path(pdf_path)
        for pi, img in enumerate(images):
            txt = pytesseract.image_to_string(img, lang="eng+kor")
            docs.append({
                "page_content": txt,
                "metadata": {"source": os.path.basename(pdf_path), "page": pi, "type": "ocr"}
            })
        return docs

    # âœ… í˜ì´ì§€ ë‹¨ìœ„ íŒŒì‹± ì‹¤í–‰
    adaptive_docs = parse_page_adaptively(pdf_path)

    # âœ… fallback ì•ˆì „ì¥ì¹˜ (í˜¹ì‹œ ì‹¤íŒ¨í–ˆì„ ë•Œ)
    if not adaptive_docs:
        t1 = extract_text_mupdf_clean_strict(pdf_path)
        t2, tbls = extract_text_pdfplumber(pdf_path)
        merged = t1 if len(t1) > len(t2) else t2
        docs.append({
            "page_content": merged,
            "metadata": {"source": os.path.basename(pdf_path), "type": "fallback"}
        })
        for tm in tbls:
            docs.append({
                "page_content": tm["content"],
                "metadata": {"source": os.path.basename(pdf_path), "type": "table", "page": tm["page"]}
            })
    else:
        docs.extend(adaptive_docs)

    return docs