{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdc07dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os, sys\n",
    "# from pathlib import Path\n",
    "# def get_project_root(marker=\"pyproject.toml\"): # í˜„ì¬ í”„ë¡œì íŠ¸ì˜ ê²½ë¡œë¥¼ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ ì‚¬ìš©ìš©\n",
    "#     cur = Path(__file__).resolve() if \"__file__\" in globals() else Path().resolve()\n",
    "#     return next((p for p in [cur, *cur.parents] if (p / marker).exists()), cur)\n",
    "\n",
    "# src_path = Path(os.getcwd()).resolve().parents[1]  \n",
    "# sys.path.append(str(src_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f4c3cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â–¶ ë¡œë”©: /Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2505.30656746v1.full_pages/qa_dataset.jsonl\n",
      "â–¶ ë¡œë”©: /Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2411.00114v1_pages/qa_dataset.jsonl\n",
      "â–¶ ë¡œë”©: /Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2502.14143v1_pages/qa_dataset.jsonl\n",
      "â–¶ ë¡œë”©: /Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2504.16736v3_pages/qa_dataset.jsonl\n",
      "â–¶ ë¡œë”©: /Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2501.00083v1_pages/qa_dataset.jsonl\n",
      "â–¶ ë¡œë”©: /Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2506.19676v3_pages/qa_dataset.jsonl\n",
      "â–¶ ë¡œë”©: /Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2408.07199v1_pages/qa_dataset.jsonl\n",
      "â–¶ ë¡œë”©: /Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2503.10970v1_pages/qa_dataset.jsonl\n",
      "â–¶ ë¡œë”©: /Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2411.15594v5_pages/qa_dataset.jsonl\n",
      "â–¶ ë¡œë”©: /Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2411.15692v2_pages/qa_dataset.jsonl\n",
      "â–¶ ë¡œë”©: /Users/seobi/PythonProjects/AxDeepScholar/src/data/agent/agent_2504.03646459v1.full_pages/qa_dataset.jsonl\n",
      "\n",
      "ì´ ì¶”ì¶œëœ (Q, A) pair ê°œìˆ˜: 2288\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def parse_qa_pairs(qa_pair_text: str):\n",
    "    \"\"\"\n",
    "    qa_pair ë¬¸ìì—´ì—ì„œ (question, answer) ìŒì„ ë¦¬ìŠ¤íŠ¸ë¡œ ì¶”ì¶œ.\n",
    "\n",
    "    ê¸°ëŒ€ í˜•ì‹ ì˜ˆ:\n",
    "    Q1: ...ì§ˆë¬¸...\n",
    "    A1: ...ë‹µë³€...\n",
    "\n",
    "    Q2: ...ì§ˆë¬¸...\n",
    "    A2: ...ë‹µë³€...\n",
    "\n",
    "    Që²ˆí˜¸ì™€ Aë²ˆí˜¸ê°€ ë§ëŠ” êµ¬ê°„ì„ ì •ê·œì‹ìœ¼ë¡œ ì¡ì•„ì„œ íŒŒì‹±.\n",
    "    \"\"\"\n",
    "    qa_pair_text = qa_pair_text.strip()\n",
    "\n",
    "    pattern = re.compile(\n",
    "        r\"Q(\\d+):\\s*(.+?)\\nA\\1:\\s*(.+?)(?=(\\nQ\\d+:)|$)\",\n",
    "        re.DOTALL\n",
    "    )\n",
    "\n",
    "    pairs = []\n",
    "    for m in pattern.finditer(qa_pair_text):\n",
    "        q = m.group(2).strip()\n",
    "        a = m.group(3).strip()\n",
    "\n",
    "        # ë‹µë³€ì—ì„œ \"ì¶œì²˜: ...\"ëŠ” ì„ë² ë”©ì—” ë…¸ì´ì¦ˆì¼ ìˆ˜ ìˆìœ¼ë‹ˆ ì˜ë¼ë‚¼ì§€ ì„ íƒ\n",
    "        a = re.sub(r\"ì¶œì²˜:.*\", \"\", a, flags=re.DOTALL).strip()\n",
    "\n",
    "        if q and a:\n",
    "            pairs.append((q, a))\n",
    "    return pairs\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "from glob import glob\n",
    "from sentence_transformers import InputExample\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# BASE_DIR = str(get_project_root() / \"src\"/ \"data\" / \"agent\")\n",
    "BASE_DIR = \"data\" / \"agent\"\n",
    "\n",
    "# _pages ë””ë ‰í† ë¦¬ ìë™ íƒìƒ‰\n",
    "pages_dirs = [\n",
    "    os.path.join(BASE_DIR, d) for d in os.listdir(BASE_DIR)\n",
    "    if d.endswith(\"_pages\") and os.path.isdir(os.path.join(BASE_DIR, d))\n",
    "]\n",
    "\n",
    "qa_pairs = []\n",
    "\n",
    "for folder in pages_dirs:\n",
    "    jsonl_path = os.path.join(folder, \"qa_dataset.jsonl\")\n",
    "    if not os.path.exists(jsonl_path):\n",
    "        continue\n",
    "\n",
    "    print(f\"â–¶ ë¡œë”©: {jsonl_path}\")\n",
    "    with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            obj = json.loads(line)\n",
    "\n",
    "            qa_text = obj.get(\"qa_pair\", \"\")\n",
    "            if not qa_text:\n",
    "                continue\n",
    "\n",
    "            # ğŸ”¥ ì—¬ê¸°ì„œ í•œ ì¤„ ì•ˆì˜ Q1/A1, Q2/A2, ... ì „ë¶€ ìª¼ê°¬\n",
    "            pairs = parse_qa_pairs(qa_text)\n",
    "\n",
    "            qa_pairs.extend(pairs)\n",
    "\n",
    "print(f\"\\nì´ ì¶”ì¶œëœ (Q, A) pair ê°œìˆ˜: {len(qa_pairs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e83538f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì—ì„œ ì—ì´ì „íŠ¸ë“¤ì˜ ë¬¸ëª… ì§„ë³´ë¥¼ ìœ„í•œ ë²¤ì¹˜ë§ˆí¬ê°€ ë¶€ì¬í•œ ì´ìœ ëŠ” ë¬´ì—‡ì´ë©°, ì´ë¡œ ì¸í•œ ì—°êµ¬ì˜ í•œê³„ëŠ” ë¬´ì—‡ì¸ê°€ìš”?',\n",
       " 'ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì—ì„œ ë¬¸ëª… ì§„ë³´ë¥¼ ìœ„í•œ ë²¤ì¹˜ë§ˆí¬ê°€ ë¶€ì¬í•œ ì´ìœ ëŠ” ê¸°ì¡´ì˜ ë²¤ì¹˜ë§ˆí¬ë“¤ì´ ì›¹ ê²€ìƒ‰, ì½”ë”©, ê²€ìƒ‰ ë° ì§ˆì˜, ì¶”ë¡ ê³¼ ê°™ì€ ë‹¤ì–‘í•œ ë„ë©”ì¸ì—ì„œì˜ ììœ¨ì  ì—ì´ì „íŠ¸ ì„±ëŠ¥ì— ì§‘ì¤‘ë˜ì–´ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì´ë¡œ ì¸í•œ ì—°êµ¬ì˜ í•œê³„ëŠ” ì—ì´ì „íŠ¸ë“¤ì´ ì‚¬íšŒì , ë¬¸í™”ì , ê¸°ìˆ ì  ì§„ë³´ë¥¼ ì´ë£¨ëŠ” ë° í•„ìš”í•œ ë³µì¡í•œ ìƒí˜¸ì‘ìš©ê³¼ ì¥ê¸°ì ì¸ ëª©í‘œ ë‹¬ì„±ì„ ì¸¡ì •í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªê²Œ ëœë‹¤ëŠ” ì ì…ë‹ˆë‹¤. ì´ëŠ” ë©€í‹° ì—ì´ì „íŠ¸ ê·¸ë£¹ì´ í˜‘ë ¥í•˜ì—¬ ë¬¸ëª…ì„ ë°œì „ì‹œí‚¤ëŠ” ê³¼ì •ì„ í‰ê°€í•˜ëŠ” ë° í•„ìš”í•œ ìƒˆë¡œìš´ ë²¤ì¹˜ë§ˆí¬ì˜ í•„ìš”ì„±ì„ ì‹œì‚¬í•©ë‹ˆë‹¤.')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_pairs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2541de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train pairs: 2059\n",
      "Val pairs: 229\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# qa_pairs = [(question, answer), ...]\n",
    "random.shuffle(qa_pairs)\n",
    "\n",
    "split = int(len(qa_pairs) * 0.9)\n",
    "train_pairs = qa_pairs[:split]\n",
    "val_pairs = qa_pairs[split:]\n",
    "\n",
    "print(\"Train pairs:\", len(train_pairs))\n",
    "print(\"Val pairs:\", len(val_pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0c2dd1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì²« ìƒ˜í”Œ: ['ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì—ì„œ ì—ì´ì „íŠ¸ë“¤ì˜ ë¬¸ëª… ì§„ë³´ë¥¼ ìœ„í•œ ë²¤ì¹˜ë§ˆí¬ê°€ ë¶€ì¬í•œ ì´ìœ ëŠ” ë¬´ì—‡ì´ë©°, ì´ë¡œ ì¸í•œ ì—°êµ¬ì˜ í•œê³„ëŠ” ë¬´ì—‡ì¸ê°€ìš”?', 'ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì—ì„œ ë¬¸ëª… ì§„ë³´ë¥¼ ìœ„í•œ ë²¤ì¹˜ë§ˆí¬ê°€ ë¶€ì¬í•œ ì´ìœ ëŠ” ê¸°ì¡´ì˜ ë²¤ì¹˜ë§ˆí¬ë“¤ì´ ì›¹ ê²€ìƒ‰, ì½”ë”©, ê²€ìƒ‰ ë° ì§ˆì˜, ì¶”ë¡ ê³¼ ê°™ì€ ë‹¤ì–‘í•œ ë„ë©”ì¸ì—ì„œì˜ ììœ¨ì  ì—ì´ì „íŠ¸ ì„±ëŠ¥ì— ì§‘ì¤‘ë˜ì–´ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì´ë¡œ ì¸í•œ ì—°êµ¬ì˜ í•œê³„ëŠ” ì—ì´ì „íŠ¸ë“¤ì´ ì‚¬íšŒì , ë¬¸í™”ì , ê¸°ìˆ ì  ì§„ë³´ë¥¼ ì´ë£¨ëŠ” ë° í•„ìš”í•œ ë³µì¡í•œ ìƒí˜¸ì‘ìš©ê³¼ ì¥ê¸°ì ì¸ ëª©í‘œ ë‹¬ì„±ì„ ì¸¡ì •í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªê²Œ ëœë‹¤ëŠ” ì ì…ë‹ˆë‹¤. ì´ëŠ” ë©€í‹° ì—ì´ì „íŠ¸ ê·¸ë£¹ì´ í˜‘ë ¥í•˜ì—¬ ë¬¸ëª…ì„ ë°œì „ì‹œí‚¤ëŠ” ê³¼ì •ì„ í‰ê°€í•˜ëŠ” ë° í•„ìš”í•œ ìƒˆë¡œìš´ ë²¤ì¹˜ë§ˆí¬ì˜ í•„ìš”ì„±ì„ ì‹œì‚¬í•©ë‹ˆë‹¤.']\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import InputExample\n",
    "\n",
    "train_examples = [\n",
    "    InputExample(texts=[q, a]) for q, a in train_pairs\n",
    "]\n",
    "\n",
    "print(\"ì²« ìƒ˜í”Œ:\", train_examples[0].texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e747437",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 16   # GPU ìƒí™© ë³´ê³  ì¡°ì ˆ\n",
    "train_dataloader = DataLoader(\n",
    "    train_examples,\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dcd040d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, losses\n",
    "\n",
    "model_id = \"BAAI/bge-m3\"\n",
    "model = SentenceTransformer(model_id)\n",
    "\n",
    "loss = losses.MultipleNegativesRankingLoss(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34d0e2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì§ˆë¬¸ ê°œìˆ˜: 229\n",
      "ì½”í¼ìŠ¤ ë¬¸ì„œ ê°œìˆ˜: 229\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
    "\n",
    "# val_pairs = [(q, a), ...]\n",
    "val_dataset = {\n",
    "    \"queries\": {},\n",
    "    \"corpus\": {},\n",
    "    \"relevant_docs\": {}\n",
    "}\n",
    "\n",
    "for i, (q, a) in enumerate(val_pairs):\n",
    "    qid = f\"q{i}\"\n",
    "    did = f\"d{i}\"\n",
    "    val_dataset[\"queries\"][qid] = q\n",
    "    val_dataset[\"corpus\"][did] = a\n",
    "    val_dataset[\"relevant_docs\"][qid] = {did}  # ì´ ì§ˆë¬¸ì˜ ì •ë‹µ ë¬¸ì„œëŠ” ìê¸° answer í•˜ë‚˜\n",
    "\n",
    "corpus = val_dataset[\"corpus\"]\n",
    "queries = val_dataset[\"queries\"]\n",
    "relevant_docs = val_dataset[\"relevant_docs\"]\n",
    "\n",
    "evaluator = InformationRetrievalEvaluator(\n",
    "    queries,\n",
    "    corpus,\n",
    "    relevant_docs\n",
    ")\n",
    "\n",
    "print(\"ì§ˆë¬¸ ê°œìˆ˜:\", len(queries))\n",
    "print(\"ì½”í¼ìŠ¤ ë¬¸ì„œ ê°œìˆ˜:\", len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286d2ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "EPOCHS = 2\n",
    "warmup_steps = int(len(train_dataloader) * EPOCHS * 0.1)\n",
    "\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, loss)],\n",
    "    epochs=EPOCHS,\n",
    "    warmup_steps=warmup_steps,\n",
    "    output_path=\"bge3_finetuned\",\n",
    "    show_progress_bar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64b29e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def evaluate_st(dataset, model_id, name, evaluator):\n",
    "    \"\"\"\n",
    "    SentenceTransformer ëª¨ë¸ì˜ ê²€ìƒ‰ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "    os.makedirs('results', exist_ok=True)\n",
    "\n",
    "    model = SentenceTransformer(model_id)\n",
    "\n",
    "    result = evaluator(model)\n",
    "\n",
    "    result_df = pd.DataFrame([result]) if isinstance(result, dict) else result\n",
    "    output_path = f'results/Information-Retrieval_evaluation_{name}_results.csv'\n",
    "    result_df.to_csv(output_path, index=False)\n",
    "\n",
    "    return result\n",
    "\n",
    "# íŒŒì¸íŠœë‹ ëª¨ë¸ í‰ê°€\n",
    "finetuned_model_path = \"exp_finetune\"\n",
    "evaluate_st(dataset=val_dataset, model_id=finetuned_model_path, name=\"finetuned\", evaluator=evaluator)\n",
    "print(\"íŒŒì¸íŠœë‹ ëª¨ë¸ í‰ê°€ ì™„ë£Œ\")\n",
    "\n",
    "# ì›ë³¸ bge-m3 í‰ê°€\n",
    "original_model_path = \"BAAI/bge-m3\"\n",
    "evaluate_st(dataset=val_dataset, model_id=original_model_path, name=\"original\", evaluator=evaluator)\n",
    "print(\"ì›ë³¸ ëª¨ë¸ í‰ê°€ ì™„ë£Œ\")\n",
    "\n",
    "# ê²°ê³¼ ë¹„êµ\n",
    "df_st_original = pd.read_csv('results/Information-Retrieval_evaluation_original_results.csv')\n",
    "df_st_finetuned = pd.read_csv('results/Information-Retrieval_evaluation_finetuned_results.csv')\n",
    "\n",
    "df_st_original['model'] = 'bge-m3'\n",
    "df_st_finetuned['model'] = 'fine_tuned'\n",
    "df_st_all = pd.concat([df_st_original, df_st_finetuned])\n",
    "df_st_all = df_st_all.set_index('model')\n",
    "\n",
    "print(\"\\nëª¨ë¸ ì„±ëŠ¥ ë¹„êµ:\")\n",
    "df_st_all"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AxDeepScholar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
